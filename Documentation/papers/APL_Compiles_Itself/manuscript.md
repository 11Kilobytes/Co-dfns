# AbstractQuick, how do you write a compiler without branching, conditionals, case statements, or recursion?The Co-dfns compiler transforms a parallel extension of the dfns dialect of APL into C and CUDA code.  It targets the GPU as well as the CPU.  It also pushes the assumptions of viable application domains for array programming. In particular, Co-dfns is implemented in the Co-dfns language.  Furthermore, an explicit design aesthetic of the compiler style requires that, whenever possible, all compiler passes have no conditionals, branching, or recursion in their descriptions.  Surprisingly, not only can this be done, but the resulting compiler exhibits both consision and simplicity.Nearly the entire compiler, including the handling of lexical scope, function lifting, and closure creation, contains no explicit branching, traditional control structures, or recursion: each compiler pass contains a simple data flow implementation with almost trivial control flow.  Using traditional array programming concepts together with insights into how to encode control flow information into simple data representations, The Co-dfns compiler demonstrates a novel and interesting approach to compiler implementation.# IntroductionModern compiler design relies on recursion and branching execution. Clang's `RecursiveASTVisitor` implements a recursive traversal combined with methods for node operations, essentially a case statement. The Nanopass compiler framework supports compilers as the composition of many small tree transformations defined by dispatching on node type and recuring on children either implicitly or explicitly. This reliance should surprise none: the patterns of control flow exhibited by recursion and branching match very naturally the essence of compiler implementation, tree traversal. What if you could not use branching or recursion?Likely, few would even make the attempt to create a compiler that used neither recursion nor branching. Indeed, what value lives in such an artifact? Such an artifact would certainly challenge and elucidate the construction of compilers in ways not previously seen. Moreover, today's modern architectures exhibit features which suggest that such an approach could also benefit the practical implementation of compilers. Particularly, modern CPU's have vector engines; GPU's generally prove difficult targets for which to implement a traditional compiler; and, distributed systems benefit from more parallel implementations. A vectorized compiler that uses data-parallel operations and simple, straightforward control flow instead of recursion and branching provides insights into tree traversal and avenues for research into the efficiency of compilers and similar programs on modern computing architectures. The Co-dfns compiler transforms a parallel extension of the dfns dialect of APL into C and Cuda code. Uniquely, the implementation is written entirely in Co-dfns. While Co-dfns readily supports recursion and branching, the project pushes the scale and range of APL by implementing the compiler using the native APL vocabulary and traditional array-programming idioms, rather than using recursion or branching. This leads to a very lean, concise, and simple compiler that exhibits very different characteristics both aesthetically and operationally.Modulo parsing, the entire Co-dfns compiler uses the APL array language vocabulary and a nanopass style composition of small, pure functional compiler passes, written with simple data-flow control, to convert between the Co-dfns language, whose AST is encoded as an array, into the target language, a mix of C and Cuda for execution on both the CPU and GPU. The compiler will also target HPX for execution of parallel code on SMP and distributed clusters.The following sections discuss the key implementation insights for each of the major front-end compiler passes for languages that exhibit similiar features to Co-dfns, including lexical scope and higher-order functions. For each pass, the key implementation insight and overall strategy of compilation demonstrates the methods and algorithms used to arrive at a fully vectorizable compiler suitable for execution on the GPU. The exposition of these passes also reveals the concision and compactness of the approach, which accounts for the small amount of code necessary to implement the compiler. Readers may observe the external artifacts and references for the entire compiler source.# Notational Conventions# Language DefinitionThe compiled language consists of a simplified intermediate AST representing a program as might reasonably appear just before function lifting and flattening passes might occur. In particular, no function body contains any literal values, and expressions are semi-flattened. An attribute table attaches to each node in the AST, in addition to children that might appear. In this sense, the AST matches very closely the model used by XML trees. The AST described in [AST table] gives each node along with its set of attributes and its children.    Module[] := (FuncExpr | Expression)*    FuncExpr[name] := Function | Variable    Function[] := Expression*    Expression[name;class] := [Expression] FuncExpr Expression | Variable    Variable[name] := ()All ASTs start with a single `Module` node as their sole root node. `FuncExpr` nodes encapsulate function objects either by reference or by their definition. Functions consist of expressions. Note that `Function` nodes model the same sort of functions described in the previous section on notation. That is, they bind one or two arguments implicitly, and receive a single right argument and a single, optional left argument. Expressions either refer to a variable binding or one of two sorts of function application. The `class` attribute of the Expression indicates the expression type, either a variable reference, a monadic application of a function, or a dyadic application. The `FuncExpr` and `Expression` nodes may contain a `name` attribute, indicating a binding for that particular object. In addition to the explicit restrictions on the AST, the compiler passes described here presume that another compiler pass has already removed any unnamed (and therefore, useless) top-level expressions as well as syntactically unreachable code within the function bodies.## Encoding the ASTBecause all the compiler passes operate over arrays, the AST must exist in some array encoding of the tree structure, in this case, a 3-column matrix. Each row corresponds to a single node in the AST, ordered by pre-order depth-first traversal. The first column contains the depth of that node in the AST, starting with 0 for the root node. The second column contains the name of the node. The third column contains a 2-column association matrix associating a given key/attribute, with the value for that attribute. This representation encodes all relevant information, but also has the added benefit of being a standard tree encoding method used within the APL programming community.  # Compiler PassesIn order to perform flattening and lifting, the following series of compiler passes suffice to move from an AST as described above to a flattened AST, described in [Flattened AST table].    Module[] := (FuncExpr | Expression)*    FuncExpr[name] := Function | Variable    Function[] := Expression*    Expression[name;class;left;right;fn;        left_env;env;slot;left_slot;right_env;right_slot] := ()    Variable[name] := ()In particular, all function bindings now occur at the top-level, and all function bodies consist of expressions with no children. Instead, each expression contains attributes for the left and right arguments as well as the function name. Additionally to the name, for each reference there are two additional attributes, giving the environment index and slot position for that variable's location. The environment indicates how far up in the lexical stack to reach, and the slot gives the appropriate position in that environment that contains the variable referenced. For simplicity this section omits expression flattening, since the techniques match those of function lifting.## Node CoordinatesTraditional implementations of lifting and flattening encode information about the structure of the AST through recursion and branching on node types. Any method of lifting and flattening must encode this information in some usable way. Instead of using recursion and control flow to encode this structural information, consider the depth vector of the AST given by `0⌷⍉⍵` where `⍵` is the AST properly encoded. This vector encodes all of the structural information required to understand the parent-child relationships of the AST. Unfortunately, it does so in a way that does not allow for local reasoning about any given two nodes in the tree without surrounding contextual information. The `rn` compiler pass re-encodes this information in such a way that the structural relationship of any two given nodes exists locally for each given node, requiring no external context. In particular, given the reference or coordinate for any two nodes, simple array operations suffice to determine whether one node is an ancestor of the other, how deep in the tree each node is in relation to the other, and whether one node appears before or after the tree in the traversal. This information will prove critically important in other passes, so the `rn` pass annotates each node with an additional `ref` attribute containing the node reference, also called a coordinate. A node coordinate is a vector of length equal to the depth of the AST, consisting of natural numbers, whose non-zero elements precede its zero elements, the count of which equal the depth of the node. Additionally, a node is an ancestor of another node if and only if its coordinate is a prefix of second node's coordinate, ignoring zeros. A coordinate corresponds to a path from the root of the tree to the node. A coordinate also represents a unique identifier for any given node as an index into a multi-dimensional array whose rank is the depth of the AST. To compute the matrix of all coordinates, consider first the set of natural numbers from zero to the depth of the AST, inclusive, given by the following expression:    ⍳1+⌈/0,dHere `d` is the depth vector of the AST. Now consider the boolean matrix given by the outer product equating each element of the range and each element of the original depth vector, given by the following expression:    d∘.= ⍳1+⌈/0,dThe result depicts a neat little pictorial representation of the information encoded in the depth vector, if given more space. In particular, the tree like structure becomes obvious. Let `d` be given the following value:    d←<Some suitable code here>The structure defined by this tree becomes more clear by the above outer product:    <Insert Example Here>The above picture suggests another way of encoding the same information by scanning along the first dimension, resulting in the prefix sum for each column of the matrix. The following expression demonstrates this result using the same depth vector:          +⍀d∘.=⍳1+⌈/0,d    <More data here>Now, each row has a unique value encoding an index, and nearly all the desired invariants exist in the above matrix. However, spurious non-zero values exist which do not contribute to the uniqueness of the coordinates and likewise provide no further useful information. These spurious values are any non-zero values that appear in columns past the depth of any given node. By taking only the number of non-zero values up to the depth of each node, the appropriate coordinate matrix emerges.       r←(1+d)↑⍤¯1+⍀d∘.=⍳1+⌈/0,d    <More data here>Given this coordinate matrix, named `r`, all the other compiler passes may now compute with the parent-child relationships without requiring recursion or branching.## Function LiftingFunction lifting involves three particular insights to complete. Firstly, note that by computing the dimensions of the space described by the coordinate matrix, that is, the range of each column in the matrix, given by the maximum value plus one for each column, any given node may be given a unique natural number as an identifier. The following expression computes this dimension information, called `rm`:    rm←1+⌈⌿rHere `r` represents the coordinate matrix. Given `rm` and any row of `r`, a unique natural number identifier for that node is given by `rm⊤⍺` where `⍺` is a row of `r`. Each node stores this information locally, so each `Function` node to lift already provides a unique variable with which to replace it in the tree. This allows all variable generation to occur without requiring an accumulator or some other stateful system, which might require non-local computation.Next, some general strategy must exist for doing the actual lifting. In this case, the answer comes from a surprising place. Assume that a function `ngh` takes a coordinate as its left argument and a matrix group of nodes as its right. IN this case the coordinate is the coordinate of the function directly enclosing the nodes given on the right. The group of nodes in the right argument is the body of the function, including all `Function` nodes appearing in the body of the function, but *without* their function bodies. In this case, the `ngh` function has all the information necessary in these two arguments to create the lifted function definition at the top level. It need only replace each `Function` node, which no longer have children, with a `Variable` node using the unique identifier derived from the coordinate given in the `ref` attribute of the `Function` node, manipulate the depth vector of the group to shift all nodes to the top-level depth, preserving order and internal relationships, and finally, add a new function definition node enclosing the function group, using the given coordinate it received as its left argument. Note the importance of reusing the coordinate after lifting. The coordinate information must preserve the original structural information of the code, even after compiler passes mostly remove this structure from the AST.Now assume that there exists a value `c` which contains the coordinate of the enclosing `Function` node for each node in the tree. This `c` matrix contains the grouping information for each set of expressions, in the form of a set of keys by which to group each node. At this point, all of the information is in place to do function lifting, if only a strategy for lifting existed. In a stroke of surprising invention, the array programming community has possessed just such a strategy for many years. The Key (written `⌸`) operator has existed as an array programming primitive since at least the J programming language and other APL variants. Given an array represention keys on the left and a set of corresponding elements on the right, it applies its left operand once for each unique key in the left argument, passing the unique key as the left argument to the operand, and the set of elements associated with that key as the right argument. Here is a simple example:          2 1 3 2 3 {⍺ ⍵}⌸ ⍳5    ┌─┬───┐    │2│0 3│    ├─┼───┤    │1│1  │    ├─┼───┤    │3│2 4│    └─┴───┘Importantly, the key operator preserves order both of the appearance of the keys as well as the appearence of the elements. By applying this key operator to the set of scope keys given by `c` and the operand `ngh`, the result is a re-ordered tree with each scope given its own top-level function definition. This also has the side-effect of lifting all top-level expressions into a single group, which turns out to be useful in future passes. Thus, given `c` and `ngh`, the following expression lifts all functions to the top level.    c ngh⌸ ⍵Here `⍵` is the body of the `Module`. How does one compute `c`? Firstly, one should compute the value `sc`, which are all the scope coordinates with their last non-zero element zeroed. Secondly, compute `rf`, the coordinate matrix of all scope enclosing nodes, namely `Module` and `Function`. Then, the following expression computes the boolean matrix indicating which elements of `rf` are prefixes or equal to each row in `sc` by using an inner product:          sc∧.(=∨0=⊢)⍉rf    <Insert data here>The function `(=∨0=⊢)` is a predicate indicating whether the right argument is a prefix or equal to the left argument. This is combined with boolean `∧` in an innder product given by `∧.(=∨0=⊢)` to compute the boolean matrix. At this point, note that if one constructs `sc` and `rf` to be lexicographically ordered, then the last true value in each row of the resulting matrix will indicate the closest enclosing scope for each node. From this information the `c` matrix can be constructed naturally. For simplicity, this treatment omits the definition of `ngh`, as it is a natural function that requires only simple manipulation in an obvious and direct way to achieve its results.The above technique using the Key operator for flattening also works when applied to the flattening of expressions which have a nested structure.## Variable Anchoring# Related Work# Future Work# ConclusionBy focusing on the use of linearizing passes and by lifting important structural information out of the control flow of the program and into the structure of the data itself, the Co-dfns compiler makes progressive, simple passes that massage the code into its final form, suitable for generation to the target language. This simplicity also results in very compact implementations for each compiler pass, as the mathematically oriented array notations such as APL provide a direct and natural language for these solutions. This concision comes in part because some of the information that might have been explicitly encoded into the control flow of the program instead lives in the AST, which, of course, does not affect the source code of the program directly. The Co-dfns compiler demonstrates not only that a branchless, recursion-free, and vector-friendly compiler can be written, but that such a compiler, while very different, does not require obscure encodings of the same algorithms, instead sitting in its own right with unique algorithms that approach the problem from a different perspective. This presents opportunities both from a pedagogical and practical standpoint. It is not clear that this new perspective will result in faster compilers on GPUs or distributed machines, but at the very least it presents an interesting avenue of research into compiler design and implementation, which results in very compact and concise code  that demonstrates just how generally useful the array oriented programming model could be.