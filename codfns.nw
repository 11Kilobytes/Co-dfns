%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass{article}

\usepackage{noweb}
\pagestyle{noweb}
\noweboptions{longxref}

\usepackage{fontspec}
\usepackage{unicode-math}
\setmainfont[Ligatures=TeX]{Libre Baskerville}
\setsansfont[Ligatures=TeX]{Lucida Sans Unicode}
\setmonofont{APL385 Unicode}
\setmathfont{TeX Gyre Termes Math}

\usepackage{polyglossia}
\setdefaultlanguage[variant=american]{english}

\usepackage{hyperref}
\usepackage{booktabs}

\usepackage[shellescape,latex]{gmp}
\gmpoptions{everymp={input boxes;}}
\usempxpackage[utf8]{inputenc}
\usempxpackage{baskervillef}

\newcommand*\noun[1]{\textsf{#1}}

\begin{document}

@
\title{The Co-dfns Compiler}
\author{Aaron W. Hsu}

\maketitle

\vfill

\noindent
Co-dfns Compiler: High-performance, Parallel APL Compiler\\
Copyright $\copyright$ 2011-2022 Aaron W. Hsu <arcfide@sacrideo.us>
\medskip

\noindent
This program is free software: you can redistribute it and/or
modify it under the terms of the GNU Affero General Public License as
published by the Free Software Foundation, either version 3 of the
License, or (at your option) any later version.
\medskip

\noindent
This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.	See the
GNU Affero General Public License for more details.\medskip

\noindent
You should have received a copy of the GNU Affero General Public License
along with this program.
If not, see http://gnu.org/licenses.
\medskip

\noindent
\emph{This program is available under other license terms. Please contact
Aaron W. Hsu <arcfide@sacrideo.us> for more information.}


\clearpage

\tableofcontents

\clearpage

\section{Introduction}

\subsection{How to Read a WEB}

\section{User's Guide}

\section{Co-dfns Architecture}

This section describes the ``big picture'' parts of the Co-dfns compiler.
The intent here is to try to show how all of the various moving parts
of the compiler fit together,
to provide a sort of road map that will give you a precise plan
for understanding how the various components affect one another.
One of the most important things to understand in any compiler
is the net effect a local change in the code
can have on the rest of the system,
so I hope that this section will help to clarify this.

The design of the Co-dfns compiler is one of austerity and minimalism.
My intent is, was, and hopefully shall remain that of producing an 
exceptionally clear design that avoids or eliminates unnecessary 
code and complexity within the design.
I attack this problem in many ways, but I primarily attempt to do 
this by both reducing the size of the code surface in total,
that is, write less code, as well as reducing the number of entry 
points and paths through that code. 
In other words, my ideal design is one in which you enter the 
compiler in some limited, but well defined and useful set of entry 
points, and then proceed in a linear fashion through the code as the
execution path, resulting finally in your result.
This is the ``ultimate'' in data flow, functionally oriented programming.

The ramifications of this design choice implies a few important things.
Firstly, it implies that I reduce and eliminate any code
that represents boilerplate or that does not actively contribute
to the ``big picture'' of the code.
This is required in an extreme degree if I am to reduce the overall
complexity of the design.
This also implies that there is very little intentional redundancy in 
the shape and style of the source,
making it very terse and compact.
Since there are intentionally very few entry and exit points through 
the control flow of the code, 
this reduces the number of dependencies for me to be aware of when 
dealing with a single piece of code,
but this also comes at the cost of not being able to see many examples
of the interfaces with that code. 
Often, there will be one, and only one place, in which a given piece 
of code is used, and I do not want the code to needlessly store 
excess information in its source that doesn't need to be there.

This all culminates in something that can be quite shocking at first:
making a change to the source is almost always a big deal.
If all the source code is meaningful and carefully constructed,
this also means that changing this code is almost always 
non-trivial, because if the code represented something trivial,
I would have tried to remove it from the code so that only the 
``big things'' were in the code itself.
Thus, anyone who wishes to view and read the compiler code should
take it upon themselves to appreciate the way in which the code flows
together,
and how the flow of the program runs, 
as doing so will be essential to understanding how to make changes to 
the source without breaking something.
Fortunately, this does come with the intended benefits of a very 
short and simple codebase that has clear flow through the system,
it just means that if you want to change something, 
make sure you realize that you are almost always likely to be working
at the ``architectural'' level, rather than at the small and trivial
level of details.

The compiler is designed to fit into a single Dyalog APL namespace,
and importantly, we do not define additional nested namespaces or 
other forms of name hiding. 
I intentionally want to restrict the namespace to a single global one.
This single global namespace should therefore contain the carefully 
curated names that matter, and any that do not matter should, ideally,
not be defined or used.
The namespace itself can be divided into three main groupings:
the public facing entry-points into the system,
the compiler logic itself,
and the utilities or other elements that serve to support the others.
This gives use the following code outline.

<<*>>=
:Namespace codfns

	<<Global Settings>>
	<<The Fix API>>
	<<User-command API>>

	<<Parser>>
	<<Compiler>>
	<<Code Generator>>
	<<Interface to the backend C compiler>>
	<<Linking with Dyalog>>

	<<Must Have APL Utilities>>
	<<Basic [[tie]] and [[put]] utilities>>
	<<The [[opsys]] utility>>
	<<AST Record Structure>>
	<<Converters between parent and depth vectors>>
	<<XML Rendering>>
	<<Pretty-printing AST trees>>

:EndNamespace
@ %def codfns

This [[<<*>>]] chunk is meant to be stored to a file. 
We have a build system for doing this that depends
on the contents of the [[<<Tangle Commands>>]] chunk. 
Thus, we follow the convention here of updating the contents of 
the [[<<Tangle Commands>>]] chunk each time that we initially define
a new chunk that is intended to be output to a file during the 
tangling process. 
See more about the build infrastructure later in this document.

<<Tangle Commands>>=
echo "Tangling src/codfns.apln..."
notangle codfns.nw > src/codfns.apln
@ %def codfns.apln

The primary user-facing interfaces into the compiler are 
[[<<The Fix API>>]] and the [[<<User-command API>>]]. 
These are the ways that you primarily drive the entire compiler.
I intentionally expose the rest of the compiler interfaces
without hiding them so that people who wish to leverage these 
other parts of the system without using the ``entire'' compiler 
pipeline are able to do so, but I do not consider this a public
interface.

This distinction matters because of our testing philosophy and our
version numbering. 
Generally speaking, our version numbering scheme only tracks a major
or minor change in the compiler when the externally facing interfaces
receive some fundamental changes.
Changes to the internal changes are \emph{not} considered for this
versioning scheme.
Moreover, since I intend for there to be great freedom in changing 
and altering the behavior of these internal pipeline interfaces,
these interfaces are not directly tested, 
and the test suite should \emph{not} include testing against these
internal interfaces.
We philosophically only test against the external interfaces,
and eschew internal unit tests.%
\footnote{You can read more of my opinions on this matter
in my article, 
\href{https://www.sacrideo.us/the-fallacy-of-unit-testing/}
{``The Fallacy of Unit Testing''}.}

The utility functions defined below the core compiler pipeline
represent functionality that is tangential to the main compiler
operation.
However, these utilities also tend to represent some specific 
insight into the design of the compiler.
Understanding the core AST structure and design as well as 
getting a grip on how to manipulate the core tree manipulation
structures are vital to understanding the rest of the code.
Therefore, this section spends more time on discussing these
topics before the upcoming sections dealing with a more detailed 
exposition of the compiler itself.
However, there are utilities that we consider more advanced, 
such as the pretty-printing functions and XML rendering
that are topics of interest to advanced users of the compiler,
but which are not part of the main compiler pipeline.
Even though these functions have intentionally general 
application and are likely to be useful not only to those 
working on the compiler itself but also to those who are using 
more advanced compiler features, 
these utilities are not critical to a deep understanding 
of the compiler, 
so these are not discussed in this section. 
Instead, we discuss those topics in the section on developer 
tooling and infrastructure concerns.

The remaining parts of this section will describe the external
facing interfaces to the compiler as well as the core underlying 
data structures and idioms that form the underlying skeleton and 
foundation for writing and working with any aspect of the compiler.
These are all feature and component agnostic elements of the system
that do not belong solely to only a single part, 
but that impact all other elements of the compiler source code,
and so it pays especially well to pay attention and understand
this code to a high degree.

@ \subsection{Global Settings}

There are some global options that we assume to exist throughout
the compiler.
These set the standard behaviors as well as serve as knobs that 
can be tweaked in some cases to identify what behaviors we want 
from the rest of the compiler.

First, we have a set of read-only global constants that are defined
to configure our APL environment.
These are the typical ones, and we try to stick to the defaults,
except that we are sane, and thus we use [[⎕IO]] set to [[0]].

<<Global Settings>>=
⎕IO ⎕ML ⎕WX←0 1 3
@ %def ⎕IO ⎕ML ⎕WX

\noindent
Additionally, we set a [[VERSION]] constant to track changes to the 
system through the distributions.
We use semantic versioning%
\footnote{\href{https://semver.org/}{https://semver.org/}}
as our versioning scheme.
That being said, we also do not have particular qualms about changing
the public API at a rapid pace, provided that we document this.

<<Global Settings>>=
VERSION←4 1 0
@ %def VERSION

\noindent
We depend on ArrayFire%
\footnote{\href{https://arrayfire.com/}{https://arrayfire.com/}}
for much of our GPU backend functionality.
This means we need to know two things,
where ArrayFire is installed
and which ArrayFire backend we should use when compiling.
We only really need to know where ArrayFire is installed on UNIX
style systems, as these systems seem to be much more variable in 
this regard, and there is an environment variable that we can use 
in Windows to find out where ArrayFire is installed more conveniently
on that platform.
We default to using [['cuda']] as our main option, but we also 
support the following options for [[AF∆LIB]]:

\begin{verbatim}
cuda opencl cpu
\end{verbatim}

\noindent
Using [['']] for [[AF∆LIB]] will use ArrayFire's unified backend,
but we don't default to this because we have seen some issues on 
some platforms with reliability problems.
To avoid this, we choose to use [[cuda]] as the default,
which tends to either work or fail explicitly, 
which allows the user to respond rather than crashing 
ungracefully in the case of the unified backend.

The least reliable backend we have seen is the [[opencl]] one, 
which seems to be more hit or miss depending on the underlying
stability of the OpenCL drivers that are installed on the user's
system.
In particular, some Linux OpenCL installations seem to be 
particularly fragile.
In such cases, always make sure that a good, solid OpenCL library
is being used.

<<Global Settings>>=
AF∆PREFIX←'/opt/arrayfire'
AF∆LIB←'cuda'
@ %def AF∆PREFIX AF∆LIB

\noindent
On Windows, we rely on the Visual Studio C/C++ compiler to build
our runtime and user code.
We have settled on trying to stay as up to date with this as 
possible. 
However, there are many different installation paths used by 
Visual Studio, which can make it difficult to know where to look
unless we hardcode each location.
Instead, we assume that Visual Studio will not be a primary 
interest to our users,
making it likely that they will be installing Visual Studio 
only as a dependency for using Co-dfns.
In this case, it is likely that they will be using the Community 
version.
Thus, we default to using the latest version of Visual Studio 
of which we are aware and using the Community version of this,
which Microsoft does not charge for.

If a different version of Visual Studio is installed, then it is 
important to figure out what the right path should be to locate 
the Visual Studio installation. 
The main thing we need to get from this path is access 
to the [[vcvarsall.bat]] batch file.
This file configures the [[cmd.exe]] environment to be able to 
find the Visual Studio compiler and work in the right way.
In the 2002 Community addition, and apparently most new versions 
of Visual Studio, this is located in the [[VC\Auxiliary\Build\]]
subdirectory of the main installation folder.
When changing this path, we want to make sure that the following
path points to the correct [[vcvarsall.bat]] file:

\begin{verbatim}
VS∆PATH,'\VC\Auxiliary\Build\vcvarsall.bat'
\end{verbatim}

\noindent
Most users will simply need to alter [[Community]] to match the 
edition of Visual Studio 2022 that they have installed on their 
system.

<<Global Settings>>=
VS∆PATH←'\Program Files\Microsoft Visual Studio'
VS∆PATH,←'\2022\Community'
@ %def VS∆PATH

\subsection{The Fix API}

One of the core entry points into the compiler is through the [[Fix]]
function.
This function is designed to mimic and more or less replace the
use of the [[⎕FIX]] function found in Dyalog APL.
Its design models that behavior, and it is important as an entry-point
because it exercises most of the core elements of the compiler.
In particular, the design of the compiler's pipeline is demonstrated
most fully in this function.

$$Parse → Compile → Generate → Backend → Link$$

\noindent
The interfaces to the [[⎕FIX]] function and the Co-dfns [[Fix]]
function differ in a few key ways.
The left argument to [[Fix]] is a character vector giving the name
to use when generating files and other artifacts.
This does \emph{not} affect the name of the resulting namespace,
since that is defined, if at all, in the file source itself.
The [[⍺]] argument only affects the name of the files and other
outputs that [[Fix]] generates.

We also print out which part of the compiler we are in when we
enter that ``phase''. Doing this helps to give us an intuitive sense
of how fast each phase is and whether one phase is taking an
abnormally long time or not.
It also helps in debugging.

<<The Fix API>>=
Fix←{
	_←a n s src←PS ⍵⊣⍞←'P'
	_←          TT _⊣⍞←'C'
	_←          GC _⊣⍞←'G'
	_←        ⍺ CC _⊣⍞←'B'
	          n NS _⊣⍞←'L'
}
@ %def Fix

The input requirements for [[Fix]] are not listed in the definition
itself, because both the parser [[PS]] and the [[Fix]] function
need to use the same basic checks,
and since the [[Fix]] function calls the parser
as its first entry point,
it doesn't make much sense to
duplicate that work in both places.
The requirements are as follows:

\begin{itemize}
	\item Scalar/Vector
	\item Character type
	\item Simple or Vector of Vectors
\end{itemize}

\noindent
We generate a [[DOMAIN ERROR]] if the inputs are not well-formed.

<<Verify source input [[⍵]], set [[IN]]>>=
IN←⍵

err←'PARSER EXPECTS SCALAR OR VECTOR INPUT'
1<≢⍴IN:err ⎕SIGNAL 11

err←'PARSER EXPECTS SIMPLE OR VECTOR OF VECTOR INPUT'
2<|≡IN:err ⎕SIGNAL 11

<<Normalize the input formatting>>

err←'PARSER EXPECTS CHARACTER ARRAY'
0≠10|⎕DR IN:err ⎕SIGNAL 11
@

The input formatting that is accepted means that newlines could be
denoted either with [[LF]], [[CR]], or [[CRLF]]
sequences inside of the vectors
themselves or they could be denoted by having separate vectors
for the various lines,
or even a mixture of both.
To simplify this situation we want to normalize them so that we are
always dealing with some combination of [[LF]], [[CR]], and [[CRLF]]
sequences
within the file itself, rather than dealing with the nested
situation.
This ensures that after verification of the input,
everything will work off of the same format.
We intentionally put a newline at the end of the file even if we
may not require one because it is possible that we are dealing
with a file that is missing its final newline.
By always adding one, we ensure that every line in the input
is always terminated by a line ending.
Life is also simpler if we just use LF as our line ending instead
of something else,
this means that future code must be aware that there could be mixed
line endings in the file.

<<Normalize the input formatting>>=
IN←∊(⊆IN),¨⎕UCS 10
@

\subsection{The User Command API}

<<User-command API>>=
∇Z←Help _
 Z←'Usage: <object> <target> [-af={cpu,opencl,cuda}]'
∇

∇r←List
 r←⎕NS¨1⍴⊂⍬ ⋄ r.Name←,¨⊂'Compile' ⋄ r.Group←⊂'CODFNS'
 r[0].Desc←'Compile an object using Co-dfns'
 r.Parse←⊂'2S -af=cpu opencl cuda '
∇

∇ Run(C I);Convert;in;out
⍝ Parameters
⍝	 AF∆LIB	 ArrayFire backend to use
 Convert←{⍺(⎕SE.SALT.Load'[SALT]/lib/NStoScript -noname').ntgennscode ⍵}
 in out←I.Arguments ⋄ AF∆LIB←I.af''⊃⍨I.af≡0
 S←(⊂':Namespace ',out),2↓0 0 0 out Convert ##.THIS.⍎in
 →0⌿⍨'Compile'≢C
 {##.THIS.⍎out,'←⍵'}out Fix S⊣⎕EX'##.THIS.',out
∇
@

\subsection{AST Record Structure}

<<AST Record Structure>>=
f∆←'ptknfsrdx'
N∆←'ABCEFGKLMNOPSVZ'
A B C E F G K L M N O P S V Z←1+⍳15
@

\subsection{Converters between parent and depth vectors}

<<Converters between parent and depth vectors>>=
P2D←{z←⍪⍳≢⍵ ⋄ d←⍵≠,z ⋄ _←{p⊣d+←⍵≠p←⍺[z,←⍵]}⍣≡⍨⍵ ⋄ d(⍋(-1+d)↑⍤0 1⊢⌽z)}
D2P←{0=≢⍵:⍬ ⋄ p⊣2{p[⍵]←⍺[⍺⍸⍵]}⌿⊢∘⊂⌸⍵⊣p←⍳≢⍵}
@

\section{Testing}

We use the \href{https://github.com/Co-dfns/APLUnit}{APLUnit}
testing framework to facilitate our testing of the Co-dfns compiler.
The test harness is designed around a testing philosophy in which we
ever only write black-box tests that work on the whole compiler
using inputs that could be created or are expected to be creatable
by end-users.
That is, we do no ``unit testing'' of our source code,
but only whole program testing.

The testing framework is provided by the [[ut.apln]] file,
which is not part of this literate program and so is not included in
this document.
In order to make some of the testing more convenient,
we define the function [[TEST]] to run the tests
that exist in the [[tests\]] subdirectory.
Each of these tests has a specific number which defines the test,
and we refer to the tests by number when running them.
Both of these testing functions assume that we are running inside
of the [[tests\]] directory or one configured identically to it.

The [[TEST]] function takes either [['ALL']] as its input or a test
number in the form of an integer.
Given an integer, we call the test matching that number in the
current working directory.

The [['ALL']] option causes [[TEST]] to run all of the tests that are
defined in the current working directory.
This command is a nicety, since we can technically do all of this
by iterating the [[TEST]] function over the range of test numbers,
but this would not create the aggregate statistics that we would
like to see at the end of the testing report.
By using [['ALL']] we get to see a complete summary of the
results of testing all the code,
rather than just the individual testing results on a per testing
group/number basis.

<<[[TEST]]>>=
TEST←{
	#.UT.(print_passed print_summary)←1
	'ALL'≡⍵:#.UT.run './'
	path←'./t',(1 0⍕(4⍴10)⊤⍵),'_*_tests.dyalog'
	#.UT.run ⊃⊃0⎕NINFO⍠1⊢path
}
@ %def TEST

The [[TEST]] function is part of the utilities that exist outside
of the [[codfns]] namespace, 
so we define a file for it.

<<Tangle Commands>>=
echo "Tangling src/TEST.aplf..."
notangle -R'[[TEST]]' codfns.nw > src/TEST.aplf
@ %def TEST.aplf

\section{Co-dfns Compiler}

\subsection{Parser}
\label{subsec:parser}

The first, and in many ways, the most complex element of the 
compiler is the parser.
APL has a number of unique issues when it comes to adequately 
parsing the language,
but the most important is handling the context-sensitive 
nature of parsing variables: depending on the type of a variable,
the parse tree can look very different.
To manage this, we make use of a linear, multi-pass style of 
parser in which the parsing process consists of numerous small 
passes over the input, each time refining the input into something
more like the final result.
The parser should take some input that matches the input requirements
of the [[Fix]] function and produce a suitable output AST.

$$PS :: Source → AST × ExportTypes × SymbolTable × Source$$

\noindent
We can think of the parser as starting with a forest of trees, 
each of which contains a single root node that represents a single
character in from the input source,
with all trees arranged in the source order.
During each pass of the parser, we progressively combine these 
trees into more complex trees until we end up at the end with a 
single tree per parsed module.
In other words, we take a fully flat forest of single-node trees 
and progressively increase the depth while reducing the number of 
root-nodes until we have our desired AST structure.

We divide the parsing roughly into two main phases,
the tokenization phase and the parsing phase. 
Unlike most compilers, we don't have a strict division in these 
two phases, so, as they say, think of them more like guidelines
than actual rules%
\footnote{
	\href{https://www.youtube.com/watch?v=WJVBvvS57j0}
		{https://www.youtube.com/watch?v=WJVBvvS57j0}
}.

<<Parser>>=
PS←{
	<<Verify source input [[⍵]], set [[IN]]>>
	
	<<Parsing Constants>>
	<<Line and error reporting utilities>>

	<<Tokenize input>>
	<<Parse token stream>>
	
	<<Compute parser exports>>
	<<Adjust AST for output>>
}
@ %def PS

When parsing, it's very helpful to have names for line endings.

<<Parsing Constants>>=
CR LF←⎕UCS 13 10
@

\subsubsection{Output of the Parser}

After we finish all of our parsing,
we need to take the resulting AST and convert that into something
that is suitable for output to the rest of the system.
We do this in a few ways. 

When we finish parsing, we expect the following fields:

\begin{center}
\begin{tabular}{cl}
\toprule
Field & Description\\
\midrule
[[d]] & Depth vector\\
[[t]] & Node type\\
[[k]] & Node sub-class or ``kind''\\
[[n]] & Name/value field\\
[[pos]] & Starting index for source position\\
[[end]] & Exclussive index for source end position\\
[[xn]] & Names of top-level exported bindings\\
[[xt]] & Types of top-level exported bindings\\
[[sym]] & Symbol Table\\
[[IN]] & Canonical source code\\
\bottomrule
\end{tabular}
\par\end{center}

@ On parser output,
we want to convert the AST to an order that follows a 
depth-first, preorder traversal order, 
so that we can switch from using the parent vector to the depth
vector.
We use this output as our main output because it is space efficient
for storage, and it works well as a canonical form to use.
Because applications may want to only use the parser and not the 
rest of the compiler, 
we want to choose an output format that is suitable for external 
as well as internal use.
This has some performance overheads,
but it is probably worth it regardless,
as reordering at this point to allow a depth vector enables some 
nice assumptions in the rest of the compiler.
We use the [[P2D]] utility to reorder all of our AST columns.
Note that things like the exported bindings and the symbol table 
are not strictly part of the AST structure, because they are of a 
different length and type than the other columns.

<<Adjust AST for output>>=
d i←P2D p ⋄ d n t k pos end I∘⊢←⊂i
@

\phantomsection
\label{topic:symbolizenfield}
There is an inefficiency in the AST representation at this point,
where the [[n]] field contains character vectors.
This inefficiency was necessary while building up the AST because
we were not sure what symbols would be created
before we parsed them,
but at this point, we know the full set of symbols that we have in 
the AST.
This means that we can convert the [[n]] field to a symbol table 
representation.
In this case, we want the [[n]] field to pair with a [[sym]] list
that contains all the unique symbols in the source.
We want [[old_n≡sym[|new_n]]] to hold for this new [[n]] field.
In other words, we want the new [[n]] field to contain negative 
integers whose magnitudes are valid indices into the [[sym]] 
symbol table.
This means that there is only one character vector per unique symbol
or numeric literal in the source code, which can greatly reduce 
memory usage.
Moreover, it is much faster to compare symbols that are represented by 
numeric index rather than character vector.
Most of the work we expect to be done on the [[n]] field, so that we 
never have to pull in [[sym]] unless we want to know the actual value 
of the symbol.
This actually mimics the feature of symbols in other languages
like Scheme,
but it comes with an additional efficiency benefit in that we do not
require the use of a full generalized pointer to represent a symbol
if we have fewer symbols. 
This means that we are very likely only going to need a single byte 
or a couple of bytes per symbol to represent it in the [[n]] field.

The choice to make all of our symbols negative in value is somewhat 
strange, but we have a good reason for doing so.
The [[n]] field is a single field that we use to contain general 
data for every node, and as such, it represents a sort of union 
type of all sorts of different data.
In particular, we also want to be able to support using the [[n]] 
field to point to other nodes in the AST, which is a feature we 
rely heavily on in the compiler transformations.
However, this feature would conflict with using the [[n]] field as an 
index into the [[sym]] table, rather than as an index into the AST.
By making symbol pointers negative, we put them into a separate 
space than the positive AST node pointers, allowing us to store 
both pointers in the same field. This may seem like a little bit of 
a strange hack, but it actually makes reasoning about things a little 
easier, because we can tend to think of [[n]] as a name, even if that 
name is pointing to an AST or a symbol, and avoids needless space 
duplication or the need to remember to update multiple fields that are 
only relevant for some nodes.

We map the $0$th index to be a null or empty symbol. 
We also want to reserve the first four symbol slots $[1,4]$
so that they will \emph{always} refer to the same symbols, 
namely, [[⍵]], [[⍺]], [[⍺⍺]], and [[⍵⍵]]. 

This gives us the following definitions for [[sym]] and [[n]].

<<Adjust AST for output>>=
sym←∪('')(,'⍵')(,'⍺')'⍺⍺' '⍵⍵',n
n←-sym⍳n
@

Finally, we want to return our AST structure in a meaningful way.
Logically, we have the AST proper, which consists of these fields:

\begin{verbatim}
d t k n pos end
\end{verbatim}

\noindent
The above fields are returned as an inverted table,
where each column is a vector of the same length.
We also want to return the variable environment,
which gives the names of our top-level bindings and their types,
also as an inverted table.
Finally, we must return a canonical representation of the source 
code that is suitable as an indexing target for the [[pos]] and [[end]]
fields, as well as the symbol table.
Thus, we have a four element vector as the return value:

\begin{verbatim}
AST TopBindingTypes SymbolTable InputSource
\end{verbatim}

\noindent
Which gives us the following return value.

<<Adjust AST for output>>=
(d t k n pos end)(xn xt)sym IN
@

\subsubsection{Handling Parsing Errors}

<<Line and error reporting utilities>>=
linestarts←(⍸1⍪2>⌿IN∊CR LF)⍪≢IN
mkdm←{⍺←2 ⋄ line←linestarts⍸⍵ ⋄ no←'[',(⍕1+line),'] '
	i←(~IN[i]∊CR LF)⌿i←beg+⍳linestarts[line+1]-beg←linestarts[line]
	(⎕EM ⍺)(no,IN[i])(' ^'[i∊⍵],⍨' '⍴⍨≢no)}
quotelines←{
	lines←∪linestarts⍸⍵
	nos←(1 0⍴⍨2×≢lines)⍀'[',(⍕⍪1+lines),⍤1⊢'] '
	beg←linestarts[lines] ⋄ end←linestarts[lines+1]
	m←∊∘⍵¨i←beg+⍳¨end-beg
	¯1↓∊nos,(~∘CR LF¨⍪,(IN∘I¨i),⍪' ▔'∘I¨m),CR}
SIGNAL←{⍺←2 '' ⋄ en msg←⍺ ⋄ EN∘←en ⋄ DM∘←en mkdm ⊃⍵
	dmx←('EN' en)('Category' 'Compiler')('Vendor' 'Co-dfns')
	dmx,←⊂'Message'(msg,CR,quotelines ⍵)
	⎕SIGNAL⊂dmx}
@ %def SIGNAL quotelines mkdm linestarts

\subsubsection{Tokenizing the Input}

<<Tokenize input>>=
⍝ Group input into lines as a nested vector
pos←(⍳≢IN)⊆⍨~IN∊CR LF

<<Mask potential strings>>
<<Remove comments>>
<<Check for unbalanced strings>>
<<Flatten parser representation>>
<<Tokenize strings>>
<<Convert [[⋄]] to [[Z]] nodes>>
<<Define character classes>>
<<Remove insignificant whitespace>>
<<Verify that all open characters are valid>>

x←IN[pos]
<<Tokenize numbers>>
<<Tokenize variables>>
<<Tokenize primitives and atoms>>
<<Compute dfns regions and type, with [[}]] as a child>>
<<Check for out of context dfns formals>>
<<Compute trad-fns regions>>
<<Identify label colons vs. others>>
<<Tokenize keywords>>
<<Tokenize system variables>>

⍝ Delete all characters we no longer need from the tree
d tm t pos end(⌿⍨)←⊂(t≠0)∨x∊'()[]{}:;'

<<Tokenize labels>>
@

\subsubsection{Parsing Token Stream}

<<Parse token stream>>=
⍝ Now that all compound data is tokenized, reify n field before tree-building
n←{1↓⍎¨'0',⍵}@{t=N}(⊂'')@{t∊Z F}1 ⎕C@{t∊K S}IN∘I¨pos+⍳¨end-pos
<<Type-specific processing of the [[n]] field>>

<<Check that all keywords are valid>>
<<Check that namespaces are at the top level>>
<<Verify that all structured statements appear within trad-fns>>
<<Verify that system variables are defined>>

⍝ Compute parent vector from d
p←D2P d

<<Compute the nameclass of dfns>>

⍝ We will often wrap a set of nodes as children under a Z node
gz←{
	z←⍵↑⍨-0≠≢⍵ ⋄ ks←¯1↓⍵
	t[z]←Z ⋄ p[ks]←⊃z ⋄ pos[z]←pos[⊃⍵] ⋄ end[z]←end[⊃⌽z,ks]
	z
}

<<Nest top-level root lines as [[Z]] nodes>>
<<Wrap all dfns expression bodies as [[Z]] nodes>>

⍝ Drop/eliminate any Z nodes that are empty or blank
_←p[i]{msk[⍺,⍵]←~∧⌿IN[pos[⍵]]∊WS}⌸i←⍸(t[p]=Z)∧p≠⍳≢p⊣msk←t≠Z
tm n t k pos end(⌿⍨)←⊂msk ⋄ p←(⍸~msk)(⊢-1+⍸)msk⌿p

<<Parse [[:Namespace]] syntax>>
<<Parse guards to [[(G (Z ...) (Z ...))]]>>
<<Parse brackets and parentheses into [[¯1]] and [[Z]] nodes>>
<<Convert [[;]] groups within brackets into [[Z]] nodes>>
<<Parse Binding nodes>>
<<Mark system variables as [[P]] nodes with appropriate kinds>>
<<Mark atoms, characters, and numbers as kind [[1]]>>
<<Mark APL primitives with appropriate kinds>>
<<Anchor variables to earliest binding in the matching frame>>
<<Convert [[M]] nodes to [[F0]] nodes>>
<<Convert [[⍺]] and [[⍵]] to [[V]] nodes>>
<<Convert [[⍺⍺]] and [[⍵⍵]] to [[P2]] nodes>>
<<Infer the type of bindings, groups, and variables>>
<<Strand arrays into atoms>>
<<Parse dyadic operator bindings>>
<<Rationalize [[F[X]]] syntax>>
<<Group function and value expressions>>
<<Parse function expressions>>
<<Parse assignments>>
<<Enclose [[V[X;...]]] for expression parsing>>
<<Parse trains>>
<<Parse value expressions>>
<<Rationalize [[V[X;...]]]>>

⍝ Sanity check
ERR←'INVARIANT ERROR: Z node with multiple children'
ERR assert(+⌿(t[p]=Z)∧p≠⍳≢p)=+⌿t=Z:

⍝ Count parentheses in source information
ip←p[i←⍸(t[p]=Z)∧n[p]∊⊂,'('] ⋄ pos[i]←pos[ip] ⋄ end[i]←end[ip]

⍝ VERIFY Z/B NODE TYPES MATCH ACTUAL TYPE

⍝ Eliminate Z nodes from the tree
zi←p I@{t[p[⍵]]=Z}⍣≡ki←⍸msk←(t[p]=Z)∧t≠Z
p←(zi@ki⍳≢p)[p] ⋄ t k n pos end(⊣@zi⍨)←t k n pos end I¨⊂ki
t k n pos end⌿⍨←⊂msk←~msk∨t=Z ⋄ p←(⍸~msk)(⊢-1+⍸)msk⌿p
@

\subsection{Compiler Transformations}

<<Compiler>>=
TT←{
	((d t k n ss se)exp sym src)←⍵
	
	⍝ Compute parent vector and reference scope
	r←I@{t[⍵]≠F}⍣≡⍨p⊣2{p[⍵]←⍺[⍺⍸⍵]}⌿⊢∘⊂⌸d⊣p←⍳≢d

	<<Lift dfns to the top-level>>
	<<Wrap expressions as binding or return statements>>
	<<Lift guard tests>>
	<<Count strand and indexing children>>
	<<Lift and flatten expressions>>
	<<Compute slots and frames>>
	<<Record exported top-level bindings>>
	<<Namify pointer variables>>
	<<Merge [[V]] nodes into [[n]] fields>>
	
	p t k n f s r d xi sym
}
@
	
\subsection{Code Generator}

<<Map generators over the linearized AST; return>>=
d i←P2D p ⋄ ast←(⍉↑d p t k n(⍳≢p)fr sl fd)[i;] ⋄ ks←{⍵⊂[0]⍨(⊃⍵)=⍵[;0]}
NOTFOUND←{('[GC] UNSUPPORTED NODE TYPE ',N∆[⊃⍵],⍕⊃⌽⍵)⎕SIGNAL 16}
dis←{0=2⊃h←,1↑⍵:'' ⋄ (≢gck)=i←gck⍳⊂h[2 3]:NOTFOUND h[2 3] ⋄ h(⍎i⊃gcv)ks 1↓⍵}
∊,∘(⎕UCS 13 10)¨pref,⊃,⌿(,⌿Zp¨⍸t=F),(,⌿Zx¨xi),(⊂⊂''),dis¨ks ast
@

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms←0⍴⊂'' ⋄ nams←0⍴⊂''
@

<<Node $\longleftrightarrow$ Generator mapping>>=
gck←0⍴⊂0 0 ⋄ gcv←0⍴⊂''
@

<<Prefix code for all generated files>>=
pref ←⊂'#include "codfns.h"'
pref,←⊂''
pref,←⊂'EXPORT int'
pref,←⊂'DyalogGetInterpreterFunctions(void *p)'
pref,←⊂'{'
pref,←⊂'	return set_dwafns(p);'
pref,←⊂'}'
pref,←⊂''
@

<<Node-specific code generators>>=
Zp←{
	n←'fn',⍕⍵
	<<Declare top-level function bindings>>
	'UNKNOWN FUNCTION TYPE'⎕SIGNAL 16
}
@

<<Node-specific code generators>>=
Zx←{
	n←sym⊃⍨|n[⍵] ⋄ rid←⍕rf[⍵]
	k[⍵]=0:⊂''
	<<Declare top-level array structures>>
	<<Declare top-level closures>>
	⍎'''UNKNOWN EXPORT TYPE''⎕SIGNAL 16'
}
@

<<Code Generator>>=
GC←{
	p t k n fr sl rf fd xi sym←⍵

	<<Symbol $\longleftrightarrow$ Name mapping>>
	<<Node $\longleftrightarrow$ Generator mapping>>

	<<Prefix code for all generated files>>

	<<Variable utilities>>
	<<Node-specific code generators>>

	<<Map generators over the linearized AST; return>>
}
@

\subsection{Backend C Compiler Interface}

<<Interface to the backend C compiler>>=
CC←{
	vsbat←VS∆PATH,'\VC\Auxiliary\Build\vcvarsall.bat'
	soext←{opsys'.dll' '.so' '.dylib'}
	libdir←opsys ''' ' '/lib64'' ' '/lib'' '
	ccf←{' -o ''',⍵,'.',⍺,''' ''',⍵,'.c'' -laf',AF∆LIB,' > ',⍵,'.log 2>&1'}
	cci←{'-I''',AF∆PREFIX,'/include'' -L''',AF∆PREFIX,libdir}
	cco←'-std=c99 -Ofast -g -Wall -fPIC -shared '
	cco,←'-Wno-parentheses -Wno-misleading-indentation '
	ucc←{⍵⍵(⎕SH ⍺⍺,' ',cco,cci,ccf)⍵}
	gcc←'gcc'ucc'so'
	clang←'clang'ucc'dylib'
	vsco←{z←'/W3 /wd4102 /wd4275 /O2 /Zc:inline /Zi /FS /Fd"',⍵,'.pdb" '
		z,←'/WX /MD /EHsc /nologo '
		z,'/I"%AF_PATH%\include" /D "NOMINMAX" /D "AF_DEBUG" '}
	vslo←{z←'/link /DLL /OPT:REF /INCREMENTAL:NO /SUBSYSTEM:WINDOWS '
		z,←'/LIBPATH:"%AF_PATH%\lib" /OPT:ICF /ERRORREPORT:PROMPT /TLBID:1 '
		z,'/DYNAMICBASE "af', AF∆LIB, '.lib" "codfns.lib" '}
	vsc0←{~⎕NEXISTS vsbat:'VISUAL C?'⎕SIGNAL 99 ⋄ '""',vsbat,'" amd64'}
	vsc1←{' && cd "',(⊃⎕CMD'echo %CD%'),'" && cl ',(vsco ⍵),' "',⍵,'.c" '}
	vsc2←{(vslo ⍵),'/OUT:"',⍵,'.dll" > "',⍵,'.log""'}
	vsc←{⎕CMD ('%comspec% /C ',vsc0,vsc1,vsc2)⍵}
	_←(⍎opsys'vsc' 'gcc' 'clang')⍺⊣⍵ put ⍺,'.c'⊣1 ⎕NDELETE f←⍺,soext⍬
	⎕←⍪⊃⎕NGET(⍺,'.log')1
	⎕NEXISTS f:f ⋄ 'COMPILE ERROR' ⎕SIGNAL 22}
@

\subsection{Linking with Dyalog}

<<Linking with Dyalog>>=
 NS←{
	MKA←{mka⊂⍵} ⋄ EXA←{exa ⍬ ⍵}
	Display←{⍺←'Co-dfns' ⋄ W←w_new⊂⍺ ⋄ 777::w_del W
	 w_del W⊣W ⍺⍺{w_close ⍺:⍎'⎕SIGNAL 777' ⋄ ⍺ ⍺⍺ ⍵}⍣⍵⍵⊢⍵}
	LoadImage←{⍺←1 ⋄ ~⎕NEXISTS ⍵:⎕SIGNAL 22 ⋄ loadimg ⍬ ⍵ ⍺}
	SaveImage←{⍺←'image.png' ⋄ saveimg ⍵ ⍺}
	Image←{~2 3∨.=≢⍴⍵:⎕SIGNAL 4 ⋄ (3≠⊃⍴⍵)∧3=≢⍴⍵:⎕SIGNAL 5 ⋄ ⍵⊣w_img ⍵ ⍺}
	Plot←{2≠≢⍴⍵:⎕SIGNAL 4 ⋄ ~2 3∨.=1⊃⍴⍵:⎕SIGNAL 5 ⋄ ⍵⊣w_plot (⍉⍵) ⍺}
	Histogram←{⍵⊣w_hist ⍵,⍺}
	Rtm∆Init←{
		_←'w_new'⎕NA'P ',⍵,'|w_new <C[]'
		_←'w_close'⎕NA'I ',⍵,'|w_close P'
		_←'w_del'⎕NA ⍵,'|w_del P'
		_←'w_img'⎕NA ⍵,'|w_img <PP P'
		_←'w_plot'⎕NA ⍵,'|w_plot <PP P'
		_←'w_hist'⎕NA ⍵,'|w_hist <PP F8 F8 P'
		_←'loadimg'⎕NA ⍵,'|loadimg >PP <C[] I'
		_←'saveimg'⎕NA ⍵,'|saveimg <PP <C[]'
		_←'exa'⎕NA ⍵,'|exarray >PP P'
		_←'mka'⎕NA'P ',⍵,'|mkarray <PP'
		_←'FREA'⎕NA ⍵,'|frea P'
		_←'Sync'⎕NA ⍵,'|cd_sync'
		0 0 ⍴ ⍬}
	mkna←{⍺,'|',('∆'⎕R'__'⊢⍵),'_cdf P P P'}
	mkf←{
		fn←⍺,'|',('∆'⎕R'__'⊢⍵),'_dwa '
		z←⊂'Z←{A}',⍵,' W'
		z,←⊂':If 0=⎕NC''⍙.',⍵,'_mon'''
		z,←⊂'	''',⍵,'_mon''⍙.⎕NA''',fn,'>PP P <PP'''
		z,←⊂'	''',⍵,'_dya''⍙.⎕NA''',fn,'>PP <PP <PP'''
		z,←⊂':EndIf'
		z,←⊂':If 0=⎕NC''A'''
		z,←⊂'	Z←⍙.',⍵,'_mon 0 0 W'
		z,←⊂':Else'
		z,←⊂'	Z←⍙.',⍵,'_dya 0 A W'
		z,←⊂':EndIf'
		z
	}
	ns←#.⎕NS⍬ ⋄ _←'∆⍙'ns.⎕NS¨⊂⍬ ⋄ ∆ ⍙←ns.(∆ ⍙)
	∆.names←(0⍴⊂''),(2=1⊃⍺)⌿0⊃⍺
	fns←'Rtm∆Init' 'MKA' 'EXA' 'Display'
	fns,←'LoadImage' 'SaveImage' 'Image' 'Plot' 'Histogram'
	fns,←'soext' 'opsys' 'mkna'
	_←∆.⎕FX∘⎕CR¨fns
	∆.(decls←⍵∘mkna¨names)
	_←ns.⎕FX¨(⊂''),⍵∘mkf¨∆.names
	_←'Z←Init'
	_,←⊂'Z←Rtm∆Init ''',⍵,''''
	_,←⊂'→0⌿⍨0=≢names'
	_,←⊂'names ##.⍙.⎕NA¨decls'
	_←∆.⎕FX _
	ns
}
@

<<DWA Function Export>>=
z,←⊂'EXPORT int'
z,←⊂n,'_dwa(struct localp *zp, struct localp *lp, struct localp *rp)'
z,←⊂'{'
z,←⊂'	struct array *z, *l, *r;'
z,←⊂'	int err;'
z,←⊂''
z,←⊂'	l = NULL;'
z,←⊂'	r = NULL;'
z,←⊂''
z,←⊂'	fn',rid,'(NULL, NULL, NULL, NULL);'
z,←⊂''
z,←⊂'	err = 0;'
z,←⊂''
z,←⊂'	if (lp)'
z,←⊂'		err = dwa2array(&l, lp->pocket);'
z,←⊂''
z,←⊂'	if (err)'
z,←⊂'		dwa_error(err);;'
z,←⊂''
z,←⊂'	if (rp)'
z,←⊂'		dwa2array(&r, rp->pocket);'
z,←⊂''
z,←⊂'	if (err) {'
z,←⊂'		release_array(l);'
z,←⊂'		dwa_error(err);'
z,←⊂'	}'
z,←⊂''
z,←⊂'	err = (',n,'->fn)(&z, l, r, ',n,'->fv);'
z,←⊂''
z,←⊂'	release_array(l);'
z,←⊂'	release_array(r);'
z,←⊂''
z,←⊂'	if (err)'
z,←⊂'		dwa_error(err);'
z,←⊂''
z,←⊂'	err = array2dwa(NULL, z, zp);'
z,←⊂'	release_array(z);'
z,←⊂''
z,←⊂'	if (err)'
z,←⊂'		dwa_error(err);'
z,←⊂''
z,←⊂'	return 0;'
z,←⊂'}'
z,←⊂''
@

\subsection{APL Runtime Architecture}

The runtime component of Co-dfns
handles the code necessary for the output
of the Code Generator to run.
This includes support for all the supported language features
as well as the runtime code for the built-in APL primitives
and system functionality.
The design of the runtime is meant to allow
for as much of the runtime as possible
to be implemented in APL.
We also want to make it as easy as possible to target
new languages for output from the compiler.

Conceptually, the code generator produces a code module
that links against an already built runtime module that
provides all the language support.
Each module has some ``backend target'' language.
In order to make retargeting the compiler as simple as possible
and to implement most of the runtime as APL,
we split the runtime code into an APL namespace,
containg all the APL code that is applicable to all backends
and that can be implemented in APL,
and a backend kernel that contains all the backend
language-specific code that we must use.
We can split the compiler into a frontend \emph{generate}
and a backend \emph{build} step.
The generate phase takes the input APL source
and generates code in the backend target language
that depends on a runtime implementation.
The build phase takes that code and uses the backend toolchain
to link, compile, and otherwise assemble the code
into an appropriate redistributable ``binary''.
The C backend, for instance, takes APL and turns it
into C code where a C compiler then builds and links it
against a runtime, finally producing a DLL.

To build the runtime, the same basic approach is used.
We use the compiler to generate a backend file
from the APL runtime code.
However, since no runtime exists for the runtime itself,
we do \emph{not} continue in the typical manner
and build with the standard backend pipeline,
which assumes the existence of a runtime.
Instead, we merge the generated code with 
the kernel for that specific backend and build
as its own standalone object.

This workflow is illustrated by Figure \ref{fig:runtimeworkflow}
showing how all of the pieces of the runtime interact with user code.

\begin{figure}[htp]
\centering
\begin{mpost}
boxjoin(a.s = b.n + (0,0.5in));
boxit.uc(\btex User APL Code etex);
boxit.bk(\btex Generated Code etex);
boxit.ns(\btex Module etex);
drawboxed(uc, bk, ns);
boxjoin(a.s = b.n + (0,0.5in));
boxit.ap(\btex APL Runtime Code etex);
boxit.rc(\btex Generated Code etex);
boxit.rtm(\btex Runtime etex);
uc.e + (0.5in,0) = ap.w;
drawboxed(ap, rc, rtm);
boxjoin();
boxit.kk(\btex Kernel etex);
kk.w = rc.e + (1pc, 0);
drawboxed(kk);
drawarrow uc.s -- bk.n;
drawarrow bk.s -- ns.n;
drawarrow ap.s -- rc.n;
draw rc.e -- kk.w;
drawarrow (rc.e + (0.5pc,0))..(rc.e + (0.5pc,-0.1in))..(rtm.n + (0,0.1in))..rtm.n;
drawdblarrow ns.e -- rtm.w;
numeric spc, lft, rgt, mid;
spc := 0.5 * ypart(uc.w - bk.w);
lft := xpart bk.w;
rgt := xpart kk.e;
mid := ypart bk.w;
draw (lft, mid + spc)..(rgt, mid + spc) dashed evenly;
draw (lft, mid - spc)..(rgt, mid - spc) dashed evenly;
label.lft(\btex{\itshape Generate} etex, (lft, mid + spc));
label.lft(\btex{\itshape Build} etex, (lft, mid - spc));
label.top(\btex{\itshape Link} etex, (ns.e + (0.5 * xpart(rtm.w - ns.e), 0)));
\end{mpost}
\caption{Process of Building and Linking the Runtime}
\label{fig:runtimeworkflow}
\end{figure}

This architecture has some interesting advantages.
First, most of the process for building the runtime
is just like building any other piece of APL code.
Second, only a small kernel and code generator need to be implemented
for a new backend,
with most of the work remaining in the APL runtime code.
Third, the runtime may be implemented using a different backend language
than that used for compiling the user code.
All that is required is that the backend for the user code
knows how to link to and access the code in the runtime object.
This permits, for instance, a Scheme or Javascript backend
to depend on a runtime implemented in C,
thus enabling greater performance while hiding any 
integration hassles from the interface exposed by the user module.
In theory, any combination of suitable backend languages may be used.

We put all the runtime primitives
into a single Co-dfns namespace called [[prim.apln]].

<<prim.apln>>=
:Namespace prim

	<<APL Primitives>>
	<<System Primitives>>

:EndNamespace
@ %def prim.apln prim

<<Tangle Commands>>=
echo "Tangling rtm/prim.apln..."
notangle -R'prim.apln' codfns.nw > rtm/prim.apln
@

Each primitive has its own unique considerations,
so we leave the definition of these primitives
to section \ref{sec:primitives}.

For each backend we must have a unique kernel and code generator.
Most of that content will be defined on a per-language feature basis below.
The rest of this section focuses
on the more generic and fundamental elements
of the kernels,
such as general organization, interface, and memory management.

\subsection{GPU C Runtime Kernel}

The main concern of a C runtime is
managing memory and adequately handling access to the DWA system.
Dyalog's DWA system permits us direct access to the
underlying interpreter array format and memory manager.
We could use this format directly
but this will not work for GPU compute
because the DWA interface connects
array elements and header information
in a way that makes GPU allocating them quite difficult,
especially if we only want the elements on the GPU.

DWA has a specific array format,
but we will delay specifying utility code
for array handling until Section \ref{sec:langfeatarrays}.
In this section, we handle the following issues:

\begin{itemize}
	\item DWA Initialization
	\item Header Structure
	\item Memory Management
	\item Datatype Management
	\item Error Reporting
\end{itemize}

We deal with the top-level error signalling 
behavior in this section, but for error signalling 
within functions, as well as arrays, module initialization
function calls, and so forth, see the appropriate 
sub-section of Language Features
(Section \ref{sec:langfeats}).

\subsubsection{The C Header}

The first order of business is the main structure
of the C runtime files and API.
We could attempt to put all our runtime code
into a single [[kernel.c]] file,
but the result would require us to maintain includes
in a way that prevents us from easily linking
the include statements to each language feature implementation
without encouraging needless duplicate includes.
Instead, we assume that each langauge feature will be given
its own C file and then we can manage includes independently.
We will make use of a single [[codfns.h]] file
that contains all the public entry points into the runtime.

<<codfns.h>>=
#pragma once
<<C runtime includes>>
<<C runtime macros>>
<<C runtime enumerations>>
<<C runtime structures>>
<<C runtime declarations>>
@ %def codfns.h

<<Tangle Commands>>=
echo "Tangling rtm/codfns.h..."
notangle -R'codfns.h' codfns.nw > rtm/codfns.h
@

Since we want to use this single header
for the runtime code \emph{and} the generated code
that will import the runtime,
an interesting situation arises regarding exports.
Both generated and runtime code must export functions
from their respective DLLs,
but in the case of the runtime,
these exported functions are also the functions
that we must import into our generated code,
we must annotate the edeclaration of such functions
differently if we are importing
than when we are exporting.
Thus, when we are building the runtime, 
we want to export all our bindings,
but when we are accessing the runtime from generated code
we want to import those same bindings
while exporting functions that we generate.

To handle this, we rely on three preprocessor definitions.
When we are building the runtime,
we will define [[EXPORTING]],
but we expect this to be undefined when building generated code.
Then we have an [[EXPORT]] definition
that always maps to the platform specific export decorator,
while [[DECLSPEC]] will be the import spec or export spec
depending on [[EXPORTING]].

It used to be the case
that each platform handled DLL importing and exporting differently,
but modern compilers all handle the [[__declspec]] syntax,
so we will use that for all platforms.

<<C runtime macros>>=
#define EXPORT __declspec(dllexport)
#ifdef EXPORTING
	#define DECLSPEC EXPORT
#else
	#define DECLSPEC __declspec(dllimport)
#endif
@ %def EXPORT EXPORTING DECLSPEC

\subsubsection{Memory and Datatype Management}

Next, we deal with handling memory and multiple data types.
Since the compiler assumes a stack machine model,
we have a unified stack that will contain many different objects,
such as functions and arrays,
so we must have a way of handling the objects
in a somewhat generic way.

While some generality is desirable,
I must curtail my Scheme-esque impulse
towards unnecessary dynamic generality.
This is a runtime, after all,
and experience shows that extra
dynamic annotation can seriously impede scalability
of the system and introduce unfortunate performance gotchas.
Rather than chase this form of programmability,
I am taking a page from Knuth's book
and aiming for ``re-editable'' code 
that can be easily, but statically, extended.
The goal is to avoid excess runtime allocation and indirection
while at the same time making it easy to add and manage datatypes.

Any such memory or type management system
must address the following questions:

\begin{itemize}
	\item How do I make an object?
	\item How do I free an object?
	\item When do I free an object?
	\item How do I keep an object alive?
	\item How do I make new data types?
\end{itemize}

In APL, most values have a stack lifetime,
which would encourage us to make use
of a stack semantics in our runtime.
However, for more involved APL,
this assumption does not hold true.
Instead, to manage our objects,
we choose to make use of reference counting.%
\footnote{\url{https://en.wikipedia.org/wiki/Reference\_counting}}
This maintains most of the predictability and low-overhead
of a stack semantics but gives us the additional power
to allow object lifetimes to extend beyond the lifetime
of their definition context.

We do not have a requirement in our system for generic object creation
(indeed, such a requirement is quite rare), 
but we do need to generically retain a reference
to an object and to release an object.
We want to enable this
without too much indirection.
To implement this, we simply require
that all our datatypes be structures that share
the following common fields. 
We call these types \noun{cells} as a convenient term.

<<Common cell fields>>=
enum cell_type ctyp;
unsigned int refc;
@ %def ctyp refc

These fields help us to answer the two most important questions
we must answer for any cell:
what type of cell is it; and, is it currently referenced?
By requiring all data structs to have these fields in common,
we can cast them about and be basolutely sure
that things will continue to work.
We define a ``void'' cell type [[struct cell_void]]
to be our minimal cell type.

<<C runtime structures>>=
struct cell_void {
	<<Common cell fields>>
};
@ %def cell_void

The [[enum cell_type]] keeps track of all known cell types.

<<C runtime enumerations>>=
enum cell_type {
	<<Cell type names>>
};
@ %def cell_type

We set the first $0th$ cell type to our void cell.

<<Cell type names>>=
CELL_VOID,
@ %def CELL_VOID

We do not make or define any generic way
to create cells; 
you must make a constructor function suitable
to the needs of the data type.
At the moment, it is the responsibility of such makers
to ensure that the common fields are 
appropriately initialized.
A maker should return a [[0]] on success
and a non-zero error on failure.
It should also take a [[struct cell_TYPE **]]
as the first argument to store the allocated cell in.
We expect the slot passed to a creator
will be a possibly previously utilized slot
on a stack or something along these lines.
This means that it is the caller's responsibility
to ensure that this slot has already been released.
Failure to do this would potentially lead to a memory leak.
However, attempting to handle this within the cell maker function
results in an API that is much too fragile and needlessly complex.
We expect to generally follow the stylistic guideline 
that a function should allocate and own its own data
and then release that data in the same function.

The basic cell maker for the [[void]] cell type looks like this:

<<Cell definitions>>=
DECLSPEC int
mk_void(struct cell_void **cell)
{
	struct cell_void *ptr;

	ptr = malloc(sizeof(struct cell_void));

	if (ptr == NULL)
		return 1;

	ptr->ctyp = CELL_VOID;
	ptr->refc = 1;
	*cell = ptr;

	return 0;
}
@ %def mk_void

A few points of style here.
The error codes should try to follow the standard APL codes.
Additionally, the target slot should not be mutated
until we are sure that all is well 
and that the object is well-formed.

<<C runtime declarations>>=
DECLSPEC int mk_void(struct cell_void **);
@

While we must define unique constructors
for the various types,
when releasing or freeing a cell of some kind,
we \emph{do} want to be able to generically free a cell.
However, this must be done with a minimum of runtime overhead.
First, we distinguish the terms ``release'' and ``free''.
If an object is freed, that object's memory is fully returned
to the memory manager,
whereas releasing is about reducing the number of references
to that object.
When a cell has no references to it, then it is freed.

Each cell type will require its own unique release function
that manages cleanly destroying the cell.
The release function for the [[void]] cell type looks like this:

<<Cell definitions>>=
DECLSPEC void
release_void(struct cell_void *cell)
{
	if (cell == NULL)
		return;

	if (--cell->refc)
		return;

	free(cell);
}
@ %def release_void

<<C runtime declarations>>=
DECLSPEC void release_void(struct cell_void *);
@

To support generic cell release,
we define a [[release_cell]] function.

<<Cell definitions>>=
DECLSPEC void
release_cell(void *cell)
{
	if (cell == NULL)
		return;

	switch (((struct cell_void *)cell)->ctyp) {
	<<Cell release cases>>
	default:
		dwa_error(99);
	}
}
@ %def release_cell

<<C runtime declarations>>=
DECLSPEC void release_cell(void *);
@

For each cell type, we must plug the type-specific release function
into this [[release_cell]] switch to enable generic releasing
for that type.
For the [[void]] type, this looks as follows:

<<Cell release cases>>=
case CELL_VOID:
	release_void(cell);
	break;
@

The above mostly suffices for dealing with cells.
However, we also want to conveniently bump
the reference count of a cell seamlessly
without explicitly setting [[refc]].
We often encounter the case where we are assigning a cell
to a new slot,
thus requiring a reference count increment.
The following function [[retain_cell]] lets us do this
in a single statment by writing:

\begin{verbatim}
slot2 = retain_cell(slot1);
\end{verbatim}

<<Cell definitions>>=
DECLSPEC void *
retain_cell(void *cell)
{
	if (cell != NULL)
		((struct cell_void *)cell)->refc++;

	return cell;
}
@ %def retain_cell

<<C runtime declarations>>=
DECLSPEC void *retain_cell(void *);
@

Fortunately, this retention function requires no extra code
as we extend the system with more data types.
This gives us the following steps if we want
to add a new data type to the runtime:

\begin{enumerate}
	\item Add the cell type to [[<<Cell type names>>]]
		as [[, CELL_TYPE]].
	\item Define the structure in [[<<C runtime structures>>]],
		making sure that [[<<Common cell fields>>]]
		are the first fields.
	\item Define an [[int mk_type(struct cell_type **, ...)]]
		function and declare it in
		[[<<C runtime declarations>>]].
	\item Define a [[void release_type(struct cell_type *)]]
		function and declare it in 
		[[<<C runtime declarations>>]].
	\item Add a case to [[<<Cell release cases>>]] 
		on [[CELL_TYPE]] that calls [[release_type]] on
		[[cell]].
\end{enumerate}

\noindent
When defining new maker and releaser functions for a cell type,
there are a few behaviors we want to be sure to implement.
These behaviors help to maximize the reliability of the functions.

\begin{itemize}
\item In the maker, do not mutate the return slot/pointer
  until you have verified
  that the allocation and all other initialization have succeeded.
  If there is an error,
  the target pointer should
  remain untouched.
\item For the release function, a [[NULL]] input should be a no-op.
\item Likewise, releasing a cell whose [[refc]] is already [[0]]
  should be a no-op, since this indicates
  that the cell is already in the process of being released.
  This should rarely occur, but it could happen under conditions
  where a cycle in the dependency graph of cells exists.
\item A cell should mark its [[refc]] down before recurring
  on its members for the same concern about cycles.
\item After calling [[free()]] on a cell, set the cell to [[NULL]].
  This should not actually matter in the course of execution,
  but it can help with debugging.
\end{itemize}

The cell handling we put into a file on its own.

<<cell.c>>=
#include <stdlib.h>

#include "codfns.h"

<<Cell definitions>>
@ %def cell.c

<<Tangle Commands>>=
echo "Tangling rtm/cell.c..."
notangle -R'cell.c' codfns.nw > rtm/cell.c
@

\subsubsection{DWA Interface and Error Handling}

Finally, we must handle the DWA connection
between a Co-dfns compiled module and the interpreter.
One constraint on this design
is the need to make a Co-dfns module
work with or without a DWA-driven interpreter.
If we are interfacing solely with a foreign, C-based system, 
we still must function somehow.

DWA modules export [[DyalogGetInterpreterFns]]
as a function to link the interpreter and the module.

\begin{figure}[htp]
\centering
\begin{mpost}
boxit.interp(\btex Interpreter etex);
boxit.getfns(\btex {\texttt DyalogGetInterpreterFunctions} etex);
boxit.module(\btex Module etex);
boxit.dwafns(\btex DWA Functions etex);
interp.c + (1in, 0.5in) = getfns.c;
getfns.c + (1in, -0.5in) = module.c;
module.c + (-1in, -0.5in) = dwafns.c;
drawboxed(interp, getfns, module, dwafns);
drawarrow interp.n -- getfns.c cutbefore bpath interp cutafter bpath getfns;
drawarrow getfns.c -- module.n cutbefore bpath getfns cutafter bpath module;
drawarrow module.s -- dwafns.c cutbefore bpath module cutafter bpath dwafns;
drawarrow dwafns.c -- interp.s cutbefore bpath dwafns cutafter bpath interp;
drawarrow getfns.c -- dwafns.c cutbefore bpath getfns cutafter bpath dwafns;
\end{mpost}
\caption{DWA module initialization}
\label{fig:dwainit}
\end{figure}

The function receives a structure from the interpreter
populated with function pointers
that enable access to various interpreter features.
A small design point comes into play here
because we do not want to unnecessarily expose our underlying model
to the user of the compiled module.
In particular, if an user is not a Dyalog interpreter,
they should not need to know about the DWA system in order to 
function.
For example, they should not need 
to know or use [[DyalogGetInterpreterFunctions]]
or the underlying functions.
Thus, we must have a way
to achieve similar functionality from different systems.

Our approach to this is to provide more generic
and explicit function for setting things
we want from any system and then to layer DWA initialization 
on top of that.

Fundamentally, the main thing that we care about
for all systems is having some means of making
a non-local escaping error report.
This main error reporting is meant to mimic
the extended signalling functionality
of the interpreter documented in the [[⎕DMX]]
object.
The DWA equivalent of this structure is given
by [[struct dwa_dmx]].

<<DWA structures and enumerations>>=
struct dwa_dmx {
	unsigned int flags;
	unsigned int en;
	unsigned int enx;
	const wchar_t *vendor;
	const wchar_t *message;
	const wchar_t *category;
};
@ %def dwa_dmx

In our APL model at the moment,
there is only one main and universal [[⎕DMX]] object
at a time, so we define a single [[dmx]] binding
to contain the current data.

<<DWA definitions>>=
struct dwa_dmx dmx;
@ %def dmx

The reality of many FFI systems is that they
do not do a good job of supporting C structs in the form
of such global variables,
so we must make sure 
that there is a meaningful way to access
the system using nothing but function calls.

In the case of errors we have an interesting situation.
In C, handling a long chain of errors demands that we are meticulous
about how we handle the interaction of the call stack and
any kind of early exit.
In our case, this means 
that any time we finally call the non-local error function
that we expect to never return,
we may be quite far removed from the original site of the error.
Thus, passing any complex data back up a call stack
could be quite complex.
Instead, we populate most of [[dmx]] 
that we care about using setter functions
and then only have a very little to worry about passing
up a call stack,
namely, the error number itself.

we define a setter function [[set_dmx_message]]
to handle setting [[dmx.message]].

<<DWA definitions>>=
DECLSPEC void
set_dmx_message(wchar_t *msg)
{
	dmx.message = msg;
}
@ %def set_dmx_message

<<C runtime declarations>>=
DECLSPEC void set_dmx_message(wchar_t *);
@

Our main non-returning function [[dwa_error]]
handles some of the parts of [[dmx]] that we do not
currently change,
and then calls the internally initialized error function provided
by whatever our interfacing system is.

<<DWA definitions>>=
DECLSPEC void
dwa_error(unsigned int n)
{
	dmx.flags = 3;
	dmx.en = n;
	dmx.enx = n;
	dmx.vendor = L"Co-dfns";
	dmx.category = NULL;

	dwa_error_ptr(&dmx);
}
@ %def dwa_error

<<C runtime declarations>>=
DECLSPEC void dwa_error(unsigned int);
@

The above requires the calling interface set [[dwa_error_ptr]],
which we handle with [[set_codfns_error]].

<<DWA definitions>>=
void (*dwa_error_ptr)(struct dwa_dmx *);

DECLSPEC void
set_codfns_error(void *fn)
{
	dwa_error_ptr = fn;
}
@ %def dwa_error_ptr set_codfns_error

<<C runtime declarations>>=
DECLSPEC void set_codfns_error(void *);
@

To link this interface into the DWA functionality,
we must extract the appropriate function pointers
out of the structure passed to [[DyalogGetInterpreterFunctions]].
We assume that the code generator will create a suitable definition
for [[DyalogGetInterpreterFunctions]]
that calls the following [[set_dwafns]], such as:

\begin{verbatim}
EXPORT int
DyalogGetInterpreterFunctions(void *fns)
{
	return set_dwafns(fns);
}
\end{verbatim}

This established a link in each compiled module
to the runtime DWA handling and allows us
to keep the DWA logic inside the runtime.
The DWA structure is relatively involved 
in its full expression,
but we do not need the full power,
so we can simplify our setup.
We also want to talk about the structure
more generically here without too much detail
that may be more properly handled in the correct 
language feature section.
At its heart, the structure is a set of functions,
which we store as an array of [[void *]] pointers.

<<DWA structures and enumerations>>=
struct dwa_wsfns {
	long long size;
	void *fns[18];
};

struct dwa_fns {
	long long size;
	struct dwa_wsfns *ws;
};
@ %def dwa_wsfns dwa_fns

It is the job of the [[set_dwafns]] function
to set the appropriate Co-dfns interface functions
and follow the initialization expectations
of the DWA system.
On successful initialization,
the function should return 0,
but we must check compatibility by examining the given structure [[size]],
return [[16]] if something is not right.

<<DWA definitions>>=
DECLSPEC int
set_dwafns(void *p)
{
	struct dwa_fns *dwa;

	if (p == NULL)
		return 0;

	dwa = p;

	if (dwa->size < (long long)sizeof(struct dwa_fns))
		return 16;

	<<Set DWA interface functions>>

	return 0;
}
@ %def set_dwafns

Assuming that the DWA structure seems valid,
we want to extract these functions into the appropriate
names that we have created for them.
An alternative would be to retain the structure
and make indirect calls into that structure,
but this is a little more awkward and would involve
both more storage and more memory indirects
for no more clarity and only more entanglement of the code.
Instead, setting the correct names at the time
of a [[set_dwafns]] call leads to a much cleaner dependency tree.
At this point, only the [[dwa_error]] function
has been designed and defined.

<<Set DWA interface functions>>=
set_codfns_error(dwa->ws->fns[17]);
@

This covers the main global DWA handling, 
but we have more to do in other sections 
to handle DWA arrays and function calling.
We benefit from having a few things together in a single C file,
so we will store our DWA code in a single C file
with an eye to making it easy to add in the appropriate code
in later sections.

<<dwa.c>>=
#include <stddef.h>
#include <stdint.h>
#include <string.h>
#include <arrayfire.h>

#include "codfns.h"

<<DWA macros>>
<<DWA structures and enumerations>>
<<DWA definitions>>
@ %def dwa.c

<<Tangle Commands>>=
echo "Tangling rtm/dwa.c..."
notangle -R'dwa.c' codfns.nw > rtm/dwa.c
@

\section{Language Features}
\label{sec:langfeats}

\subsection{Comments and Whitespace}

Early in the parsing process,
we want to unify and simplify whitespace and comments in the code
so that none of the future code has to worry about it.
There are a few things to consider.

First, comments should be completely eliminated from the tree
so that we never attempt to parse anything inside of a comment.
we cannot make this our first step in the parser
because character vectors may have [[⍝]] characters in them.
It is okay to have ``string-like'' things  within comments
because we can safely ignore anything in a comment
as long as we can reliably and accurately identify
the semantically meaningful [[⍝]] characters 
from those in a string.

This makes comment parsing and character vector parsing 
an intertwined process.
We must identify such strings first,
which we can do by making a Boolean mask [[msk]] marking out
the possible strings,
but we cannot parse these yet
because some of these may appear
inside a comment
and should not be parsed.
Once we have the potential string regions,
only all [[⍝]] outside of these regions 
must be the semantic comment starts.
A little thought suffices to prove that no semantic comment
can appear inside a potential string region:
if a semantic marker was inside a potential string region,
this would mean that there are no previous markers on the line,
but that means that the string region must be a real region,
and that means that the [[⍝]] character is not semantic.

We assume that we are still in our nested line representation
at this point because strings and comments are line-local,
so it is much easier to handle them in the nested form.
Assuming that [[msk]] is the nested Boolean mask of potential regions,
there are a few representations we could use,
based on whether we include or exclude the leading or trailing 
[[']] quote characters.
Fortunately for us, this does not matter,
since we are mostly interested in using [[msk]] to find
the semantic [[⍝]] points.
After that, we do not need [[msk]].
Since the start and end [[']] characters will never match [[⍝]],
the search for semantic [[⍝]] characters is the same regardless.
This allows us to filter [[pos]] and [[msk]] down
with the following code.
We do not need [[end]] yet because we do not care
about the extra whitespace in our implied regions at the moment.

<<Remove comments>>=
pos msk⌿¨⍨←⊂∧⍀¨(~msk)⍲'⍝'=IN∘I¨pos
@

AFter handling comments,
we must make sure that we have adequately checked our string syntax.
After this, we still want to do a few more things
to normalize the whitespace in our source.
We want to normalize line endings by removing occurrences of [[⋄]]
and using a single [[Z]] node to wrap all lines.
We also want to reduce most of the clearly unnecessary whitespace
so that we do not need to scan useless characters all the time
during tokenization.
We plan to eliminate all unneeded character nodes at the end
of tokenization anyways,
but there is no reason to make all the tokenization passes 
traverse so much blank space all the time.

After we have checked the syntax on character vectors,
we no longer require the nested representation,
but we find yet another interesting point of design.
If we choose to handle [[⋄]] nodes before tokenizing strings,
we are now free to do either,
we must continue to use [[msk]] to make sure
we do not match [[⋄]] characters that appear in strings.
On the other hand, tokenizing strings is much more nicely expressed
using a flattened representation.
But when we go to eliminate whitespace,
it might be nice to do this on a nested representation
to gain access to the leading and trailing whitespace idioms.

In the end, I find it more objectionable to continue persisting
the [[msk]] value longer than necessary,
so my primary concern is to tokenize strings as quickly as I can
instead of continuing to use the nested representation.
This means flattening right away and then tokenizing strings
right away.
We can also observe that removing leading and trailing whitespace
is simply a special case of removing duplicate or insignificant
whitespace anywhere in the source. 
Once strings are tokenized,
we are free to eliminate insignificant whitespace from anywhere
in the source at once.
This more general approach has a much richer invariant
at the end of it anyways.
This makes the case for early flattening a slam dunk.

Flattening takes the nested representations of [[pos]] and [[msk]]
and converts them into simple arrays.
When doing this we must retain the line divisions somehow.
To do this, we introduce the [[t]] field to give a type
to each character, 
which we now begin thinking of more like nodes
in a fully flat and unconnected forest.
We use type [[0]] for unparsed character data,
but introduce our first type to represent a line, [[Z]].
We will continue to think of [[Z]] nodes
as ``miscellaneous container'' nodes. 
At this point, we put a [[Z]] node as the start of each line,
pointing to the first character of the line, given by [[⊃¨pos]].

<<Flatten parser representation>>=
t←⊃0⍴⊂pos
t pos msk(∊,∘⍪⍨)←Z (⊃¨pos) 0
@

After strings have been appropriately tokenized,
we are free to handle the final main points,
which are to eliminate insignificant whitespace
and to make all [[⋄]] characters into [[Z]] nodes.
The latter is trivial.

<<Convert [[⋄]] to [[Z]] nodes>>=
t[⍸'⋄'=IN[pos]]←Z
@

Eliminating insignificant whitespace is not as cut and dry.
There is the question of how much to remove.
We think the benefit of knowing that all whitespace
is isngificant further down the compiler pipeline
is a nice enough invariant to have
that it is worth pursuing,
not to mention the inherent increase in efficiency.

We observe that knowing that a group of spaces
is insignificant requires knowing 
what is on the right \emph{and} the left.
It does not suffice to know only one side.
It would be possible to compute this all at one go,
but we can make this much easier by first reducing all 
contiguous spaces down
so that there is no contiguous whitespace.
This will ensure that it is much easier 
to check the left and right sides.

First, we must define what we consider valid whitespace.
In this case, all newlines should have already been converted
into [[Z]] nodes, 
and as far as I can tell,
APL does not permit more exotic forms of whitespace in the source.
That leaves only tabs and spaces.

<<Define character classes>>=
WS←⎕UCS 9 32
@ %def WS

Now we should eliminate any contiguous whitespace characters.
One thing we must remember at this point
is how we must handle [[Z]] nodes.
We must make sure not to eliminate any [[Z]] nodes,
which might happen if we only check the value of [[IN[pos]]]
because [[pos]] for a [[Z]] node is likely to point
to a whitespace character.
Contiguous whitespace is simply whitespace 
that has whitespace to its left.
We could also define it as right instead of left,
but defining it as left will have the nice side effect
of removing all leading whitespace.
Since a typical APL source should have more leading whitespace
than trailing,
editors often automatically remove trailing whitespace,
this seems like a nice free win.

At this point, we have to update three fields:
[[t]], [[pos]], and [[end]].

<<Remove insignificant whitespace>>=
t pos end⌿⍨←⊂~(t=0)∧(¯1⌽IN[pos])∊WS
@

White the contiguous whitespace removed,
we can focus on eliminating insignificant whitespace.
The only way for a space to be significant is for both
its left and right neighbors to be non-breaking or merging characters.
In APL, these are alphabetic characters, digits, [[¯]], [[⍺]], [[⍵]], 
[[.]], and [[⎕]]. 
We do not need to get this absolutely perfect
because tokenization will handle that;
this is just to remove obvious excess before continuing.

<<Remove insignificant whitespace>>=
msk←⊃1 ¯1∧.((alp,num,'¯⍺⍵⎕.')∊⍨⌽)⊂x←IN[pos]
t pos end⌿⍨←⊂msk∨(t≠0)∨~x∊WS
@

\subsection{Valid source input character set}

An APL source should contain
only a limited set of valid characters
outside of character vectors and comments.
We want to verify this is the case as early in the parser as possible
since this limited character set is quite useful
in the rest of the parser.
However, we must do this after tokenizing away
any character vectors and comments
to avoid false positives on their contents.

While we are validating the characters,
it is also a good time to classify various characters
into their appropriate categories.
We do that first.
Most obvious is the set of alphabetic characters.
These are the characters in addition to numeric digits
that constitute valid characters for variable names.

<<Define character classes>>=
alp←'ABCDEFGHIJKLMNOPQRSTUVWXYZ_
alp,←'abcdefghijklmnopqrstuvwxyz'
alp,←'ÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖØÙÚÛÜÝß
alp,←'àáâãäåæçèéêëìíîïðñòóôõöøùúûüþ'
alp,←'∆⍙ⒶⒷⒸⒹⒺⒻⒼⒽⒾⒿⓀⓁⓂⓃⓄⓅⓆⓇⓈⓉⓊⓋⓌⓍⓎⓏ'
@ %def alp

\noindent
The numbers should get their own unique class.

<<Define character classes>>=
num←⎕D
@ %def num

Next are the syntax characters.
These are characters that exist primarily as non-primitive annotations
mainly useful in parsing,
they may also represent components of compound tokens.
We split these into two classes: 
class [[syna]] are the characters 
that may form more compound units,
but that generally represent atomic values absent other context;
class [[synb]] contains the rest,
including [[⍺]] and [[⍵]].

<<Define character classes>>=
syna←'⍬⎕⍞#'
synb←'¯[]{}()'':⍺⍵⋄;'
@ %def syna synb

Primitives are a little more complex.
Fortunately, all our primitives are essentially single characters%
\footnote{Ignore [[∘.]] for the moment,
  or imagine it as an application of [[.]]
  if it makes you feel better.},
but we must handle operators with more care.
Primitives that we always treat as functions 
present little challenge,
but primitives that are operators or that may behave like operators
are another story.
The so-called schizophrenic primitives 
actually present the least trouble;
we must parse them uniquely so they will get their own class.
For the more mundane operators, though,
the legitimate design space is larger.
Obviously, there are monadic and dyadic operators,
but we could treat them all as a single class.
In the case of parsing, 
\emph{not} distinguishing monadic and dyadic strikes me
as a grand mistake,
especially since these two sets are exclusive to one another.
But, there are dyadic operators that take more than one type of operand,
that is, they accept array and function operands.
Do we want to split this classification up
to differentiate operand types as well as operator arity?

I say no.
The reasoning is simple.
Operators as a class will always be exclusively divided by arity,
but there is much less clean division of operand type classifications.
Moreover, if we make the distinction at parse time,
we must also handle user-defined operators somewhat uniquely.
The end result is a vastly expanded state space for the problem
with the only real benefit being a slightly earlier error message
about operator type errors.
It is not at all clear that this is even a good thing.
We will not alter the parse tree in any way
by choosing not to distinguish based on operand type,
but we gain the ability to treat
user-defined operators as the same class as primitive operators,
greatly reducing the state space
without loss of overall fidelity.

Finally, since we will be handling assignment uniquely anyways,
we can just treat it as a function primitive for most cases
without much trouble.
This gives the following definitions.

<<Define character classes>>=
prmfs←'+-×÷|⌈⌊*⍟○!?~∧∨⍲⍱<≤=>≥≠
prmfs,←'≡≢⍴,⍪⌽⊖⍉↑↓⊂⊆⊃∊⍷∩∪⍳⍸⌷⍋⍒⍎⍕⊥⊤⊣⊢⌹∇←→'
prmmo←'¨⍨&⌶⌸'
prmdo←'∘.⍣⍠⌺⍤⍥@'
prmfo←'/⌿\⍀'
prms←prmfs,prmmo,prmdo,prmfo
@ %def prmfs prmmo prmdo prmfo prms

With the character classes defined,
we can verify that all characters outside of strings are valid.
We must remember to include [[WS]] in this set.

<<Verify that all open characters are valid>>=
∨⌿msk←~IN[pos]∊alp,num,syna,synb,prms,WS:{
	EM←'SYNTAX ERROR: INVALID CHARACTER(S) IN SOURCE',CR
	EM,←quotelines ⍸msk
	EM ⎕SIGNAL 2
}⍬
@

\subsection{Strings and characters}

APL has a single string syntax.
As an atomic unit that exists at much the same level
as that of a number,
the main impact of string support occurs in
the parser, code generator, and runtime primitives.
It has minimal impact on the main compiler transformations.

Taking a high level view,
we want to parse, compile, generate, and work in the runtime
with strings.
At the compile level, we should make it so
that strings are handled in the same way
that any simple array is handled.
Likewise, handling character arrays should mostly work
just the same as any other array
as long as we have an appropriate type tag.
It is in parsing that the most work is required.
We must also ensure that we can properly convert the data
into a good runtime representation during code generation.

Strings must be handled early on in the parser,
since a character vector may contain all sorts of content,
making it almost impossible to parse most other content
without first parsing strings.
However, comments also have this feature,
and we must intertwin the parsing of strings
with the parsing of comments.
The fundamental issue is that comments may hold things
that look like strings and strings may enclose things
that look like comments. 
In principle, the first marker, either [[']] or [[⍝]],
takes precedence,
so we must figure out how to do that.
Since comments completely block out all the rest of a line,
the most information comes from checking each line
for all things that look like strings first.
Then, we can look for any comment markers 
that are not inside of strings
and use that to eliminate any strings
that are really just inside of comments.
We can accomplish this on the nested [[pos]] representation
using the common [[≠⍀]] idiom to produce [[msk]]
that is used in the previous section.
We must also remember to mark the double quotes separately
to handle escaped quotes.

<<Mask potential strings>>=
msk←(''''''∘⍷¨x)∨≠⍀¨''''=x←IN∘I¨pos
@

Once that is done, 
we must eliminate comments
so that we can continue to parse the strings
that are ``real.''
Before tokenizing the strings,
we must check that they are balanced on a line.
Since we are still using the nested representation at this point,
we can do this pretty easily by checking the end of the line
for any open strings.
We should report all unbalanced string ranges that we find.

<<Check for unbalanced strings>>=
0≠≢lin←⍸⊃∘⌽¨msk:{
	EM←'UNBALANCED STRING',('S'⌿⍨2≤≢lin),CR
	2 EM SIGNAL ∊(msk⌿¨pos)[lin]
}⍬
@

The [[msk]] value now contains well-formed strings in the source,
ready for tokenization.
It is nicer to do the tokenization on a flat representation,
so we wait to perform this next step
until we have flattened [[pos]] and [[msk]].
This means we have [[t]] to worry about, too.

At this point,
after tokenizing strings,
it will no longer be the case
that we can think of each [[pos]] as pointing to a single character
modulo whitespace.
That makes this a good time to introduce the [[end]] field.
To begin with,
we will assume that all nodes point to a single character.

<<Tokenize strings>>=
end←1+pos
@

We now must consider how we want to handle a string's node type
in [[t]].
Thinking to what we want,
eventually, we want all simple arrays to match in type.
But at this moment,
there is no real concept of the array as such,
and there really will not be until we appropriately handle stranding.
At that point,
we can imagine a single array type with sub-kinds.
At this point,
we really have tokens and not any specific sub-typed AST structure.
Thus, we want to avoid needing to introduce the [[k]] field
for as long as possible within the parser.
To do this, we will give tokens that are atomic,
such as numbers, strings, and the [[syna]] class,
their own node types until we have
an appropriate conceptual representation
for unifying them later on.
In the case of strings,
we will assign them type [[C]].

We should take a moment to consider a few things:
what node to convert to type [[C]],
where to begin the string region for [[pos]],
and where to end it with [[end]].
I think it makes the most sense
to include the opening and closing quote characters 
in the range of the token,
for at least two reasons.
First, if we are using the [[pos]] and [[end]] data
for something like syntax highlighting or text editing,
it makes more sense for the whole unit to highlight;
editing a string, say, to delete it, does not make much sense
without the quotes, 
especially if we want to think of this as a single atomic unit.
Additionally, if [[IN[pos]]] for a string points to [[']]
instead of an element inside of the string,
we can know the token by its [[pos]] value as well as by its type [[t]].
This can make our future calculations simpler.
Finally, we can avoid the somewhat problematic case 
of an empty string resulting in [[pos = end]].

The starting point in this case is already pointing in the right spot
if we choose to use the opening quote node as our new [[C]] node,
meaning that we need only update [[t]] and [[end]] and not [[pos]],
so we will use that node.
Our flattened [[msk]] value defined above will put a [[1]]
in the opening quote position,
and a [[0]] in the closing quote position,
and [[1]]'s in all the string content positions.
This makes it easy to use the [[2<.⌿]] and [[2>⌿]] idioms
to select the opening and closing quote positions.

<<Tokenize strings>>=
t[i←⍸2<⌿0⍪msk]←C
end[i]←end[⍸2>⌿msk⍪0]
@

Once the [[end]] field is right,
we no longer require the rest of the string nodes
as elements in the forest,
allowing us to remove them and free up space
while hiding string data visibility,
thus completing tokenization.
We also no longer need the [[msk]] data,
so we can let it go.

<<Tokenize strings>>=
t pos end⌿⍨←⊂(t≠0)∨~¯1⌽msk
@

And this is basically all that must be done to handle strings
in the parser,
assuming our array handling adequately unifies 
all the atomic elements into the appropriate 
simple and stranded array representations.
However, our choice to make the [[pos]] and [[end]] fields
contain the opening and closing quotes in a string
means that we must process the [[n]] field of [[C]] nodes
when it is created.

<<Type-specific processing of the [[n]] field>>=
n←⍎¨@{t=C}n
@

So much for parsing, and, indeed, compilation.
Next, we must handle code generation.
By the time we reach the code generator,
the [[C]] nodes ought to have disappeared
and all simple and strand arrays ought to belong to the same [[A]] type.
Since array handling code is mostly common 
across all element types,
we handle those elements in Section \ref{sec:langfeatarrays} on arrays;
our only responsibility in this section is the unique processing
necessary to deal with the character element type(s).
For the generator, we must be able to map literal character arrays
to a [[data[]]] array with an appropriate C data type.
We must also map this to an appropriate [[enum array_type]].
This assumes that we will use this logic in some kind
of ``make data'' helper that receives element data
as a vector and generates the appropriate code.

Handling text in the compiler is an interesting problem
because almost all Unicode-based text encodings 
have some sort of variable length aspect to them.
In most languages that is fine,
but for APL,
which has random access indexing built in to its overall paradigm,
the lack of random access-ness in these encodings 
is not really acceptable.
That means we might want to take the UTF-32 model,
but for most of the common cases, 
that is massively inefficient just to be able to handle the edges.
Instead, we will adopt the same scheme that the Dyalog interpreter uses,
which will have the added benefit of buffer compatibility 
between the two representations.

The Dyalog interpreter preserves arbitrary indexing
at the cost of needing 3 data types for characters.
Table \ref{table:charactertypes} shows the APL types mapped
to the C type we will use in the runtime.
Rather than store textual data in one of the UTF encodings,
this representationstores literal code points from Unicode,
and it uses the smallest byte size that it can 
to store all code points in a buffer with the same element size.
That is, if all the points in an array are 
in the range [[[0,2*8)]], 
then we only need to use the 8-bit datatype, and so on.

\begin{table}[htp]
\centering
\caption{Type associations between APL, C, ArrayFire, and Co-dfns}
\label{table:charactertypes}
\begin{tabular}{rlll}
\toprule
APL & C & ArrayFire & Co-dfns \\
\midrule
[[80]] & [[uint8_t]] & [[u8]] & [[ARR_CHAR8]] \\
[[160]] & [[uint16_t]] & [[u16]] & [[ARR_CHAR16]] \\
[[320]] & [[uint32_t]] & [[u32]] & [[ARR_CHAR32]] \\
\bottomrule
\end{tabular}
\end{table}

We also make a note here that there is another datatype
in the interpreter used to handle Classic character array types,
but we are explicitly not going to support 
any Classic edition features or data types.
To do the mapping, we will use [[⎕DR]] for testing the data type,
but [[⎕UCS]] in its monadic form to get the code points.

<<Element [[data]] and [[type]] generator cases>>=
3>i←80 160 320⍳⎕DR ⍵:{
	bits←⍕i⊃8 16 32
	points←⊃{⍺,',',⍵}⌿⍕¨⎕UCS ⍵
	z←⊂'uint',bits,'_t data[] = {',points,'};'
	z,←⊂'enum array_type type = ARR_CHAR',bits,';'
z}⍵
@

This ensures that the code generator can produce character data,
but we must also add support for characters 
into the runtime proper.
This means:

\begin{itemize}
\item Defining appropriate [[enum array_type]] values
\item Mapping these values to the corresponding ArrayFire types
\item Supporting conversion to/from DWA arrays
\end{itemize}

\noindent
We used the following array types.

<<Array element types>>=
ARR_CHAR8, ARR_CHAR16, ARR_CHAR32,
@ %def ARR_CHAR8 ARR_CHAR16 ARR_CHAR32

These array types must also map to ArrayFire types
so that we can allocate them appropriate on the GPU.
ArrayFire does not have any notion of character types,
so we will use their unsigned types as a useful alternative.

<<Cases for selecting device [[values]] dtype>>=
case ARR_CHAR8:
	dtype = u8;
	break;
case ARR_CHAR16:
	dtype = u16;
	break;
case ARR_CHAR32:
	dtype = u32;
	break;
@

In addition to getting data into an ArrayFire array,
we must also be able to get data in and out of DWA arrays.
Fortunately, this is fairly easy at this level
because our element types are all the same size
as the Dyalog interpreter's element types.
This means that we do not need to do any pre- or post-processing
of the incoming our outgoing data before we simply copy it over.
And htat means the only thing we need to do is to map
between DWA character types and Co-dfns character types.

Dyalog has these DWA character types:

<<DWA character types>>=
DWA_CHAR8, DWA_CHAR16, DWA_,
@ %def DWA_CHAR8 DWA_CHAR16 DWA_CHAR32

All that remains to to map these to and from Co-dfns types.

<<Cases for selecting [[type]] based on DWA type>>=
case DWA_CHAR8:
	type = ARR_CHAR8;
	break;
case DWA_CHAR16:
	type = ARR_CHAR16;
	break;
case DWA_CHAR32:
	type = ARR_CHAR32;
	break;
@

<<Cases for selecting [[type]] based on array type>>=
case ARR_CHAR8:
	type = DWA_CHAR8;
	break;
case ARR_CHAR16:
	type = DWA_CHAR16;
	break;
case ARR_CHAR32:
	type = DWA_CHAR32;
	break;
@

And this concludes all the handling necessary 
to put characters into the system.
It only deals with characters, 
and you need to read Section \ref{sec:langfeatarrays} to see
how it all fits together.
You can also see the numeric handling 
in Section \ref{subsec:numbers} for a treatment
of element types that are not as simple and straightforward.

\subsection{Numbers}
\label{subsec:numbers}

Supporting numbers in the compiler
is a little more complex
than handling characters
because there are many more numeric forms
than there are character forms,
but the main strategy remains the same:
we must tokenize and parse them
into [[N]] nodes,
ideally ignore them in the compiler transformations,
and then generate the appropriate data
and add datatype support for them in the runtime.

Our aim at the moment
is to support the primary numeric types supported by Dyalog,
as given in 
Table \ref{table:numerictypes}.

\begin{table}[htp]
\centering
\caption{Supported numeric datatypes and their equivalencies}
\label{table:numerictypes}
\begin{tabular}{rllll}
\toprule
APL & Co-dfns & C & ArrayFire & Convert? \\
\midrule
[[11]] & [[ARR_BOOL]] & [[char]] & [[b8]] & Yes \\
[[83]] & [[ARR_SINT]] & [[int16_t]] & [[s16]] & Yes \\
[[163]] & [[ARR_SINT]] & [[int16_t]] & [[s16]] & No \\
[[323]] & [[ARR_INT]] & [[int32_t]] & [[s32]] & No \\
[[645]] & [[ARR_DBL]] & [[double]] & [[f64]] & No \\
[[1289]] & [[ARR_CMPX]] & [[struct apl_cmpx]] & [[c64]] & No \\
\bottomrule
\end{tabular}
\end{table}

Notice that we do not support 128-bit decimal floats at the moment
and that we treat type 83 like type 163 and type 11 as type 83
when we represent these values on device or in the runtime.
This is because ArrayFire does not have any internal support
for bitvectors or signed 8-bit integers.
This has some performance and space considerations for user code,
but in my previous experience, 
it is not even clear how much benefit we may get from using bitvectors
on GPU acceleration devices.
At any rate, the juice isn't worth the squeeze at this moment
in terms of implementation complexity.

APL also has a number of syntaxes for specifying numeric values.
We want to support the syntaxes in Table \ref{table:numericsyntaxes}.

\begin{table}[htp]
\centering
\caption{Numeric APL syntaxes and whether they are C compatible}
\label{table:numericsyntaxes}
\begin{tabular}{rll}
\toprule
Form & Type & C compatible? \\
\midrule
[[123]] & Integer & Yes \\
[[¯123]] & Signed & No \\
[[12.3]] & Floating & Yes \\
[[¯12.3]] & Signed float & No \\
[[NeN]] & Exponent & Yes \\
[[NjN]] & Complex & No \\
\bottomrule
\end{tabular}
\end{table}

When handling numbers,
we can imagine a future in which we support more types
and more syntaxes than Dyalog APL may support,
and in that case, 
it is important that we do not depend
on the numeric parser built into the interpreter.
Even with the numeric types overlapping entirely, we must ensure
that we do not somehow lose precision
that we may want.
the benefit of using the built-in interpreter at the time of parsing
is that all the numbers come into a specific reified form
rather than remaining in their textual form,
so we can save a lot of effort simply by doing so,
as long as we maintain precision.
This also has the advantage of making 
the runtime generation code simpler,
since we do not need to support so many numeric forms.
this has enough advantages to make it worth doing internally,
but we must still tokenize the numbers ourselves
and manage the code generation.

The first step is tokenizing the numbers into atomic units.
here, we must recognize the various numeric forms
and distinguish numbers from digits used in names.
By doing this before tokenizing things like variables,
we can simplify the parsing of those other entities.

When we begin to tokenize,
[[x]] contains the relevant characters we need to examine.
The first challenge is to identify the clusters of digits
that will form the core units of our numeric forms.
Vitally, we want to distinguish digits that contribute to a number
from those that are part of some kind of name.
In APL this is more subtle than you may think,
because a name may have, 
but not begin with,
digits in it,
which is typical,
but a digit that is contiguous to a name and before it,
such as [[5x5]] is \emph{not} an error!
Rather, it is parsed the same as [[5 x5]],
meaning a number followed by a name.
This means we must eliminate digit clusters
that are contiguous to alphabetic forms only on the right side.
It gets a little more complex hwen we consider [[e]] and [[j]] forms.
Our basic rule is that we have a precedence of 
[[digits → . → ¯ → e → j]].
Consider [[1e1e1]]. 
Here, this should be parsed as [[1e1 e1]].
This is because numeric forms are greedy (left associative)
and you may only have a single [[e]] or [[j]] per unit at each level.
This provides an order for handling the creation of [[dm]],
which should be a mask for all number groups.
We can begin by initializing [[dm]] to all the possible numeric digits.

<<Tokenize numbers>>=
dm←x∊num
@ %def dm

Our plan is to progressively expand [[dm]] to encompass
all the elements of a valid numeric form while removing or eliminating
any digits that belong to names.

\begin{table}[htp]
\centering
\caption{Precedence of numeric syntax and parse order}
\label{table:numericprecedence}
\begin{tabular}{ccl}
\toprule
[[dm]] phase & Syntax & Notes \\
\midrule
0 & $[0-9]$ & Must appear at least once \\
1 & [[.]] & Only one per phase 0 group \\
2 & [[¯]] & Must prefix phase 1 groups, one only \\
3 & [[e]] & Must connect two phase 2 groups \\
4 & [[j]] & Must connect two phase 3 groups \\
\bottomrule
\end{tabular}
\end{table}

This progressive expansion serves to form a set
of ``phases'' for parsing the numeric forms.
The only syntax that must appear in a number are
the numeric digits themselves; 
all other forms are optional.
If we proceed one phase at a time,
we can check for errors in the simpler forms
before adding more complexity.

Since [[e]] and [[j]] are both valid variable name components,
we have a potential conflict between numeric forms and names.
APL resolves this by requiring the numeric [[e]] and [[j]] forms
to have two numbers,
one on each side, contiguous to it and only allowing one [[e]] or [[j]]
per unit,
though a single [[j]] unit may contain two [[e]] units.
When combined with the greedy parsing rule for these forms,
meaning the leftmost [[e]] or [[j]] is used
for a number if more than one [[e]]/[[j]] appears contiguous
to the same phase 2 number group.

All of this means an interesting parsing dependency:
there may be some sequences that look like numbers,
but actually are part of a name,
but we do not know this until we are able to identify
what [[e]] and [[j]] characters are part of a number or not.
Fortunately, this issue does not cause problems
because we can tackle it in much the same way as we handled strings
and comments.
Once any character in a sequence of alphanumeric characters definitely 
is a part of a name and not a number, all the subsequent characters
in the sequence must be part of the same name and not a number.
This is much like how a comment works.
Thus, we can begin by identifying all potentially numeric [[e]] and [[j]]
characters, 
eliminate those that are not the first in their units,
and finally mask off potential numbers that actually form 
a part of a name.

The first syntax we should add is the dot.
This is a blessedly simple form 
because there can be only a single occurrence, 
it may appear anywhere contiguous to a digit,
and if we find more than one, 
we know that we can signal an error.
We can only have these nice guarantees right now
because we are not considering [[e]] or [[j]] at this point;
we are only dealing with the smallest 
and most tightly bound compound number,
which is the floating-point value.

We can add any dot we find if it is 
on either side and contiguous to a digit,
which is just [[dm]] at the moment.

<<Tokenize numbers>>=
dm∨←('.'=x)∧(¯1⌽dm)∨1⌽dm
@

Now [[dm]] contains the digits and any contiguous dots.
Before proceeding, we should verify
that we do not have multiple dots in a single group.

<<Tokenize numbers>>=
∨⌿msk←1<+⌿¨dm⊆'.'=x:{
	EM←'MULTIPLE . IN FLOAT'
	2 EM SIGNAL ∊msk/dm⊆pos
}⍬
@

Now we can add the high minuses.
We can only have a single high minus in a numeric group.
Thankfully, this also means that we do not permit 
something like [[¯1¯2]],
as that would greatly complicate things.
When checking for well formed syntax,
we must check for duplicates/multiples in the same way as for dots,
but we must also handle orphaned high minuses,
since nay high minus that does not attach to a [[dm]] group
must be a syntax error.

<<Tokenize numbers>>=
dm∨←('¯'=x)∧1⌽dm
∨⌿msk←1<+⌿¨dm⊆'¯'=x:{
	EM←'MULTIPLE ¯ IN NUMBER'
	2 EM SIGNAL ∊msk⌿dm⊆pos
}⍬
∨⌿msk←('¯'=x)∧~dm:{
	EM←'ORPHANED ¯'
	2 EM SIGNAL msk⌿pos
}⍬
@

So much for the simple phases.
Now we must handle the more complex compound cases for [[e]] and [[j]].
It is at this point that the wheels come off a little bit.
The tokenizer in the Dyalog interpreter does very little lookahead.
As a result, there were some interesting design decisions made
to support dot and [[e]]/[[j]] since these are overloaded forms.
We have already embraced the idea 
that multiple dots near digits should be treated as an error.
However, with [[e]], since the exponent to [[e]] must be an integer
and not a float,
we have the strange result, in the interpreter, 
that [[1e0.5j3]] and [[1e.5j3]] parse differently.
In the first case, [[0.5]] binds to the [[e]]
and results in a syntax error,
but the second case parses without error as [[1 e 0.5j3]]!
This is because, at the time of seeing the dot,
the tokenizer recognizes that the [[e.]] combination
can never lead to a valid parse for [[e]] as a number,
and so it decides that [[e]] must be part of a name instead,
and parsing then continues with recognizing the complex [[0.5j3]].

\phantomsection
\label{topic:compoundnumberparsing}
This is madness.

I cannot in good conscience replicate this behavior into Co-dfns.
Thus, I intend to deviate from this behavior,
and so I will spend sometime justifying my decision for posterity.

The primary problem is that the behavior breaks cognitive predictability,
which can be seen by how such behavior violates 
our parsing precedence tower
(see Table \ref{table:numericprecedence}).
This makes the parsing code more idiosyncratic,
mushes all the numeric forms into a much less crisp tower,
and, perhaps worst of all, reduces or eliminates any human model
of parsing based on a more chunked or abstract form,
forcing the human mind to parse at a character-by-character
operational level,
which is most certainly \emph{not} what APL is about.

The root of all of this is that Dyalog APL thinks of [[0.5]]
as a character stream and not as a number.
Instead, by the time we think about [[e]], 
we should no longer worry 
about whether a dot is part of a number or not.
Given [[e.5]], there is no possible context in which
we may want [[0.5]] to be anything but a number.
This is how most people will parse this,
and they ought to be able to do so.
Now, given this, I should not expect [[0.5]] to parse differently
than [[.5]] in my code.
Anywhere, this should be a number, 
because the dot binds strongly to numbers.
Anywhere that this model breaks ought to result in a syntax error
as an ambiguous piece of code.
Look at the following scary examples from the interpreter:

\begin{verbatim}
1e0.5j3 → SYNTAX ERROR
 e0.5   → e0 0.5
1e.5j3  → 1 e 0.5j3
\end{verbatim}

\noindent
Oh, the horror!
In my opinion, I have two choices.
I can take the attitude of attempting to parse these
any way that I can,
or, I can introduce a syntax error for all the cases
that do not make sense to me.
Given that I want to make the compiler produce more helpful errors
and be a more congruent and consistent system,
I am of a mind to treat these as syntax errors.
In the future, I can imagine supporting floating exponents,
and that would permit the [[1e0.5]] and [[1e.5]] cases.
The [[e0.5]] case strikes me as something
that should always be an error.
The principle in play here is that [[D.D]] should always parse
as a number.
The other stuff comes out of this.

How does this affect our handling of [[e]] and [[j]]?
The first impact is one of simplification; 
any time we encounter [[e]] or [[j]] contiguous
with a set of decimal numbers,
we can confidently parse this as an exponent or complex form.
This also has the effect of committing at this point
to the decimal forms we already see in [[dm]].
That is, if they have a dot in them,
we know that they must be a number or a syntax error.
This makes the reasoning a little simpler,
but it adds an additional syntax error case
that we must handle.
We must check and handle the case 
where a float appears as an exponent
and also where a name is contiguous to a number.

Handling these requires
that we know what [[e]] and [[j]] characters
map to numeric forms and which do not.
There remains a question of whether we should handle
the contiguous name error now or later.
We can first note that the name error 
must come from any name contiguous with a number
and not just [[e]] or [[j]].
We can also make the observation that the name error
remains valid even after we theoretically parse [[e]] and [[j]] forms.
As long as a float is somewhere in a numeric form,
we have this error possibility.
The challenge is when we should error on a float exponent 
vs. an ambiguous name/number situation.
Given that we must do most of the work to parse 
[[e]] and [[j]] anyways to handle the name error,
it makes sense to put it after
we have mostly parsed those,
but by that time, we also could error on a float exponent.

I think it would be confusing
to start a complaint about the contents of a number
before confirming all number parses are unambiguous.
Moreover, at least in theory,
all the numbers are just potential numbers
until we mask out the numbers
that are really a part of names.
So verifying the forms of [[e]] should occur much later.
Before that, we must complete the rest of the numeric parsing.

Handling [[e]] and [[j]] is the same basic thing.
We must find and mark potential [[e]]'s and [[j]]'s.
We must make sure that each [[dm]] group matches 
against only a single [[e]] or [[j]].
So, a string of [[e]]'s such as [[1e1e1]] is the same
as [[1e1 e1]].
We must do the [[e]]'s separate from and before the [[j]]'s
are handled.
This allows us to maintain the invariant
that a [[dm]] group will only contain a single [[e]]
at the time we handle [[e]].
After dealing with [[j]], 
a [[dm]] group may contain more than one [[e]].

While we must make [[j]] and [[e]] separately,
we can mask off names as a unit later.
This is safe to do because both [[e]] and [[j]] forms 
have numbery things on either side.
This means that any potential [[j]] we find
and mark will still be eliminated
by the later masking,
so there is no need to attempt to eliminate 
such false positives earlier on right after marking the [[e]]'s.
Likewise, we will not miss any [[j]] forms for the same reason.
By delaying the masking,
we make it more convenient to handle
and we can separate our concerns about masking and error handling
from concerns about handling [[e]] and [[j]].
All we need concern ourselves with at this point
is identifying [[e]] forms that would be [[e]] forms unless
they are part of a name.

The approach is to check for an [[e]] contiguous to
two [[dm]] groups.
After this, we must eliminate as candidates
all but the first [[e]] in any one [[dm]] group.

<<Tokenize numbers>>=
dm∨←(msk←x∊'Ee')∧(¯1⌽dm)∧1⌽dm
dm←dm⍀∊{1@(⊃⍸⍵)~⍵}¨dm⊆msk
@

And we must do the same basic thing to handle the [[j]] forms.

<<Tokenize numbers>>=
dm∨←(msk←x∊'Jj')∧(¯1⌽dm)∧1⌽dm
dm←dm⍀∊{1@(⊃⍸⍵)~⍵}¨dm⊆msk
@

Since we have done the above in meticulous order
and captured only the [[e]] and [[j]] forms
that make syntactic sense,
there are no syntax errors to signal at this point.
Furthermore,
[[dm]] now contains the potential numbers
of our source,
and all that remains is to figure out 
which ones are names instead of numbers.

The main syntax error we want to address 
at this point is the ambiguous parsing 
that we may get because of a dot.
If we have a mask of names, it is not hard
to identify these points;
they are dots adjacent to a name on the left
that ends in a digit where there is a digit
on the right of the dot.
We do want to figure out how to
return a meaningful error message,
and that is more difficult.
Just highlighting the dot might be enough,
but we should consider the impact of highlighting other stuff
around the dot.
We could highlight the number around the dot
or even include the name in full as well.
What will give the most clarity to the end user?
I think we want to highlight the number,
since that is the main contention.
Thus, when we handle masking off names,
we must ensure that we are not removing 
the numeric information that we may want.

We have another consideration when 
we handle the masking off of names.
A high minus is a legtimate terminator of a name,
but a dot, as we have seen above, may not be.
The dot may legitamitely represent a reference to Inner Product,
or be ambiguous, or be numeric,
as we are masking off names.
After masking names, we can tell the class
of the dots by examining what is on their left and right,
as seen in Table \ref{table:parsenumericdot}.
All of this gives us a suitable strategy 
for handling the final cleanup of [[dm]].
After masking off the names we can fix up dots
in [[dm]] and handle ambiguous numeric errors.

\begin{table}[htp]
\centering
\caption{Parsing cases for dot;
[[V]] = name, [[N]] = number, [[D]] = digit}
\label{table:parsenumericdot}
\begin{tabular}{ll}
\toprule
Pattern & Class \\
\midrule
[[V.V]] & Inner Product \\
[[V.N]] & Numeric \\
[[VD.N]] & Ambiguous \\
[[N.V]] & Numeric \\
[[N.N]] & Numeric \\
\bottomrule
\end{tabular}
\end{table}

To mask off the names,
we must simply recognize what units of digits
are right of a contiguous unit of alphabetics.
These are digits that must belong to names.
We must be sure when we do this
that we are not breaking [[dm]] units somehow
and failing to recognize syntax errors.
If there are only digits,
then dropping some units will cause no other change,
but what about dropping in a group with non-digits?
In the case of dots,
we already know that we will need to fix these up.
For a high minus, it will always appear at the beginning
of any digit unit,
and so you cannot mask off such a unit,
and any units masked off past it are fine.
For [[e]] and [[j]], the masking off of such units
can only mean that they are not numeric.
Thus, we can be reasonably sure that this approach 
cannot mask off digits that it should not.
It \emph{will} mask off all the digits that should be masked off.

<<Tokenize numbers>>=
(msk⌿dm)←∊∧⍀¨(msk←x∊alp,num)⊆dm
@

We can handle the ambiguous errors and the dot cleanup in any order,
but the handling of the dots first will make it more convenient
to identify ambiguous cases.
The dots that are still in [[dm]] but that are isolated and alone
are not numeric,
so we should remove them.

<<Tokenize numbers>>=
dm[⍸dm∧(x='.')∧~(¯1⌽dm)∨1⌽dm]←0
@

And with that we are free to handle the ambiguous parsing errors,
which is anywhere a numeric dot is contiguous with a non-numeric digit
on its left.

<<Tokenize numbers>>=
∨⌿msk←∨⌿¨dm⊆dm∧(x='.')∧¯1⌽(~dm)∧x∊num:{
	EM←'AMBIGUOUS PLACEMENT OF NUMERIC FORM'
	2 EM SIGNAL ∊msk⌿dm⊆pos
}⍬
@

With all that handled,
the [[dm]] mask now contains a full and accurate set of numeric forms
in the source.
At this point we have not checked our numbers
to ensure that they are actually representable in our runtime,
but we do not care about that during tokenization.
The numbers at this point are at least theoretically representable
in some theoretical system.

We will make one concession in this case,
which is to check to ensure our exponents are integers and not floats.
This is a simple check that we can make right now
and is different than the limits on range and representation.
We mostly just need to examine all the exponent parts of our numbers
and check for a decimal point.
This requires that we break apart the numeric parts
that we want from the groups in [[dm]].
However, when doing so, we want to keep a good link back
to the source so that we can highlight the errors that we want.
In this case, any floating exponents are an error in some real part
of the code,
so we will want to highlight the whole real part when we find an error.
The easy way to do this is to have a mask [[rm]] of the real parts.

<<Tokenize numbers>>=
∨⌿msk←∨⌿¨'.'={1⊃(⍵⊆⍨~⍵∊'Ee'),⊂''}¨x⊆⍨rm←dm∧~x∊'Jj':{
	EM←'NON-INTEGER EXPONENT'
	2 EM SIGNAL ∊msk⌿rm⊆pos
}⍬
@

That is all that we want to do at this point in tokenization.
We are now free to use [[dm]] to handle the final tokenization
of these values.
Unlike with comments and strings,
there is not as much value to removing dead nodes at this point
since we still have a number of other items to tokenize
that we may want to deal with using [[dm]],
and since most numbers are quite small and we are unlikely 
to gain much advantage by the reduction at the moment.
Instead, we merely need to update [[t]] and [[end]] 
for the starting node in each [[dm]] group as appropriate,
with [[pos]] of course already pointing at the correct position.
For the [[end]] field we want to use the [[end]] value 
from the last character in the [[dm]] group. 
We will use type [[N]] for the type of a number token.

<<Tokenize numbers>>=
t[i←⍸2<⌿0⍪dm]←N
end[i]←end⌿⍨2>⌿dm⍪0
@

Now that the numeric tokens are there,
what else remains with the parser?
Eventually, we want these [[N]] nodes removed 
into their own [[A]] nodes,
but we will handle that in our handling of arrays.
Right now, the [[N]] nodes are not processed,
meaning that their [[n]] field will contain strings
instead of real numeric values.
There is an argument to be made that we should keep them
in character form
because this will allow us to use more numeric forms
that may not be supported by Dyalog APL.
That would shunt off handling of the numeric values to the runtime.
Such a decision would be short-sighted:
if we have a self-hosting compiler,
this will not matter,
and not evaluating the numeric values in the parser
would inhibit many potential compilation passes
that we may want to add.

This means that we must evaluate the [[n]] field of the [[N]] nodes
into real numeric values.
To do this, we could attempt to do all of the parsing ourselves,
but handling that is a remarkably subtle endeavour
with many pitfalls.
Instead, we will rely on the [[⎕VFI]] system function
to do this work for us.
We will signal a syntax error if we are unable to parse 
one of the [[n]] fields.

<<Type-specific processing of the [[n]] field>>=
msk vals←⎕VFI ⍕n⍕(t=N)⌿n
~∧⌿msk:{
	EM←'CANNOT REPRESENT NUMBER'
	2 EM SIGNAL ∊((t=N)⍀~msk)⌿pos+⍳¨end-pos
}⍬
n[⍸t=N]←vals
@

After this, by the end of the parser, 
the [[N]] nodes should have been converted/merged into [[A]] nodes,
so there is nothing else I can think of to handle numeric values
in the parser.

What about in the compiler transformations? 
As with character vectors,
we should not need to deal with them at all.

This takes us to the code generator.
We are in a similar position for code generation
as we are with character vectors,
in that we can assume that by the time we are at the code generator,
we have a simple [[A]] type that we expect 
to encapsulate most of the generic array generation code.
Here, it is our responsibility to generate a [[data[]]] array
with appropriate elements and type,
and to connect that with the appropriate [[enum array_type]].

Unlike the character element type,
we have more than one numeric type that we must handle,
and each one may require a little bit different handling,
but especially types such as complex numbers.
To make this more concrete,
Table \ref{table:numerictypes} shows the Dyalog numeric type
and the associated C type and [[enum array_type]] 
that we will use.
We also indicate the underlying ArrayFire element type.
Notice that some types will have some conversion,
while others will not.

We must make a generator case for each numeric type 
to encode that data.
For the real numbers, 
we do not need to do any processing of the data
because the numbers will be cast automatically and correctly
in their formatted form directly from APL.

<<Element [[data]] and [[type]] generator cases>>=
5>i←11 83 163 323 645⍳⎕DR ⍵:{
	⎕PP←17
	ct←i⊃(⊂'char'),(2⍴⊂'int16_t'),'int32_t' 'double'
	at←i⊃'BOOL' 'SINT' 'SINT' 'INT' 'DBL'
	z←⊂ct,' data[] = {',(⊃{⍺,',',⍵}⌿⍕¨⍵),'};'
	z,←⊂'enum array_type type = ARR_',at,';'
z}⍵
@

The ``ugly duckling'' in the room is the complex number.
This is because platform support for complex numbers
varies in how it is handled.
The main culprit is Microsoft Visual Studio.%
\footnote{\raggedright
  \url{docs.microsoft.com/en-us/cpp/c-runtime-library/complex-math-support}}
Because MSVC uses structs to represent complex numbers 
instead of the C99 style built-ins,
Dyalog APL uses a struct-based model
that matches the MSVC model,
rather than the C99 [[double complex]] form.%
\footnote{Actually, I am rather okay with this,
being something of a C89 traditionalist in aesthetic anyways.}
We will follow this model and define our own complex number struct.

<<C runtime structures>>=
struct apl_cmpx {
	double real;
	double imag;
};
@ %def apl_cmpx

This matches the format used by Dyalog's interpreter,
allowing us to pull data straight out of a DWA array.
We are also fortunate in that ArrayFire also
makes use of the struct-based approach,
allowing us to do simple initialization without any data conversion.
This means that we can define [[data[]]] fairly normally
except that we must initialize it
as a struct and not as single values.

<<Element [[data]] and [[type]] generator cases>>=
1289=⎕DR ⍵:{
	⎕PP←17
	mk_struct←{'{',(9○⍵),',',(11○⍵),'}'}
	comma←{⍺,',',⍵}
	vals←⊃comma⌿mk_struct¨⍵
	z←⊂'struct apl_cmpx data[] = {',vals,'};'
	z,←⊂'enum array_type type = ARR_CMPX;'
z}⍬
@

The above ensures that the array code generator
will have the appropriate data to work with.
All that remains is to add support for numeric types
into the runtime.

To support numbers in the runtime we must address the following:

\begin{itemize}
\item Add appropriate [[enum array_type]] values
\item Map [[enum array_type]] values to ArrayFire representations
\item Add support for numeric conversion to/from DWA to ArrayFire
\end{itemize}

We defined the following array types for numerics:

<<Array element types>>=
ARR_BOOL, ARR_SINT, ARR_INT, ARR_DBL, ARR_CMPX,
@ %def ARR_BOOL ARR_SINT ARR_INT ARR_DBL ARR_CMPX

\noindent
Each of these element types corresponds 
to the specific ArrayFire type indicated in 
Table \ref{table:numerictypes}.

<<Cases for selecting device [[values]] dtype>>=
case ARR_BOOL:
	dtype = b8;
	break;
case ARR_SINT:
	dtype = s16;
	break;
case ARR_INT:
	dtype = s32;
	break;
case ARR_DBL:
	dtype = f64;
	break;
case ARR_CMPX:
	dtype = c64;
	break;
@

When we handle DWA array inputs that come from the interpreter,
we want to handle the input data
and possibly pre-process the data
if we need to.
We assume that [[data]] is a pointer to the DWA numeric data buffer
and that [[count]] contains the element count of [[data]].
We must do any processing to the DWA data
in the cases where the representation in the DWA buffer
does not match the runtime representation.
We assume that we are casing over the DWA element types
of the [[data]] buffer.

The DWA element type consists of both simple and compound numeric types,
some of which overlap with the [[enum array_type]] values.

<<Simple DWA numeric element types>>=
DWA_BOOL, DWA_TINT, DWA_SINT, DWA_INT, DWA_DBL,
@ %def DWA_BOOL DWA_TINT DWA_SINT DWA_INT DWA_DBL

<<Compound DWA numeric element types>>=
DWA_CMPX, DWA_R, DWA_F, DWA_Q,
@ %def DWA_CMPX DWA_R DWA_F DWA_Q

In most cases, there is no pre-processing necessary,
since the representation matches.
However, in the cases of Boolean arrays and tiny integers,
this is not the case.

With a Boolean array, 
the DWA representation uses a bitvector encoding 
in which the first element in a byte is the most significant,
which I am calling big endian.
We must convert this to the [[b8]] format used
in ArrayFire,
which is simply using a single byte per Boolean.

<<Cases for pre-processing DWA [[data]] buffer>>=
case DWA_BOOL:{
	char *buf = calloc(count, sizeof(char));

	if (buf == NULL) {
		err = 1;
		break;
	}

	for (size_t i = 0; i < count; i++) {
		char off = 7 - (i % 8);
		uint8_t bytes = data;
		buf[i] = 1 & (bytes[i / 8] >> off);
	}

	data = buf;
	break;
}
@

In the [[DWA_TINT]] case,
we do not have an 8-bit signed integer representation 
that we can use because of a limitation in the underlying ArrayFire
implementation.
Instead, we must convert these values to 16-bit values.

<<Cases for pre-processing DWA [[data]] buffer>>=
case DWA_TINT:{
	int16_t *buf = calloc(count, sizeof(int16_t));

	if (buf == NULL) {
		err = 1;
		break;
	}

	for (size_t i = 0; i < count; i++)
		buf[i] = ((int8_t *)data)[i];

	data = buf;
	break;
}
@

Since we have allocated new memory for these data types,
we must also remember to clean them up at the end,
which we handle with these cleanup cases.

<<Cases for cleaning up the DWA [[data]] buffer>>=
case DWA_BOOL:
case DWA_TINT:
	free(data);
	break;
@

We must also handle getting the right array type
for a given DWA element types. 
We assume that when converting from a DWA value
to a runtime array
that we will want to calculate a [[type]] value
from each DWA element type.

<<Cases for selecting [[type]] based on DWA type>>=
case DWA_BOOL:
	type = ARR_BOOL;
	break;
case DWA_TINT:
case DWA_SINT:
	type = ARR_SINT;
	break;
case DWA_INT:
	type = ARR_INT;
	break;
case DWA_DBL:
	type = ARR_DBL;
	break;
case DWA_CMPX:
	type = ARR_CMPX;
	break;
@

The previous cases will now allow us to go 
from a DWA value to a runtime value,
but we must also go in the other direction.
We must be able to take a runtime buffer and convert it
into a DWA value.
Fortunately, going in the opposite direction is much easier,
because the runtime numeric types all have a bit-compatible
analogue in the DWA numeric element types.
This means that we can do a straight bulk copy
into the DWA buffer assuming
that we know the correct DWA type
for each [[enum array_type]] without any pre-processing.
We will assume a [[switch]] statement over the runtime array type
wherein we set [[type]] to the appropriate dwa type.

<<Cases for selecting [[type]] based on array type>>=
case ARR_BOOL:
	type = DWA_TINT;
	break;
case ARR_SINT:
	type = DWA_SINT;
	break;
case ARR_INT:
	type = DWA_INT;
	break;
case ARR_DBL:
	type = DWA_DBL;
	break;
case ARR_CMPX:
	type = DWA_CMPX;
	break;
@

With the above in place, 
we now can handle data buffers in literal and DWA form 
and we can correctly store numeric data
as the correct ArrayFire type.

Obviously, this is not a complete handling of array values.
All we have done here is manage the numeric logic.
The logic for array handling that is common among all elements
and units,
such as strings and variables,
will be discussed in a separate section.
For now, this completes the handling of numeric values
across the parser, compiler, code generator, and runtime.

\subsection{Variables}

Besides character vectors and array literals,
variables are the next atomic, user-defined unit
we must handle.
Variables may be bound to functions, operators, namespaces, and arrays.
They can be assigned and referenced.
We must deal with them throughout the lifetime of the compiler pipeline.
How we choose to handle variables affects our approach 
to functions and operators.

Since variables are so caught up in so many things that we do
across many features,
we must decide how much we will do in this section 
and how much we will leave to the future sections.
Here are some things that we will \emph{not} address in this section:

\begin{itemize}
\item Scoping rules and resolving variable references
\item Binding and assignment of variables
\item Computing free references and variable blocks
\item Functions and their methods of variable declarations
\item Closure management, creation, \&c.
\item Variable environments
\item Runtime lookup of variables
\end{itemize}

\noindent
This helps us to minimize the amount of stuff
we must handle in this section.
We \emph{will} address the following topics:

\begin{itemize}
\item Parsing and tokenizing
\item The variable node type and its interpretation
\item Variable node code generation and handling
\item The underlying runtime representation of variables
\end{itemize}

Let's begin with tokenizing and parsing.
For the most part, assuming that numbers have been properly masked
in a [[dm]] Boolean vector, 
a variable will just consist of all alphanumeric characters
contiguous to one another and not part of [[dm]].
We can tokenize this easily by marking 
the start of each group with type [[V]],
which is our variable type,
and extending the [[end]] field to point to the end of the group.

<<Tokenize variables>>=
msk←(~dm)∧x∊alp,num
t[i←⍸2<⌿0⍪msk]←V
end[i]←end⌿⍨2>⌿msk⍪0
@

This handles most variables, 
but we note that dfns formals are still a type of variable
in some sense,
so we must deal with them.
However, we will discuss handling them 
and their unqiue quirks in Section \ref{subsubsec:dfns}
on dealing with dfns.

After tokenization, variables play a primary role
in the ambiguity inherent in the APL syntax.
Most notably, in order to fully parse some APL expressions,
we must know the type of the variables involved
in that expression.
We need only refine the variable type in the parser
to the point of knowing whether the variable
is an array, function, namespace, monadic operator,
or dyadic operator.
However, this may not always be possible,
so we must also accommodate ambiguous variables.

Parsing variables is mainly concerned with this type inference,
but because this is a pretty involved subject,
we will dedicate anentire section to it
(see Section \ref{subsec:bindingsandtypes}).
Our main concern here is to make sure 
that our [[V]] type has the appropriate expressivity.

After parsing, the compiler must arrange the closures
in the system to that variables are appropriately threaded
through the system so that assignments
and references ``do the right thing.''
This necessitates the handling of mutation,
particularly at the name level.
We must also ensure that we are able to reference 
the appropriate slot even in the presence of 
mixed dynamically and lexically scoped variables 
as well as global and namespace references.

If we eliminate the particular issues
that are unique to the specific uses of variables,
we find a few things that are common across all uses:

\begin{itemize}
\item Mutation
\item Type
\item Value/Dereferencing
\item Declaration
\item Initialization
\item Release
\end{itemize}

Let's first consider typing.
We mainly want to know whether a variable
is mutable or immutable,
and its primary nameclass.
This leads to the types for the [[k]] field 
given in Table \ref{tab:variablekinds}.

\begin{table}[htp]
\centering
\caption{Variable Kinds and their Meanings}
\label{tab:variablekinds}
\begin{tabular}{cl}
\toprule
Kind & Meaning \\
\midrule
[[¯N]] & A mutable variable of type [[N]] \\
[[0]] & Unresolved (Parser) or Stack (Code Gen.) \\
[[1]] & Array \\
[[2]] & Function \\
[[3]] & Monadic Operator \\
[[4]] & Dyadic Operator \\
[[5]] & Namespace \\
[[6]] & Ambiguous \\
[[1N]] & Local variable of type [[N]] \\
[[2N]] & Lexical free variable of type [[N]] \\
[[3N]] & Dynamic free variable of type [[N]] \\
\bottomrule
\end{tabular}
\end{table}

Actually assigning types to variable is discussed
in Section \ref{subsec:bindingsandtypes}.
Once types are assigned,
the compiler must also mark the mutable variables
before code generation 
so that we can properly thread names and memory locations
through the system.
However, not much more needs to be done
in the compiler itself to handle variables,
except that we will note 
that we do not consider a variable itself
as something that we want remaining in the tree
after compilation,
as all the work is handled by other, more operational nodes.
Variables themselves only serve as a link
between pieces of data,
and so they do not \emph{do} very much at the low-level.
This means that by the time we reach the end of tree transformation
and make it to the code generator,
there should be no [[V]] nodes.

Rather than [[V]] nodes
at the point of code generation,
we assume that all the main operational nodes,
such as expression nodes,
which will be in the tree at code generation time,
will maintain a set of argument values.
These argument values will be variables of one kind or another.
There are three places that may generate variable names.
The input variables that come from the input source
are handled in a symbol table created by the parser.
During compilation, we may want to create links 
from one node to another,
such as when we are lifting a function.
We use the node id/address instead of a variable name
to mark that name.
At code generation, since we maintain a stack discipline
for expression evaluation,
we must have some way to talk about pulling items off the stack.

To manage these references,
we have the [[n]] field during parsing and compilation.
\phantomsection
We symbolize our [[n]] field at the end of parsing,
giving a negative value to elements in the symbol table
(see Section \ref{subsec:parser},
pg. \pageref{topic:symbolizenfield}).
By the end of the compilation phase,
we want to unify the combined [[n]] field values somehow.
To do this, we can convert all pointer values
to a reference in the symbol table.
By doing so, we can guarantee that all of our references
will fall into a single unified domain.
By simplifying and unifying the [[n]] field 
before code generation rolls around,
we minimize the amount of backend specific code
we must write,
which is a major goal of ours.
We must be careful, however, to not introduce 
any backend specific code into the compiler and parser.

In addition to the name of the variable and its nameclass,
we want to know whether or not the variable
is mutable as well as what scoping rules it is using.
All of this information shoudl be stored
in the argument lists of the code generation nodes
(the [[n]] field data).
Table \ref{tab:variablekinds} shows how the [[k]] field
encodes this additional information.
These prefixes help us to handle 
the interesting mixed scoping situation
we have in APL.
We have four basic ways that a variable may be referenced:

\begin{description}
\item[Local] A variable defined and refernced in the same function
\item[Lexical] A free variable referenced using lexical scoping
\item[Dynamic] A free variable referenced with dynamic scoping
\item[Global] An unprefixed reference in a global scope
\end{description}

\noindent
During compilation, we expect to mark all variables 
as belonging to one of these ``scope'' domains.
This is what the work on parsing and lexical analysis should do.

When we make references to other nodes
in the [[n]] field of a [[V]] node,
this comes about usually from lifting some node
or otherwise relocating that node.
These references are essentially unscoped and unambiguous references
that are globally unique.
Thus, these go into the global set.
See [[<<Adjust AST for output>>]] for the initial [[n]] field 
symbol table creation.
By the time we start code generation,
the [[n]] field should have been unified to contain only 
symbol table references.

What does this mean we need to do?
We are assuming that our [[V]] nodes will all be marked
at type inference with the right nameclass,
and a mutability analysis will mark the variables 
as mutable or not.
Similarly, a resolution pass will mark the free and local values
and whether or not they are lexical or dynamic.
We also assume that each operational node 
will regularize its AST forms to contain only [[V]] nodes
as arguments.
This leaves us with the following responsibilities:

\begin{itemize}
\item Remove [[V]] nodes into the [[n]] field
\item Unify the [[n]] field of [[V]] nodes to contain only symbols
\end{itemize}

\noindent
Assuming the above is handled, we will be ready for code generation.

Notice, at this point, we have used mostly the [[k]] field
as a means of encoding metadata into our AST,
especially about things like scope and mutability.
This is not the only model that we could use.
Some compilers, such as some Scheme compilers,
will instead apply a code transformation to the source
to take one concept, such as mutable variables,
and express it in terms of another.
we could convert variable mutation into mutation on the cell 
of a scallar box array, for instance.
Likewise, we could convert dynamic scope into parameters 
and use something akin to [[DYNAMIC-WIND]] in Scheme
to implement dynamic scope.
There is much to recommend this approach,
but in considering it here, 
I am not convinced that it will minimize the runtime kernel burden,
and it would certainly make the compiler more complex.
Thus, in my estimation, it is simpler in this case
to simply mark variables appropriately
and then produce the right code from that,
since that would have been necessary anyways 
to make things at all performant.

Let's unify the [[n]] field. 
Since this means taking all of the positive [[n]] field values
of [[V]] nodes and creating new names for them,
we must consider the possibility of name clashes.
If we do not have some way of prefixing our names,
then we must have the chance of a conflict.
Fortunately, if we assume all user-given names
are marked as either one of lexical, dynamic, or local,
then we can mark all pointer variables as global
and be sure that they all live in separate ``scope'' domains.
All we must do is ensure that code generation takes
these prefixes into account and we are golden.

We must generate names for our pointer variables
only after we are sure that all the lifting passes
that might introduce more pointer variables
are done.
Then, we simply generate new names and update 
the [[sym]] table and the appropriate [[n]] fields.

<<Namify pointer variables>>=
i←⍸(t=V)∧n≥0
sym∪←x←('ptr',⍕)¨n[i]
n[i]←-sym⍳x
@

\noindent
There is no need to mark them with any kind of prefix
because the global prefix is the default.

Towards the end of the transformations,
all the operational nodes will have unified and flattened
their representations
so that they contain only variables.
In previous iterations of this compiler,
we let [[V]] nodes lift into the stack as well.
This was simpler in some ways,
but resulted ultimately in a more complex runtime
that was too dependent on runtime language features
while also not taking into account that we were working
in higher level langauges and could thus afford
to use the name resolution in those languages.

By making the variables arguments to more operational nodes,
we simplify the runtime semantics while also putting variables
in a context with more information about their intended use,
such as whether they are meant as an lvar to be assigned 
or a value to dereference.
Moreover, not always pushing things on and off the stack 
makes things faster as well.

Our concern now is to remove the [[V]] nodes from the AST
and instead put them into the [[n]] field of their parents.
We must decide how we would like to encode this.
We have two main sources of information in a [[V]] node:
the [[k]] field and the [[n]] field. 
We could attempt to merge these,
but any merge would use up more space
or be too complex to decode.
Instead, we can simply treat them as independent pairs
that we want to put into the [[n]] field.
The result is that we will give each operational node
with [[V]] node children a [[2 N]] matrix of [[V]] nodes
where [[N]] is the number of children. 
Row 0 will be the [[k]] field and row 1 the [[n]] field.

When merging the [[V]] nodes,
we must be careful to properly delete
the [[V]] nodes after they have been merged.
This requires that we use the usual idioms for node deletion,
but also that we avoid doing a pointer recomputation
on the newly added data stored in the [[n]] field.
We do this by filtering out any non-scalar
[[n]] field data, 
since this indicates that we are working with non-pointer data.

<<Merge [[V]] nodes into [[n]] fields>>=
_←p[i]{n[⍺]←⊂⍉k[⍵],⍪n[⍵]}⌸i←⍸t=V
p←i(⊢-1+⍸)(msk←t≠V)⌿p
r←i(⊢-1+⍸)msk⌿r
t k ss se(⌿⍨)←msk
n←i(⊢-1+⍸)@(0=≡¨)msk⌿n
@

With that, all the variables are ready for code generation.
Code generation for variables is different from handling
the operational nodes since we have moved the [[V]] nodes
from the AST.
Instead, we must provide the appropriate utilities
so that the code generators for the operational nodes
can generate various uses for variables as needed.
What functionality do we need and what utility function
will handle this functionality? 
Table \ref{tab:variableutilities} summarizes this information.

\begin{table}[htp]
\caption{Utilities for handling variables in code generation}
\label{tab:variableutilities}
\centering
\begin{tabular}{llll}
\toprule
Function & Left Arg. & Right Arg. & Intent \\
\midrule
[[var_ckinds]] & --- & types & Base C type-prefix \\
[[decl_vars]] & types & names & Declares variables \\
[[init_vars]] & types & names & Initializes variables \\
[[clean_vars]] & --- & name\_strs & Sanitizes APL names \\
[[var_values]] & types & names & References to value \\
[[var_refs]] & types & names & References to slot \\
[[kill_vars]] & types & names & Release statements \\
\bottomrule
\end{tabular}
\end{table}

Let's handle variable declaration first.
Since we retain the variable type information 
at code generation time,
we could produce fairly tight types 
for our declarations.
We are bolstered in this thinking
because we know that the typeclass 
of such values cannot change
in our code.
In some cases, this means that we may definitely need
to do some type casting for all to be well.
However, we hope that we will be more typed overall
than less typed,
and that would encourage us to use the types.
Since we will also not be putting variables on the stack
in most cases,
the main source of type ambiguity is removed
and so it makes more sense to use the types than to ignore them.

When we declare a variable,
we mostly just care about the nameclass and its mutability,
not its scope,
since the declaration syntax is the same for all scopes.
Table \ref{tab:variabletypeequivalence} gives the type
in C for each variable type.

\begin{table}[htp]
\caption{Kind and type equivalencies between C and APL}
\label{tab:variabletypeequivalence}
\centering
\begin{tabular}{clll}
\toprule
Kind & Prefixed & Prefixed Mutable & Global \\
\midrule
1 & [[array]] & [[array_box]] & N/A \\
2 & [[func]] & [[func_box]] & [[func_ptr]] \\
3 & [[moper]] & [[moper_box]] & [[moper_ptr]] \\
4 & [[doper]] & [[doper_box]] & [[doper_ptr]] \\
5 & [[env]] & [[env_box]] & [[env_ptr]] \\
6 & [[void]] & [[void_box]] & N/A \\
\bottomrule
\end{tabular}
\end{table}

To help simplify our utilities, 
we define a utility [[var_ckinds]] that gives the various base types
in the appropriate order, 
shifted by one to omit the stack variable type.

<<Variable utilities>>=
var_ckinds←{
	0∊⍵:'STACK VARIABLES HAVE NO C KIND'⎕SIGNAL 99
	types←'' 'array' 'func' 'moper' 'doper'
	types,←'env' 'void'
	types[10||⍵]
}
@ %def var_ckinds

However, when we declare a variable, it is almost never in isolation.
We only ever declare variables as part 
of a function, namespace, or guard block,
and that means we almost always want to declare mutliple variables 
at a time.

We could consider all of the structures that we might want
to make as a part of declaring capture lists for our various types
in this section,
but it is better that these feature-specific considerations
are handled in their own sections.
We want to focus here on variables alone.
We also note that the declaration of global pointers 
is special because they have unique types
that are not allocated like cells.
Because of this,
we will not declare anything particular for those,
but instead assume that we have an appropriate [[typedef]]
defined for each such type.

Thus, given a set of types on the left and names on the right,
[[decl_vars]] will produce the declaration for each type and name
as a vector of character vectors.

<<Variable utilities>>=
decl_vars←{
	0∊⍺:'CANNOT DECLARE STACK VARIABLE'⎕SIGNAL 99
	1∊|⍺:'CANNOT DECLARE GLOBAL ARRAY'⎕SIGNAL 99
	6∊|⍺:{
		EM←'CANNOT DECLARE AMBIGUOUS GLOBAL'
		EM ⎕SIGNAL 99
	}⍬
	z←'' 'struct cell_'[10≤|⍺]
	z,¨←var_ckinds ⍺
	z,¨←'' '_ptr'[10≥|⍺]
	z,¨←' ' '_box '[¯10≥⍺]
	z,¨←'' '*'[10≤|⍺]
	z,¨←clean_vars sym[|⍵]
	z,¨';'
}
@ %def decl_vars

The [[decl_vars]] utility and all the other utilities depend 
on a function [[clean_vars]] that makes sure the name
that we use in the generated code is valid.
Among the names and compilers that I tested,
only the [[∆]] and [[⍙]] were disallowed,
so we must map these characters to something else in our source.
For our uses, it seems reasonable to mpa them 
to [[_del_]] and [[_delubar_]], respectively.
In addition to normal variables,
we must also handle the formal arguments used by dfns.
We will convert them to a fully spelled out variant.

<<Variable utilities>>=
clean_vars←{
	cequv←'_del_' '_delubar_' 'alpha' 'omega'
	(,¨'∆⍙⍺⍵')⎕R cequv⊢⍵
}
@ %def clean_vars

Next, we must consider how we want to initialize our variables.
For our global and immutable variables,
assigning them to [[NULL]] will suffice,
but for our mutable values,
we must allocate them with an appropriate [[_box]] type.
These boxes are meant to contain the real value so that
mutations to the cell value will propagate to all references 
to the variable.

Why do we make a separate box type for each value type in our system?
This goes back to our discussion above about managing our types.
Since all the variables should end up with the same nameclass
throughout their lives,
we want to leverage the C runtime type system
as an extra check to ensure that we catch our mistakes
a little earlier than we might if we had just made
all mutable variables becomes a [[struct cell_void_box]] type.

Initializing mutable variables means making a call 
to the corresponding [[mk_type()]] function 
and handling any errors.
We follow the same argument conventions as for [[decl_vars]],
namely, types on the elft and name on the right.

<<Variable utilities>>=
init_vars←{
	0∊⍺:{
		EM←'CANNOT INITIALIZE STACK VARIABLE'
		EM ⎕SIGNAL 99
	}⍬
	(10>|⍺)∧⍺<0:{
		EM←'GLOBAL VARIABLES CANNOT BE MUTABLE'
		EM ⎕SIGNAL 99
	}⍬
	z←(≢⍵)⍴⊂⊂''
	i←⍸⍺>0
	z[i]←⊂¨(⍺[i] var_refs ⍵[i]),¨⊂' = NULL;'
	init←{
		z←⊂'err = mk_',⍺,'_box(&',⍵,', NULL);'
		z,←⊂'if (err)'
		z,←⊂TB,'goto fail;'
	z}
	types←var_ckinds ⍺[i]
	names←⍺[i] var_refs ⍵[i]
	z[i]←types init names
	⊃⍪⌿z
}
@ %def init_vars

Now we can declare and initialize variables
that we want to use,
but the interesting issues of getting values
out of a name and refering to a name are up next.
We need two different functions to encode this functionality, namely,
[[var_refs]] and [[var_values]]. 
Why two?
If we want to mutate, set, or access a variable's value,
this may be different than the reference to that variable.
When passing a variable as a free variable to another function
via closure,
we want a reference to the variable,
so that a mutation will propagate to the original variable slot.
However, if we have a mutable variable,
that reference will be to the box holding the value
and not the variable value itself,
so we must also be able to reference the value 
for mutation, binding, or use.

Let's start with [[var_refs]], since a variable reference
is the same regardless of the mutability of a variable.
we also do not really care about the type at this point, either.
We do care about the scope. We want to prefix our variable 
with the appropriate [[struct]] prefix 
to access the right variable.
In the case of locals, we assume that there 
is a structure value name [[loc]] that contains the variables.
The same holds true for the lexical variables 
that we expect to be in [[lex]], 
and the dynamic variables in [[dyn]], 
with the difference that [[lex]] and [[dyn]] are pointers 
to structs and not the structs themselves.
Thus, our only real goal is to produce 
an appropriately prefixed reference.
As with variable declarations and initializations,
we generally want to reference more than one variable at a time,
since the main use of references are in passing between functions.

The only variables that make sense for reference
are those that will be passing between functions,
and that means any stack varaibles need not apply.
We can thus think of a variable reference
as matching the following pattern:

\begin{verbatim}
<scope><deref><name>
\end{verbatim}

\noindent
And that gives us the following code.

<<Variable utilities>>=
var_refs←{
	0∊⍺:{
		EM←'CANNOT REFERENCE STACK VARIABLE'
		EM ⎕SIGNAL 99
	}⍬
	z←'' 'loc.' 'lex->' 'dyn->'[⌊10×|⍺]
	z,¨clean_vars sym[|⍵]
}
@ %def var_refs

Now we can tackle the [[var_values]] function,
which is used much like the [[var_refs]] function 
but with the intent to always get the actual value
of a variable instead of its slot.
Here, we must support the stack variable type.

The pattern for a value reference is much the same
as that of [[var_refs]] except that we must dereference
a mutable box if a variable is mutable
and we must also handle dereferencing a stack value.
In the case of a box,
we assume that all boxes store their contents
in a [[value]] field.
This gives us the following patterns
for non-stack variable values:

\begin{verbatim}
{loc.,lex->,dyn->,}name[->value]
\end{verbatim}

\noindent
When we encounter a stack variable,
we must treat this as if we were popping something off the stack.
We assume that a value [[stkhd]] will be a pointer
to the next unused slot in the stack,
meaning that we can pop the topmost value off 
the stack with the following:

\begin{verbatim}
*--stkhd
\end{verbatim}

\noindent
This gives us the following definition for [[var_values]].

<<Variable utilities>>=
var_values←{
	z←'' 'loc.' 'lex->' 'dyn->'[⌊10×|⍺]
	z,¨←clean_vars sym[|⍵]
	z,¨←'' '->value'[⍺<0]
	z[⍸0=⍺]←⊂'*--stkhd'
z}
@ %def var_values

Finally, we want to be able to release a variable.
We could get away without this utility,
since we already have runtime functions
of the form [[release_type()]] that serve this purpose,
but having an explicit [[kill_vars]] function allows us 
to generate type-specific kill statements 
without invoking dynamic dispatch
over the cell type 
via the [[release_cell()]] function.

<<Variable utilities>>=
kill_vars←{
	type←var_ckinds ⍺
	'release_'∘,¨type,¨'(',¨(⍺ var_refs ⍵),');'
}
@ %def kill_vars

With these utilities in place,
we have a sufficiently expressive vocabulary
for handling names in the future code sessions.
However, we are purposefully omitting utilities for name listing.
With any dynamic code such as an implementation of Execute ([[⍎]])
or [[⎕NC]],
there is a need to know what names exist in a given scope.
In order to do this at runtime,
we must maintain a name list
of all variables in a given scope.
We choose not to provide a utility function
for this because the exact details of this may change
between features,
but also because it is quite easy to generate a name list
from [[sym[|⍵]]] without the need for a utility function.
The main consideration is remembering to use wide characters
for names and string literals.

With all the variable code generation utilities done,
we turn our attention to handling variables at runtime.
Fortunately, most of this is already done for us.
Variables themselves mostly rely on the underlying target language's
handling of variable names.
Managing memory and the like is also 
not a variable-specific consideration.

Variables do introduce one semantic conept 
that persists into the runtime that belongs uniquely to variables,
mutability.
In order to support mutability in the langauge with our present design,
we must have a unique box type for each of our cell types
in the system.

We define the following box types:

<<Cell type names>>=
CELL_VOID_BOX,
CELL_ARRAY_BOX,
CELL_FUNC_BOX,
CELL_MOPER_BOX,
CELL_DOPER_BOX,
CELL_ENV_BOX,
@ %def CELL_VOID_BOX CELL_ARRAY_BOX CELL_FUNC_BOX CELL_MOPER_BOX CELL_DOPER_BOX CELL_ENV_BOX

Each box type follows the same basic construction,
with a single [[value]] field to hold a pointer to the non-boxed value.

<<C runtime structures>>=
struct cell_array_box {
	<<Common cell fields>>
	struct cell_array *value;
}
struct cell_func_box {
	<<Common cell fields>>
	struct cell_func *value;
}
struct cell_moper_box {
	<<Common cell fields>>
	struct cell_moper *value;
}
struct cell_doper_box {
	<<Common cell fields>>
	struct cell_doper *value;
}
struct cell_env_box {
	<<Common cell fields>>
	struct cell_env *value;
}
struct cell_void_box {
	<<Common cell fields>>
	struct cell_void *value;
}
@ %def cell_void_box cell_array_box cell_func_box cell_moper_box cell_doper_box cell_env_box

The maker and releaser functions also remain similar among all box types.

<<box.c>>=
#include <stdlib.h>
#include "codfns.h"

#define DEF_BOX_FNS(type, name) \
DECLSPEC int \
mk_##type##_box(struct cell_##type##_box **box,
    struct cell_##type *value) \
{\
	struct cell_##type##_box *tmp;\
\
	tmp = malloc(sizeof(struct cell_##type##_box));\
\
	if (tmp == NULL)\
		return 1;\
\
	tmp->ctype = CELL_##name##_BOX;\
	tmp->refc = 1;\
	tmp->value = value;\
\
	*box = tmp;\
\
	return 0;\
}\
\
DECLSPEC void\
release_##type##_box(struct cell_##type##_box *box)\
{\
	if (box == NULL)\
		return;\
\
	if (!box->refc)\
		return;\
\
	box->refc--;\
\
	if (box->refc)\
		return;\
\
	release_##type(box->value);\
	free(box);\
	box = NULL;\
}

DEF_BOX_FNS(void, VOID);
DEF_BOX_FNS(array, ARRAY);
DEF_BOX_FNS(func, FUNC);
DEF_BOX_FNS(moper, MOPER);
DEF_BOX_FNS(doper, DOPER);
DEF_BOX_FNS(env, ENV);
@ %def mk_void_box release_void_box mk_array_box release_array_box mk_func_box release_func_box mk_moper_box release_moper_box mk_doper_box release_doper_box mk_env_box release_env_box

<<Tangle Commands>>=
echo "Tangling rtm/box.c..."
notangle -R'box.c' codfns.nw > rtm/box.c
@ %def box.c

<<C runtime declarations>>=
DECLSPEC int mk_array_box(struct cell_array_box **,
     struct cell_array *);
DECLSPEC int mk_func_box(struct cell_func_box **,
     struct cell_func *);
DECLSPEC int mk_moper_box(struct cell_moper_box **,
     struct cell_moper *);
DECLSPEC int mk_doper_box(struct cell_doper_box **,
     struct cell_doper *);
DECLSPEC int mk_env_box(struct cell_env_box **,
     struct cell_env *);
DECLSPEC int mk_void_box(struct cell_void_box **,
     struct cell_void *);
DECLSPEC void
release_array_box(struct cell_array_box *);
DECLSPEC void
release_func_box(struct cell_func_box *);
DECLSPEC void
release_moper_box(struct cell_moper_box *);
DECLSPEC void
release_doper_box(struct cell_doper_box *);
DECLSPEC void
release_env_box(struct cell_env_box *);
DECLSPEC void
release_void_box(struct cell_void_box *);
@

And, finally, we link all this up in the generic release cases.

<<Cell release cases>>=
case CELL_ARRAY_BOX:
	release_array_box(cell);
	break;
case CELL_FUNC_BOX:
	release_func_box(cell);
	break;
case CELL_MOPER_BOX:
	release_moper_box(cell);
	break;
case CELL_DOPER_BOX:
	release_doper_box(cell);
	break;
case CELL_ENV_BOX:
	release_env_box(cell);
	break;
case CELL_VOID_BOX:
	release_void_box(cell);
	break;
@

And with that,
all the box types are defined and the necessary runtime support
for variable handling is complete.

This concludes the variables section.
However,
this section is sensitive to the other sections
in that if the types that are used throughout the system
ever change,
this section must be updated.
Likewise,
we must put any new semantics that might affect 
how we support and handle variables
in here.

\subsection{Arrays}
\label{sec:langfeatarrays}

<<Mark atoms, characters, and numbers as kind [[1]]>>=
k[⍸t∊A C N]←1
@

<<Strand arrays into atoms>>=
i←|i⊣km←0<i←i[⍋|(i,⍨←-∪p[i]),p[i←⍸t[p]∊B Z]]
msk←(t[i]∊C N)∨msk∧⊃1 ¯1∨.⌽⊂msk←km∧(t[i]∊A C N V Z)∧k[i]=1
np←(≢p)+⍳≢ai←i⌿⍨am←2>⌿msk⍪0 ⋄ p←(np@ai⍳≢p)[p] ⋄ p,←ai ⋄ km←2<⌿0⍪msk
t k n pos end(⊣,I)←⊂ai ⋄ k[ai]←1 6[∨⌿¨msk⊆t[i]≠N]
t n pos(⊣@ai⍨)←A(⊂'')(pos[km⌿i]) ⋄ p[msk⌿i]←ai[(msk←msk∧~am)⌿¯1++⍀km]
i←⍸(t[p]=A)∧(k[p]=6)∧t=N
p,←i ⋄ t k n pos end(⊣,I)←⊂i ⋄ t k n(⊣@i⍨)←A 1(⊂'')
@

<<Count strand and indexing children>>=
n[⍸(t∊A E)∧k=6]←0 ⋄ n[p⌿⍨(t[p]∊A E)∧k[p]=6]+←1
@

<<Node $\longleftrightarrow$ Generator mapping>>=
gck,←(A 1)(A 6)
gcv,←'Aa' 'As'
@

<<Declare top-level array structures>>=
k[⍵]=1:{
	z ←⊂'struct array *',n,';'
z}⍵
@

<<Cell type names>>=
CELL_ARRAY, 
@ %def CELL_ARRAY

<<C runtime enumerations>>=
enum array_type {
	ARR_SPAN,
	<<Array element types>>
	ARR_MIXED, ARR_NESTED
};

enum array_storage {
	STG_HOST, STG_DEVICE
};
@ %def array_type array_storage

<<C runtime structures>>=
struct cell_array {
	<<Common cell fields>>
	enum array_storage storage;
	enum array_type type;
	void *values;
	unsigned int rank;
	unsigned long long shape[];
};
@ %def cell_array

<<Array definitions>>=
DECLSPEC int
mk_array(struct cell_array **dest,
    enum array_type type, enum array_storage storage,
    unsigned int rank, unsigned long long *shape, void *values)
{
	struct cell_array *arr;
	size_t	size;
	int	err;

	size = sizeof(struct cell_array) + rank * sizeof(unsigned long long);
	arr = malloc(size);

	if (arr == NULL)
		return 1;

	arr->ctyp	= CELL_ARRAY;
	arr->refc	= 1;
	arr->type	= type;
	arr->storage	= storage;
	arr->rank	= rank;
	arr->values	= NULL;

	size = 1;

	for (unsigned i = 0; i < rank; ++i) {
		arr->shape[i] = shape[i];
		size *= shape[i];
	}

	err = 0;

	switch (storage) {
	case STG_DEVICE:
		err = fill_device_array(arr, values, size, type);
		break;

	case STG_HOST:
		err = fill_host_array(arr, values, size, type);
		break;

	default:
		err = 16;
	}

	if (err) {
		free(arr);
		return err;
	}

	*dest = arr;

	return 0;
}
	


DECLSPEC void
release_array(struct cell_array *arr)
{
	if (arr == NULL)
		return;

	arr->refc--;

	if (arr->refc)
		return;

	if (arr->type == ARR_NESTED) {
		struct cell_array **values = arr->values;

		for (unsigned int i = 0; i < arr->rank; i++)
			release_array(values[i]);
	}

	if (arr->values)
		switch (arr->storage) {
		case STG_HOST:
			free(arr->values);
			break;
		case STG_DEVICE:
			af_release_array(arr->values);
			break;
		default:
			dwa_error(999);
		}

	free(arr);
}
@ %def mk_array release_array

<<C runtime declarations>>=
DECLSPEC int mk_array(struct cell_array **, ...);
DECLSPEC void release_array(struct cell_array *);
@

<<Cell release cases>>=
case CELL_ARRAY:
	release_array(cell);
	break;
@

<<array.c>>=
#include <stddef.h>
#include <stdlib.h>
#include <arrayfire.h>

#include "codfns.h"

#if AF_API_VERSION < 38
#error "Your ArrayFire version is too old."
#endif

int
fill_device_array(struct array *arr, void *vals, size_t size, enum array_type typ)
{
	af_dtype	aftyp;

	arr->values = NULL;

	switch (typ) {
	case ARR_BOOL:
		aftyp = b8;
		break;

	case ARR_SINT:
		aftyp = s16;
		break;

	case ARR_INT:
		aftyp = s32;
		break;

	case ARR_DBL:
		aftyp = f64;
		break;

	case ARR_CMP:
		aftyp = c64;
		break;

	case ARR_NESTED:
	case ARR_CHAR:
	case ARR_MIXED:
	default:
		return 16;
	}

	if (!size) {
		size = 1;

		return af_constant(&arr->values, 0, 1, &size, aftyp);
	}

	return af_create_array(&arr->values, vals, 1, &size, aftyp);
}

int
fill_host_array(struct array *arr, void *vals, size_t size, enum array_type typ)
{
	struct array **data;
	struct pocket **pkts;
	int	err;

	if (typ != ARR_NESTED)
		return 16;

	arr->values = NULL;

	if (!size)
		size++;

	pkts = vals;
	data = calloc(size, sizeof(struct array *));

	if (data == NULL)
		return 1;

	for (size_t i = 0; i < size; i++) {
		err = dwa2array(&data[i], pkts[i]);

		if (err) {
			free(data);
			return err;
		}
	}

	arr->values = data;

	return 0;
}

<<Array definitions>>
@ %def array.c

<<Tangle Commands>>=
echo "Tangling rtm/array.c..."
notangle -R'array.c' codfns.nw > rtm/array.c
@

<<DWA definitions>>=
struct pocket *
getarray(enum dwa_type type, unsigned rank, long long *shape, struct localp *lp)
{
	return (dwa->ws->getarr)(type, rank, shape, lp);
}

char *
cnvu8_ch(uint8_t *buf, size_t count)
{
	char *res;

	res = calloc(count, sizeof(char));

	if (res == NULL)
		return res;

	for (size_t i = 0; i < count; i++)
		res[i] = 1 & (buf[i/8] >> (7 - (i % 8)));

	return res;
}

int16_t *
cnvi8_i16(int8_t *buf, size_t count)
{
	int16_t *res;

	res = calloc(count, sizeof(int16_t));

	if (res == NULL) 
		return res;

	for (size_t i = 0; i < count; i++)
		res[i] = buf[i];

	return res;
}

DECLSPEC int
dwa2array(struct array **tgt, struct pocket *pkt)
{
	struct	array *arr;
	long	long *shape;
	void	*data;
	size_t	count;
	int	err;
	unsigned	int rank;

	rank	= pkt->rank;
	shape	= pkt->shape;
	data	= DATA(pkt);

	switch (pkt->type) {
	case 15: /* Simple */
		switch (pkt->eltype) {
		case APLU8:
			count = 1;

			for (unsigned int i = 0; i < rank; i++)
				count *= shape[i];

			data = cnvu8_ch(data, count);

			if (data == NULL) {
				err = 1;
				goto done;
			}

			err = mk_array(&arr, ARR_BOOL, STG_DEVICE, rank, shape, data);

			free(data);
			break;

		case APLTI:
			count = 1;

			for (unsigned int i = 0; i < rank; i++)
				count *= shape[i];

			data = cnvi8_i16(data, count);

			if (data == NULL) {
				err = 1;
				goto done;
			}

			err = mk_array(&arr, ARR_SINT, STG_DEVICE, rank, shape, data);

			free(data);
			break;

		case APLSI:
			err = mk_array(&arr, ARR_SINT, STG_DEVICE, rank, shape, data);
			break;

		case APLI:
			err = mk_array(&arr, ARR_INT, STG_DEVICE, rank, shape, data);
			break;

		case APLD:
			err = mk_array(&arr, ARR_DBL, STG_DEVICE, rank, shape, data);
			break;

		case APLZ:
			err = mk_array(&arr, ARR_CMP, STG_DEVICE, rank, shape, data);
			break;

		default:
			err = 16;
		}
		break;
	case 7: /* Nested */
		switch (pkt->eltype) {
		case APLP:
			err = mk_array(&arr, ARR_NESTED, STG_HOST, rank, shape, data);
			break;

		default:
			err = 16;
		}
		break;

	default:
		err = 16;
	}

done:
	if (err)
		return err;

	*tgt = arr;

	return 0;
}

DECLSPEC int
array2dwa(struct pocket **dst, struct array *arr, struct localp *lp)
{
	struct	pocket *pkt;
	unsigned	int rank;
	long	long *shape;
	enum	dwa_type dtyp;
	size_t	count, esiz;
	int	err;

	if (arr == NULL) {
		if (lp)
			lp->pocket = NULL;

		goto done;
	}

	rank = arr->rank;
	shape = arr->shape;

	if (rank > 15)
		return 16;

	switch (arr->type) {
	case ARR_BOOL:
		dtyp = APLTI;
		esiz = sizeof(int8_t);
		break;

	case ARR_SINT:
		dtyp = APLSI;
		esiz = sizeof(int16_t);
		break;

	case ARR_INT:
		dtyp = APLI;
		esiz = sizeof(int32_t);
		break;

	case ARR_DBL:
		dtyp = APLD;
		esiz = sizeof(double);
		break;

	case ARR_CMP:
		dtyp = APLZ;
		esiz = sizeof(dcomplex);
		break;

	case ARR_NESTED:
		dtyp = APLP;
		esiz = sizeof(void *);
		break;

	case ARR_MIXED:
	case ARR_CHAR:
	default:
		return 16;
	}

	pkt = getarray(dtyp, rank, shape, lp);

	count = 1;
	for (size_t i = 0; i < rank; i++)
		count *= shape[i];

	switch (arr->storage) {
	case STG_DEVICE:
		err = af_get_data_ptr(DATA(pkt), arr->values);

		if (err)
			return err;

		break;

	case STG_HOST:
		memcpy(DATA(pkt), arr->values, esiz * count);
		break;

	default:
		return 999;
	}

	if (arr->type == ARR_NESTED) {
		void **values = DATA(pkt);

		for (size_t i = 0; i < count; i++) {
			err = array2dwa(&(struct pocket *)values[i], values[i], NULL);

			if (err)
				return err;
		}
	}

done:
	if (dst)
		*dst = pkt;

	return 0;
}
@ %def dwa2array array2dwa

<<C runtime declarations>>=
DECLSPEC int dwa2array(struct cell_array **, void *);
DECLSPEC int array2dwa(void **, struct cell_array *, void *);
@

<<DWA macros>>=
#if defined(_WIN32)
#define dcomplex _Dcomplex
#else
#define dcomplex double complex
#endif

#define DATA(pp) ((void *)&(pp)->shape[(pp)->rank])
@ %def dcomplex DATA

<<DWA structures and enumerations>>=
enum dwa_type { 
	APLNC=0, APLU8, APLTI, APLSI, APLI, APLD, 
	APLP,    APLU,  APLV,  APLW,  APLZ, APLR, APLF, APLQ
};

struct pocket {
	long	long length;
	long	long refcount;
	unsigned	int type	: 4;
	unsigned	int rank	: 4;
	unsigned	int eltype	: 4;
	unsigned	int _0		: 13;
	unsigned	int _1		: 16;
	unsigned	int _2		: 16;
	long	long shape[1];
};
@ %def dwa_type

\subsection{Primitives}

<<Node $\longleftrightarrow$ Generator mapping>>=
gck,←(P 0)(P 1)(P 2)(P 3)(P 4)
gcv,←'Pv' 'Pv' 'Pf' 'Po' 'Po'
@

<<Node-specific code generators>>=
Pf←{id←(syms⍳sym[|4⊃⍺])⊃nams
	z ←⊂'*stkhd++ = retain_cell(',id,');'
z}
@

\subsubsection{APL Primitives}

<<Tokenize primitives and atoms>>=
t[⍸(~dm)∧x∊prms]←P ⋄ t[⍸x∊syna]←A
@

<<Mark APL primitives with appropriate kinds>>=
k[⍸n∊,¨prmfs]←2 ⋄ k[⍸n∊,¨prmmo]←3 ⋄ k[⍸n∊,¨prmdo]←4
k[⍸n∊,¨prmfo]←5
k[i←⍸msk←(n∊⊂,'∘')∧1⌽n∊⊂,'.']←3 ⋄ end[i]←end[i+1] ⋄ n[i]←⊂,'∘.'
t k n pos end⌿⍨←⊂msk←~¯1⌽msk ⋄ p←(⍸~msk)(⊢-1+⍸)msk⌿p
@

\subsubsection{System Functions and Variables}

<<Tokenize system variables>>=
si←⍸('⎕'=IN[pos])∧1⌽t=V
t[si]←S ⋄ end[si]←end[si+1] ⋄ t[si+1]←0
@

<<Verify that system variables are defined>>=
SYSV←,¨'Á' 'A' 'AI' 'AN' 'AV' 'AVU' 'BASE' 'CT' 'D' 'DCT' 'DIV' 'DM'
SYSV,←,¨'DMX' 'EXCEPTION' 'FAVAIL' 'FNAMES' 'FNUMS' 'FR' 'IO' 'LC' 'LX'
SYSV,←,¨'ML' 'NNAMES' 'NNUMS' 'NSI' 'NULL' 'PATH' 'PP' 'PW' 'RL' 'RSI'
SYSV,←,¨'RTL' 'SD' 'SE' 'SI' 'SM' 'STACK' 'TC' 'THIS' 'TID' 'TNAME' 'TNUMS'
SYSV,←,¨'TPOOL' 'TRACE' 'TRAP' 'TS' 'USING' 'WA' 'WSID' 'WX' 'XSI'
SYSF←,¨'ARBIN' 'ARBOUT' 'AT' 'C' 'CLASS' 'CLEAR' 'CMD' 'CONV' 'CR' 'CS' 'CSV'
SYSF,←,¨'CY' 'DF' 'DL' 'DQ' 'DR' 'DT' 'ED' 'EM' 'EN' 'EX' 'EXPORT'
SYSF,←,¨'FAPPEND' 'FCHK' 'FCOPY' 'FCREATE' 'FDROP' 'FERASE' 'FFT' 'IFFT'
SYSF,←,¨'FHIST' 'FHOLD' 'FIX' 'FLIB' 'FMT' 'FPROPS' 'FRDAC' 'FRDCI' 'FREAD'
SYSF,←,¨'FRENAME' 'FREPLACE' 'FRESIZE' 'FSIZE' 'FSTAC' 'FSTIE' 'FTIE'
SYSF,←,¨'FUNTIE' 'FX' 'INSTANCES' 'JSON' 'KL' 'LOAD' 'LOCK' 'MAP' 'MKDIR'
SYSF,←,¨'MONITOR' 'NA' 'NAPPEND' 'NC' 'NCOPY' 'NCREATE' 'NDELETE' 'NERASE'
SYSF,←,¨'NEW' 'NEXISTS' 'NGET' 'NINFO' 'NL' 'NLOCK' 'NMOVE' 'NPARTS'
SYSF,←,¨'NPUT' 'NQ' 'NR' 'NREAD' 'NRENAME' 'NREPLACE' 'NRESIZE' 'NS'
SYSF,←,¨'NSIZE' 'NTIE' 'NUNTIE' 'NXLATE' 'OFF' 'OR' 'PFKEY' 'PROFILE'
SYSF,←,¨'REFS' 'SAVE' 'SH' 'SHADOW' 'SIGNAL' 'SIZE' 'SR' 'SRC' 'STATE'
SYSF,←,¨'STOP' 'SVC' 'SVO' 'SVQ' 'SVR' 'SVS' 'TCNUMS' 'TGET' 'TKILL' 'TPUT'
SYSF,←,¨'TREQ' 'TSYNC' 'UCS' 'VR' 'VFI' 'WC' 'WG' 'WN' 'WS' 'XML' 'XT'
SYSD←,¨'OPT' 'R' 'S'
∨⌿msk←(t=S)∧~n∊'⎕',¨SYSV,SYSF,SYSD:{
	ERR←2'INVALID SYSTEM VARIABLE, FUNCTION, OR OPERATOR'
	ERR SIGNAL∊pos[⍵]{⍺+⍳⍵-⍺}¨end[⍵]
}⍸msk
@

<<Mark system variables as [[P]] nodes with appropriate kinds>>=
k[⍸(t=S)∧n∊'⎕',¨SYSV]←1 ⋄ k[⍸(t=S)∧n∊'⎕',¨SYSF]←2 ⋄ k[⍸(t=S)∧n∊'⎕',¨SYSD]←4
t[⍸t=S]←P
@

\subsection{Brackets}

\subsubsection{Indexing}

<<Convert [[;]] groups within brackets into [[Z]] nodes>>=
_←p[i]{k[z←⊃⍪⌿gz¨g←⍵⊂⍨¯1⌽IN[pos[⍵]]∊';]']←1 ⋄ t[z]←Z P[1=≢¨g]}⌸i←⍸t[p]=¯1
@

<<Verify brackets have function/array target>>=
x←{⍵⌿⍨~∧⍀t[⍵]=¯1}U⌽¨x
0∨.=≢¨x:'BRACKET SYNTAX REQUIRES FUNCTION OR ARRAY TO ITS LEFT'⎕SIGNAL 2
@

<<Enclose [[V[X;...]]] for expression parsing>>=
i←i[⍋p[i←⍸(t[p]∊B Z)∧(k[p]=1)∧p≠⍳≢p]] ⋄ j←i⌿⍨jm←t[i]=¯1
t[j]←A ⋄ k[j]←¯1 ⋄ p[i⌿⍨1⌽jm]←j
@

<<Rationalize [[V[X;...]]]>>=
i←i[⍋p[i←⍸(t[p]=A)∧k[p]=¯1]] ⋄ msk←~2≠⌿¯1,ip←p[i] ⋄ ip←∪ip ⋄ nc←2×≢ip
t[ip]←E ⋄ k[ip]←2 ⋄ n[ip]←⊂'' ⋄ p[msk⌿i]←msk⌿(≢p)+1+2×¯1++⍀~msk
p,←2⌿ip ⋄ t,←nc⍴P E ⋄ k,←nc⍴2 6 ⋄ n,←nc⍴,¨'[' ''
pos,←2⌿pos[ip] ⋄ end,←∊(1+pos[ip]),⍪end[ip] ⋄ pos[ip]←pos[i⌿⍨~msk]
@

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,';' ⋄ nams,←⊂'span'
@

<<Node $\longleftrightarrow$ Generator mapping>>=
gck,←⊂E 6
gcv,←⊂'Ei'
@


\subsubsection{Axis Operator}

<<Rationalize [[F[X]]] syntax>>=
_←p[i]{
	⊃m←t[⍵]=¯1:'SYNTAX ERROR:NOTHING TO INDEX'⎕SIGNAL 2
	k[⍵⌿⍨m∧¯1⌽(k[⍵]∊2 3 5)∨¯1⌽k[⍵]=4]←4
0}⌸i←⍸(t[p]∊B Z)∧(p≠⍳≢p)∧k[p]∊1 2
i←⍸(t=¯1)∧k=4 ⋄ j←⍸(t[p]=¯1)∧k[p]=4
(≢i)≠≢j:{
	2'AXIS REQUIRES SINGLE AXIS EXPRESSION'SIGNAL ∊pos[⍵]+⍳¨end[⍵]-pos[⍵]
}⊃⍪⌿{⊂⍺⌿⍨1<≢⍵}⌸p[j]
∨⌿msk←t[j]≠Z:{
	2'AXIS REQUIRES NON-EMPTY AXIS EXPRESSION'SIGNAL ∊pos[⍵]+⍳¨end[⍵]-pos[⍵]
}msk⌿p[j]
p[j]←p[i] ⋄ t[i]←P ⋄ end[i]←1+pos[i]
@

\subsection{Bindings and Types}
\label{subsec:bindingsandtypes}

<<Parse Binding nodes>>=
⍝ Mark bindable nodes
bm←(t=V)∨(t=A)∧n∊,¨'⎕⍞'
bm←{bm⊣p[i]{bm[⍺]←(V ¯1≡t[⍵])∨∧⌿bm[⍵]}⌸i←⍸(~bm[p])∧t[p]=Z}⍣≡bm

⍝ Binding nodes
_←p[i]{
	t[⍵⌿⍨(n[⍵]∊⊂,'←')∧0,¯1↓bm[⍵]]←B
	b v←{(⊃¨x)(1↓¨x←⍵⌿⍨{t[⊃⍵]=B}¨⍵)}¯1⌽¨⍵⊂⍨1,¯1↓t[⍵]∊P B
	∨⌿~bm[∊v]:'CANNOT BIND ASSIGNMENT VALUE'⎕SIGNAL 2
	p[⍵]←(⍺,b)[0,¯1↓+⍀t[⍵]=B]
	n[b]←n[∊v] ⋄ t[∊v]←¯7 ⋄ pos[b]←pos[∊v] ⋄ end[b]←end[⊃⌽⍵]
0}⌸i←⍸(t[p]=Z)∧p≠⍳≢p
t k n pos end⌿⍨←⊂msk←t≠¯7 ⋄ p←(⍸~msk)(⊢-1+⍸)msk⌿p
@

<<Infer the type of bindings, groups, and variables>>=
z x←↓⍉p[i]{⍺⍵}⌸i←⍸(t[p]∊B Z)∧p≠⍳≢p
<<Verify brackets have function/array target>>
_←{
	k[msk⌿z]←k[x⌿⍨msk←(k[⊃¨x]≠0)∧1=≢¨x]
	z x⌿⍨←⊂~msk

	k[z⌿⍨msk←k[⊃¨x]=4]←3
	z x⌿⍨←⊂~msk

	k[z⌿⍨msk←{(2 3 5∊⍨k[⊃⍵])∨4=(⍵,≢k)[0⍳⍨∧⍀k[⍵]=1]⌷k,0}∘⌽¨x]←2
	z x⌿⍨←⊂~msk

	k[z⌿⍨msk←k[⊃∘⌽¨x]=1]←1
	z x⌿⍨←⊂~msk

	k[i]←k[vb[i←⍸t=V]]
≢z}⍣(=∨0=⊣)≢z
'FAILED TO INFER ALL BINDING TYPES'assert 0=≢z:
@

<<Parse dyadic operator bindings>>=
⍝ PARSE B←D...
⍝ PARSE B←...D
@

<<Node $\longleftrightarrow$ Generator mapping>>=
gck,←(B 1)(B 2)(B 3)(B 4)
gcv,←'Bv' 'Bf' 'Bo' 'Bo'
@

<<Node-specific code generators>>=
Bf←{id←sym⊃⍨|4⊃⍺
	z ←⊂id,' = retain_cell(stkhd[-1]);'
z}
@

\subsection{Assignments}

<<Parse assignments>>=
⍝ Wrap all assignment values as Z nodes
i km←⍪⌿p[i]{(⍺⍪⍵)(0,1∨⍵)}⌸i←⍸(t[p]∊B Z)∧(p≠⍳≢p)∧k[p]∊1
j←i⌿⍨msk←(t[i]=P)∧n[i]∊⊂,'←' ⋄ nz←(≢p)+⍳zc←+⌿msk
p,←nz ⋄ t k n,←zc⍴¨Z 1(⊂'') ⋄ pos,←1+pos[j] ⋄ end,←end[p[j]]
zm←¯1⌽msk ⋄ p[km⌿i]←(zpm⌿(i×~km)+zm⍀nz)[km⌿¯1++⍀zpm←zm∨~km]

⍝ This is the definition of a function value at this point
isfn←{(t[⍵]∊O F)∨(t[⍵]∊B P V Z)∧k[⍵]=2}

⍝ Parse modified assignment to E4(V, F, Z)
j←i⌿⍨m←msk∧(¯1⌽isfn i)∧¯2⌽(t[i]=V)∧k[i]=1 ⋄ p[zi←nz⌿⍨msk⌿m]←j
p[i⌿⍨(1⌽m)∨2⌽m]←2⌿j ⋄ t k(⊣@j⍨)←E 4 ⋄ pos end n{⍺[⍵]@j⊢⍺}←vi zi,⊂vi←i⌿⍨2⌽m

⍝ Parse bracket modified assignment to E4(E6, O2(F, P3(←)), Z)
j←i⌿⍨m←msk∧(¯1⌽isfn i)∧(¯2⌽t[i]=¯1)∧¯3⌽(t[i]=V)∧k[i]=1
p[zi←nz⌿⍨msk⌿m]←ei←i⌿⍨3⌽m ⋄ t k end(⊣@ei⍨)←E 4(end[zi])
p t k n(⊣@(i⌿⍨2⌽m)⍨)←ei E 6(⊂'')
p,←j ⋄ t,←P⍴⍨≢j ⋄ k,←3⍴⍨≢j ⋄ n,←(≢j)⍴⊂,'←' ⋄ pos,←pos[j] ⋄ end,←end[j]
p t k n pos(⊣@j⍨)←ei O 2(⊂'')(pos[fi←i⌿⍨1⌽m]) ⋄ p[fi]←j

⍝ Parse bracket assignment to E4(E6, P2(←), Z)
j←i⌿⍨m←msk∧(¯1⌽t[i]=¯1)∧¯2⌽(t[i]=V)∧k[i]=1 ⋄ p[zi←nz⌿⍨msk⌿m]←ei←i⌿⍨2⌽m
t k end(⊣@ei⍨)←E 4(end[zi]) ⋄ p t k n(⊣@(i⌿⍨1⌽m)⍨)←ei E 6(⊂'')
p t k(⊣@j⍨)←ei P 2

⍝ Parse modified strand assignment
⍝ Parse strand assignment

⍝ SELECTIVE MODIFIED ASSIGNMENT
⍝ SELECTIVE ASSIGNMENT
@

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'←' ⋄ nams,←⊂'get'
@

<<Node $\longleftrightarrow$ Generator mapping>>=
gck,←⊂E 4
gcv,←⊂'Eb'
@

\subsection{Expressions}

<<Parse brackets and parentheses into [[¯1]] and [[Z]] nodes>>=
_←p[i]{
	x←IN[pos[⍵]]
	bd←+⍀bm←(bo←'['=x)+-bc←']'=x
	pd←+⍀pm←(po←'('=x)+-pc←')'=x
	0≠⊃⌽bd:{
		ix←pos[⍵]{x+⍳(⌈⌿⍵)-x←⌊⌿⍺}⍥{⍵⌿⍨0≠bd}end[⍵]
		2'UNBALANCED BRACKETS'SIGNAL ix
	}⍵
	0≠⊃⌽pd:{
		ix←pos[⍵]{x+⍳(⌈⌿⍵)-x←⌊⌿⍺}⍥{⍵⌿⍨0≠pd}end[⍵]
		2'UNBALANCED PARENTHESES'SIGNAL ix
	}⍵
	(po⌿bd)∨.≠⌽pc⌿bd:{
		'OVERLAPPING BRACKETS AND PARENTHESES'⎕SIGNAL 2
	}⍵
	p[⍵]←(⍺,⍵)[1+¯1@{⍵=⍳≢⍵}D2P +⍀¯1⌽bm+pm]
	t[bo⌿⍵]←¯1 ⋄ t[po⌿⍵]←Z
	end[po⌿⍵]←end[⌽pc⌿⍵] ⋄ end[bo⌿⍵]←end[⌽bc⌿⍵]
0}⌸i←⍸(t[p]=Z)∧p≠⍳≢p
t k n pos end⌿⍨←⊂msk←~IN[pos]∊')' ⋄ p←(⍸~msk)(⊢-1+⍸)msk⌿p
@

<<Group function and value expressions>>=
i km←⍪⌿p[i]{(⍺⍪⍵)(0,1∨⍵)}⌸i←⍸(t[p]∊B Z)∧(p≠⍳≢p)∧k[p]∊1 2
@

<<Lift and flatten expressions>>=
p[i]←p[x←p I@{~t[p[⍵]]∊F G}⍣≡i←⍸t∊G A B C E O P V] ⋄ j←(⌽i)[⍋⌽x]
p t k n r{⍺[⍵]@i⊢⍺}←⊂j ⋄ p←(i@j⊢⍳≢p)[p]
@

\subsubsection{Value Expressions}

<<Parse value expressions>>=
i km←⍪⌿p[i]{(⍺⍪⍵)(0,(2≤≢⍵)∧1∨⍵)}⌸i←⍸(t[p]∊B Z)∧(k[p]=1)∧p≠⍳≢p
msk←m2∨fm∧~¯1⌽m2←km∧(1⌽km)∧~fm←(t[i]=O)∨(t[i]≠A)∧k[i]=2
t,←E⍴⍨xc←+⌿msk ⋄ k,←msk⌿msk+m2 ⋄ n,←xc⍴⊂''
pos,←pos[msk⌿i] ⋄ end,←end[p[msk⌿i]]
p,←msk⌿¯1⌽(i×~km)+km×x←¯1+(≢p)++⍀msk ⋄ p[km⌿i]←km⌿x
@

<<Node $\longleftrightarrow$ Generator mapping>>=
gck,←(E 1)(E 2)
gcv,←'Em' 'Ed'
@

<<Node-specific code generators>>=
Em←{
	z ←⊂'c = *--stkhd;'
	z,←⊂'w = *--stkhd;'
	z,←⊂'(c->fn)((struct array **)stkhd++, NULL, w, c->fv);'
	z,←⊂'release_cell(c);'
	z,←⊂'release_cell(w);'
z}
@

\subsubsection{Function Expressions}

<<Parse function expressions>>=
⍝ Mask and verify dyadic operator right operands
(dm←¯1⌽(k[i]=4)∧t[i]∊F P V Z)∨.∧(~km)∨k[i]∊0 3 4:{
	'MISSING RIGHT OPERAND'⎕SIGNAL 2
}⍬

⍝ Refine schizophrenic types
k[i⌿⍨(k[i]=5)∧dm∨¯1⌽(~km)∨(~dm)∧k[i]∊1 6]←2 ⋄ k[i⌿⍨k[i]=5]←3

⍝ Rationalize ∘.
jm←(t[i]=P)∧n[i]∊⊂,'∘.'
jm∨.∧1⌽(~km)∨k[i]∊3 4:'MISSING OPERAND TO ∘.'⎕SIGNAL 2
p←((ji←jm⌿i)@(jj←i⌿⍨¯1⌽jm)⍳≢p)[p] ⋄ t[ji,jj]←t[jj,ji] ⋄ k[ji,jj]←k[jj,ji]
n[ji,jj]←n[jj,ji] ⋄ pos[ji,jj]←pos[ji,ji] ⋄ end[ji,jj]←end[jj,jj]

⍝ Mask and verify monadic and dyadic operator left operands
∨⌿msk←(dm∧¯2⌽~km)∨(¯1⌽~km)∧mm←(k[i]=3)∧t[i]∊F P V Z:{
	2'MISSING LEFT OPERAND'SIGNAL ∊pos[⍵]+⍳¨end[⍵]-pos[⍵]
}i⌿⍨msk
msk←dm∨mm

⍝ Parse function expressions
np←(≢p)+⍳xc←≢oi←msk⌿i ⋄ p←(np@oi⍳≢p)[p] ⋄ p,←oi ⋄ t k n pos end(⊣,I)←⊂oi
p[g⌿i]←oi[(g←(~msk)∧(1⌽msk)∨2⌽dm)⌿xc-⌽+⍀⌽msk]
p[g⌿oi]←(g←msk⌿(1⌽mm)∨2⌽dm)⌿1⌽oi ⋄ t[oi]←O ⋄ n[oi]←⊂''
pos[oi]←pos[g⌿i][msk⌿¯1++⍀g←(~msk)∧(1⌽mm)∨2⌽dm]
ol←1+(k[i⌿⍨(2⌽mm)∨3⌽dm]=4)∨k[i⌿⍨(1⌽mm)∨2⌽dm]∊2 3
or←(msk⌿dm)⍀1+k[dm⌿i]=2
k[oi]←3 3⊥↑or ol
@

<<Node $\longleftrightarrow$ Generator mapping>>=
gck,←(O 1)(O 2)(O 4) (O 5) (O 7) (O 8)
gcv,←'Ov' 'Of' 'Ovv' 'Ofv' 'Ovf' 'Off'
@

\subsection{Trains}

<<Parse trains>>=
⍝ TRAINS
@

\subsection{Functions}

<<Declare top-level function bindings>>=
k[⍵]∊0 2:{
	z ←⊂'int'
	z,←⊂n,'(struct array **z, struct array *l, struct array *r, void *fv[]);'
	z,←⊂''
z}⍵
@

<<Declare top-level closures>>=
k[⍵]=2:{
	z ←⊂'struct closure *',n,';'
	z,←⊂''
	<<DWA Function Export>>
z}⍵
@

<<Cell type names>>=
CELL_CLOSURE,
@ %def CELL_CLOSURE

<<C runtime structures>>=
struct cell_closure {
	<<Common cell fields>>
	int (*fn)(struct cell_array **,
	    struct cell_array *, struct cell_array *, void **);
	unsigned int fs;
	void *fv[];
}
@ %def cell_closure

<<Closure definitions>>=
DECLSPEC int
mk_closure(struct cell_closure **k,
    int (*fn)(struct cell_array **,
        struct cell_array *, struct cell_array *, void **),
    unsigned int fs)
{
	size_t sz;
	struct cell_closure *ptr;

	sz = sizeof(struct cell_closure) + fs * sizeof(void *);
	ptr = malloc(sz);

	if (ptr == NULL)
		return 1;

	ptr->ctyp = CELL_CLOSURE;
	ptr->refc = 1;
	ptr->fn = fn;
	ptr->fs = fs;

	*k = ptr;

	return 0;
}

DECLSPEC void
release_closure(struct cell_closure *k)
{
	if (k == NULL)
		return;

	k->refc--;

	if (k->refc)
		return;

	for (unsigned int i = 0; i < k->fs; i++)
		release_cell(k->fv[i]);
	
	free(k);
}
@ %def mk_closure release_closure

<<C runtime declarations>>=
DECLSPEC int mk_closure(struct cell_closure **,
    int (*)(struct cell_array **,
        struct cell_array *, struct cell_array *, void **),
    unsigned int);
DECLSPEC void release_closure(struct cell_closure *);
@

<<Cell release cases>>=
case CELL_CLOSURE:
	release_closure(cell);
	break;
@

<<Closure definitions>>=
DECLSPEC int
apply_dop(struct cell_closure **z,
    struct cell_closure *op, void *l, void *r)
{
	int err;

	err = mk_closure(z, op->fn, op->fs+2);

	if (err)
		return err;

	(*z)->fv[0] = l;
	(*z)->fv[1] = r;

	memcpy(&(*z)->fv[2], op->fv, op->fs * sizeof(op->fv[0]));

	for (unsigned int i = 0; i < (*z)->fs; i++)
		retain_cell((*z)->fv[i]);

	return 0;
}
@ %def apply_dop apply_mop

<<closure.c>>=
#include <stdlib.h>
#include <string.h>

#include "codfns.h"

<<Closure definitions>>
@ %def closure.c

<<Tangle Commands>>=
echo "Tangling rtm/closure.c..."
notangle -R'closure.c' codfns.nw > rtm/closure.c
@

<<C runtime declarations>>=
@

\subsubsection{Dfns}
\label{subsubsec:dfns}

The formal variables for dfns must be parsed specially.
One issue with this is that, mostly,
these formals are \emph{read-only},
making them not very variable.
Moreover, they do not compose with other units
in the same way that normal variables do to create
things like labels or system bindings.
This creates a little bit of an issue,
since choosing to give dfns formals a distinct type
means we must handle them uniquely when they \emph{do} act
compatibly with variables;
but, choosing to treat them like [[V]] types would mean
excluding them explicitly all the times 
that they do not act like other variables.
Between these two options,
note that there are really only two ways in which dfns formals
start to behave more like variables.
First, the optional syntax for [[⍺]] allowing the form [[⍺←x]]
makes [[⍺]] a bindable thing.
Additionally, together with [[⍺⍺]] and [[⍵⍵]],
it means that [[⍺]] may have an ambiguous type that 
requires special handling at runtime.
However, when we consider all the way sin which a variable token
may be used that are incompatible with dfns formals,
we find it much easier to think of the formals
as their own unique thing.
Then, we can handle binding [[⍺]] as a special case
--- which it is --- and infering types on [[⍺⍺]] and [[⍵⍵]]
can be manually integrated.

Instead of marking [[⍺]] and [[⍵]] as [[V]], 
we will mark them as type [[A]], 
since we \emph{mostly} like to think of them as atomic arrays,
and we will mark [[⍺⍺]] and [[⍵⍵]] as type [[P]] 
at the moment for the same reason.

to identify the actual tokens themselves,
most of the work is adequately done simply by finding
occurrences of [[⍺]] or [[⍵]] in the source,
and accounting for the double [[⍺⍺]] and [[⍵⍵]] cases.
Unfortunately, this is not always the case.
In normal APL, we coudl have something like [[⍺⍺⍺⍺⍺]],
which would parse as [[⍺⍺ ⍺⍺ ⍺]].
We can handle this by observing that [[≠⍀]] applied to a vector
of [[1]]'s produces an alternating sequence of [[1]]'s 
and [[0]]'s equivalent to [[1 0⍴⍨≢]]. 
This gives us a simple way to select the dfns formals.

<<Tokenize potentially contiguous [[⍺]] and [[⍵]] formals>>=
tkm←msk⍀∊≠⍀¨msk⊆msk←'⍺'=x
aam←tkm∧1⌽msk
am←tkm∧~1⌽msk
tkm←msk⍀∊≠⍀¨msk⊆msk←'⍵'=x
wwm←tkm∧1⌽msk
wm←tkm∧~1⌽msk
t[⍸am∨wm]←A
t[i←⍸aam∨wwm]←P
end[i]+←1
@

Note that the [[end]] field above only needs to be modified
in the case of the [[⍺⍺]] and [[⍵⍵]] tokens 
because the [[⍺]] and [[⍵]] tokens will always have length 1
and are set correctly from the start.

While this does parse the formals well enough,
I am less than satisfied,
because the [[⍺⍺⍺⍺⍺]] look is rather poor in my opinion,
and it encourages an ambiguous style.
It is a somewhat arbitrary and confusing to simply mandate
the parser handle things in this manner.%
\footnote{I understand that there is good reason for doing this
  from an architectural standpoint of left to right parsing.}
I consider it less obscene than the issues around
handling exponents and complex numbers
(c.f. Section \ref{subsec:numbers}, pg. 
\pageref{topic:compoundnumberparsing}),
but I am still bothered enough by it to not let it stand.
I want to include how to parse it at the very least
to show how to do it conveniently,
but I still think the right call is to generate a syntax error
whenever two formals are contiguous to one another.

<<Tokenize variables>>=
∨⌿msk←3≤≢¨grp←(pos⊆⍨'⍺'=x),pos⊆⍨'⍵'=x:{
	EM←'AMBIGUOUS FORMALS'
	2 EM SIGNAL ∊msk⌿grp
}⍬
@

Assuming that we are going to error on contiguous formals,
we can parse the dfns formal syntax much more simply, like so.

<<Tokenize variables>>=
msk←('⍺⍺'⍷x)∨'⍵⍵'⍷x
t[i←⍸msk]←P
end[i]+←1
t[⍸(~msk∨¯1⌽msk)∧x∊'⍺⍵']←A
@

<<Compute dfns regions and type, with [[}]] as a child>>=
t[⍸'{'=x]←F ⋄ 0≠⊃d←¯1⌽+⍀1 ¯1 0['{}'⍳x]:'UNBALANCED DFNS'⎕SIGNAL 2
@

<<Compute the nameclass of dfns>>=
k←2×t∊F ⋄ k[∪p⌿⍨(t=P)∧n∊⊂'⍺⍺']←3 ⋄ k[∪p⌿⍨(t=P)∧n∊⊂'⍵⍵']←4
@

<<Wrap all dfns expression bodies as [[Z]] nodes>>=
_←p[i]{end[⍺]←end[⊃⌽⍵] ⋄ gz¨⍵⊂⍨1,¯1↓t[⍵]=Z}⌸i←⍸t[p]=F
'Non-Z dfns body node'assert t[⍸t[p]=F]=Z:
@

<<Check for out of context dfns formals>>=
∨⌿(d=0)∧(t=P)∧IN[pos]∊'⍺⍵':'DFN FORMAL REFERENCED OUTSIDE DFNS'⎕SIGNAL 2
@

<<Convert [[⍺]] and [[⍵]] to [[V]] nodes>>=
t←V@(i←⍸(t=A)∧n∊,¨'⍺⍵')⊢t ⋄ vb[i]←i
@

<<Convert [[⍺⍺]] and [[⍵⍵]] to [[P2]] nodes>>=
k[⍸(t=P)∧n∊'⍺⍺' '⍵⍵']←2
@

<<Anchor variables to earliest binding in the matching frame>>=
rf←¯1@{~t[⍵]∊F G M}p[rz←I@{~(t[⍵]=Z)∧(t[p[⍵]]∊F G M)∨p[⍵]=⍵}⍣≡⍨p]
rf[i]←p[i←⍸t=G] ⋄ rz[i]←i ⋄ rf←rf I@{rz∊p[i]⊢∘⊃⌸i←⍸t[p]=G}rf
mk←{⍺[⍵],⍪n[⍵]}
fr←rf mk⊢fb←fb[⍳⍨rf mk⊢fb←fb I∘(⍳⍨)U⊖rz mk⊢fb←⍸t=B] ⋄ fb,←¯1
vb←fb[fr⍳rf mk i]@(i←⍸t=V)⊢¯1⍴⍨≢p
vb[i⌿⍨(rz[i]<rz[b])∨(rz[i]=rz[b])∧i≥b←vb[i←i⌿⍨vb[i]≠¯1]]←¯1
_←{z/⍨¯1=vb[1⌷z]←fb[fr⍳⍉n I@1⊢z←rf I@0⊢⍵]}⍣≡⍉{rf[⍵],⍪⍵}⍸(t=V)∧vb=¯1
∨⌿msk←(t=V)∧vb=¯1:{
	6'ALL VARIABLES MUST REFERENCE A BINDING'SIGNAL∊pos[⍵]{⍺+⍳⍵-⍺}¨end[⍵]
}⍸msk
@

<<Lift dfns to the top-level>>=
p,←n[i]←(≢p)+⍳≢i←⍸(t=F)∧p≠⍳≢p ⋄ t k n r(⊣,I)←⊂i ⋄ p r I⍨←⊂n[i]@i⊢⍳≢p
t[i]←C
@

<<Wrap expressions as binding or return statements>>=
i←(⍸(~t∊F G)∧t[p]=F),{⍵⌿⍨2|⍳≢⍵}⍸t[p]=G ⋄ p t k n r⌿⍨←⊂m←2@i⊢1⍴⍨≢p
p r i I⍨←⊂j←(+⍀m)-1 ⋄ n←j I@(0≤⊢)n ⋄ p[i]←j←i-1
k[j]←-(k[r[j]]=0)∨0@({⊃⌽⍵}⌸p[j])⊢(t[j]=B)∨(t[j]=E)∧k[j]=4 ⋄ t[j]←E
@

<<Node $\longleftrightarrow$ Generator mapping>>=
gck,←(E ¯1)(E 0)
gcv,←'Ek'  'Er' 
@

<<Compute slots and frames>>=
⍝ Compute slots for each frame
s←¯1,⍨∊⍳¨n[∪x]←⊢∘≢⌸x←0⌷⍉e←∪I∘⍋⍨rn←r[b],⍪n[b←⍸t=B]

⍝ Compute frame depths
d←(≢p)↑d ⋄ d[i←⍸t=F]←0 ⋄ _←{z⊣d[i]+←⍵≠z←r[⍵]}⍣≡i ⋄ f←d[0⌷⍉e],¯1
@

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'∇' ⋄ nams,←⊂'this'
@

<<Node $\longleftrightarrow$ Generator mapping>>=
gck,←(C 1)(C 2)(F 2)(F 3)(F 4)
gcv,←'Ca' 'Cf' 'Fn' 'Fm' 'Fd'
@

<<Node-specific code generators>>=
Cf←{id←⍕4⊃⍺
	z ←⊂'mk_closure((struct closure **)stkhd++, fn',id,', 0);'
z}
@

<<Node-specific code generators>>=
Ek←{
	z ←⊂'release_cell(*--stkhd);'
	z,←⊂''
z}
@

<<Node-specific code generators>>=
Er←{
	z ←⊂'*z = *--stkhd;'
	z,←⊂'goto cleanup;'
	z,←⊂''
z}
@

<<Node-specific code generators>>=
Fn←{id←⍕5⊃⍺ ⋄ x←⍉⊃⍪⌿⍵ ⋄ t←2⌷x ⋄ k←3⌷x
	hsw←(t=O)∨(t=E)∧k∊1 2 ⋄ hsa←((t=E)∧k=2)∨(t=O)∧k∊4 5 7 8
	z ←⊂'int'
	z,←⊂'fn',id,'(struct array **z, '
	z,←⊂'    struct array *l, struct array *r, void *fv[])'
	z,←⊂'{'
	z,←⊂'	void	*stk[128];'
	z,←⊂'	void	**stkhd;'
	z,←hsw⌿⊂'	void	*w;'
	z,←hsa⌿⊂'	void	*a;'
	z,←hsw⌿⊂'	struct	closure *c;'
	z,←⊂''
	z,←⊂'	stkhd = &stk[0];'
	z,←⊂''
	z,← '	',¨⊃,⌿dis¨⍵
	z,←⊂'	*z = NULL;'
	z,←⊂''
	z,←⊂'cleanup:'
	z,←⊂'	return 0;'
	z,←⊂'}'
	z,←⊂''
z}
@

\subsubsection{Trad-fns}

<<Compute trad-fns regions>>=
∨⌿Z≠t⌿⍨1⌽msk←(d=0)∧'∇'=x:'TRAD-FNS START/END LINES MUST BEGIN WITH ∇'⎕SIGNAL 2
0≠⊃tm←¯1⌽≠⍀(d=0)∧'∇'=x:'UNBALANCED TRAD-FNS'⎕SIGNAL 2
∨⌿Z≠t⌿⍨⊃1 ¯1∨.⌽⊂(2>⌿tm)⍪0:'TRAD-FNS END LINE MUST CONTAIN ∇ ALONE'⎕SIGNAL 2
@

\subsection{Guards}

<<Parse guards to [[(G (Z ...) (Z ...))]]>>=
_←p[i]{
	0=+⌿m←':'=IN[pos[⍵]]:⍬
	⊃m:'EMPTY GUARD TEST EXPRESSION'⎕SIGNAL 2
	1<+⌿m:'TOO MANY GUARDS'⎕SIGNAL 2
	t[⍺]←G ⋄ p[ti←gz⊃tx cq←2↑(⊂⍬)⍪⍨⍵⊂⍨1,¯1↓m]←⍺ ⋄ k[ti]←1
	ci←≢p ⋄ p,←⍺ ⋄ t k pos end⍪←0 ⋄ n,←⊂'' ⋄ k[gz cq,ci]←1
0}⌸i←⍸t[p[p]]=F
@

<<Lift guard tests>>=
p[i]←p[x←¯1+i←{⍵⌿⍨~2|⍳≢⍵}⍸t[p]=G] ⋄ t[i,x]←t[x,i] ⋄ k[i,x]←k[x,i]
n[x]←n[i] ⋄ p←((x,i)@(i,x)⊢⍳≢p)[p]
@

<<Node $\longleftrightarrow$ Generator mapping>>=
gck,←⊂G 0
gcv,←⊂'Gd'
@

\subsubsection{Error Guards}

\subsection{Labels}

<<Identify label colons vs. others>>=
t[⍸tm∧(d=0)∧∊((~⊃)∧(<⍀∨⍀))¨':'=(t=Z)⊂IN[pos]]←L
@

<<Tokenize labels>>=
ERR←'LABEL MUST CONSIST OF A SINGLE NAME'
∨⌿(Z≠t[li-1])∨(V≠t[li←⍸1⌽msk←t=L]):ERR ⎕SIGNAL 2
t[li]←L ⋄ end[li]←end[li+1]
d tm t pos end(⌿⍨)←⊂~msk
@

<<Parse labels>>=
⍝ XXX: Parse labels
@

\subsection{Statements}

\subsubsection{What is a keyword?}

<<Tokenize keywords>>=
ki←⍸(t=0)∧(d=0)∧(':'=IN[pos])∧1⌽t=V
t[ki]←K ⋄ end[ki]←end[ki+1] ⋄ t[ki+1]←0
ERR←'EMPTY COLON IN NON-DFNS CONTEXT, EXPECTED LABEL OR KEYWORD'
∨⌿(t=0)∧(d=0)∧':'=IN[pos]:ERR ⎕SIGNAL 2
@

<<Check that all keywords are valid>>=
KW←'NAMESPACE' 'ENDNAMESPACE' 'END' 'IF' 'ELSEIF' 'ANDIF' 'ORIF' 'ENDIF'
KW,←'WHILE' 'ENDWHILE' 'UNTIL' 'REPEAT' 'ENDREPEAT' 'LEAVE' 'FOR' 'ENDFOR'
KW,←'IN' 'INEACH' 'SELECT' 'ENDSELECT' 'CASE' 'CASELIST' 'ELSE' 'WITH'
KW,←'ENDWITH' 'HOLD' 'ENDHOLD' 'TRAP' 'ENDTRAP' 'GOTO' 'RETURN' 'CONTINUE'
KW,←'SECTION' 'ENDSECTION' 'DISPOSABLE' 'ENDDISPOSABLE'
KW,¨⍨←':'
msk←~KW∊⍨kws←n⌿⍨km←t=K
∨⌿msk:('UNRECOGNIZED KEYWORD ',kws⊃⍨⊃⍸msk)⎕SIGNAL 2
@

\subsubsection{Namespaces}

<<Check that namespaces are at the top level>>=
msk←kws∊':NAMESPACE' ':ENDNAMESPACE'
∨⌿msk∧km⌿tm:'NAMESPACE SCRIPTS MUST APPEAR AT THE TOP LEVEL'⎕SIGNAL 2
@

<<Nest top-level root lines as [[Z]] nodes>>=
_←(gz 1⌽⊢)¨(t[i]=Z)⊂i←⍸d=0
'Non-Z top-level node'assert t[⍸p=⍳≢p]=Z:
@

<<Parse [[:Namespace]] syntax>>=
nss←n∊⊂':NAMESPACE' ⋄ nse←n∊⊂':ENDNAMESPACE'
ERR←':NAMESPACE KEYWORD MAY ONLY APPEAR AT BEGINNING OF A LINE'
Z∨.≠t⌿⍨1⌽nss:ERR ⎕SIGNAL 2
ERR←'NAMESPACE DECLARATION MAY HAVE ONLY A NAME OR BE EMPTY'
∨⌿(Z≠t⌿⍨¯1⌽nss)∧(V≠t⌿⍨¯1⌽nss)∨Z≠t⌿⍨¯2⌽nss:ERR ⎕SIGNAL 2
ERR←':ENDNAMESPACE KEYWORD MUST APPEAR ALONE ON A LINE'
∨⌿Z≠t⌿⍨⊃1 ¯1∨.⌽⊂nse:ERR ⎕SIGNAL 2
t[nsi←⍸1⌽nss]←M ⋄ t[nei←⍸1⌽nse]←-M
n[i]←n[1+i←⍸(t=M)∧V=1⌽t] ⋄ end[nsi]←end[nei]
x←⍸p=⍳≢p ⋄ d←+⍀(t[x]=M)+-t[x]=-M
0≠⊃⌽d:':NAMESPACE KEYWORD MISSING :ENDNAMESPACE PAIR'⎕SIGNAL 2
p[x]←x[D2P ¯1⌽d]

⍝ Delete unnecessary namespace nodes from the tree, leave only M's
msk←~nss∨((¯1⌽nss)∧t=V)∨nse∨1⌽nse
t k n pos end⌿⍨←⊂msk ⋄ p←(⍸~msk)(⊢-1+⍸)msk⌿p
@

In the parser,
the [[xn]] and [[xt]] fields are not part of the AST proper, 
but form an auxiliary analysis that is exceptionally useful,
and so we include this as a part of the output of the parser.
After parsing a module, we want to extract out the top-level 
bindings and what their types are,
which we can then use to feed into things like the linker 
and other areas that might need to know what names are available
in a given module.
Top-level bindings are identified as bindings that appear as a 
part of an initialization function, also known as [[F0]].

<<Compute parser exports>>=
msk←(t=B)∧k[I@{t[⍵]≠F}⍣≡⍨p]=0
xn←(0⍴⊂''),msk⌿n ⋄ xt←msk⌿k
@ %def xn xt

<<Record exported top-level bindings>>=
xi←⍸(t=B)∧k[r]=0
@ %def xi

<<Node $\longleftrightarrow$ Generator mapping>>=
gck,←⊂F 0
gcv,←⊂'Fz'
@

<<Node-specific code generators>>=
Fz←{id←⍕5⊃⍺ ⋄ awc←∨⌿(3⌷x){(⍵∊A O)∨(⍵=E)∧⍺>0}2⌷x←⍉⊃⍪⌿⍵
	z ←⊂'int init',id,' = 0;'
	z,←⊂''
	z,←⊂'EXPORT int'
	z,←⊂'init(void)'
	z,←⊂'{'
	z,←⊂' return fn',id,'(NULL, NULL, NULL, NULL);'
	z,←⊂'}'
	z,←⊂''
	z,←⊂'int'
	z,←⊂'fn',id,'(struct array **z, '
	z,←⊂'    struct array *l, struct array *r, void *fv[])'
	z,←⊂'{'
	z,←⊂'	void	*stk[128];'
	z,←⊂'	void	**stkhd;'
	z,← awc⌿⊂'	void	*a, *w;'
	z,← awc⌿⊂'	struct	closure *c;'
	z,←⊂''
	z,←⊂'	if (init',id,')'
	z,←⊂'		return 0;'
	z,←⊂''
	z,←⊂'	stkhd = &stk[0];'
	z,←⊂'	init',id,' = 1;'
	z,←⊂'	cdf_init();'
	z,←⊂''
	z,← '	',¨⊃,⌿dis¨⍵
	z,←⊂'	return 0;'
	z,←⊂'}'
	z,←⊂''
z}
@

<<init.c>>=
#include "codfns.h"

int
init(void);

EXPORT int
cdf_init(void)
{
	return init();
}
@ %def init.c

<<Tangle Commands>>=
echo "Tangling rtm/init.c..."
notangle -R'init.c' codfns.nw > rtm/init.c
@

<<C runtime declarations>>=
DECLSPEC int cdf_init(void);
@

\subsubsection{Structured Programming Statements}

<<Verify that all structured statements appear within trad-fns>>=
msk←kws∊KW~':NAMESPACE' ':ENDNAMESPACE' ':SECTION' ':ENDSECTION'
∨⌿msk←msk∧~km⌿tm:{
	msg←2'STRUCTURED STATEMENTS MUST APPEAR WITHIN TRAD-FNS'
	msg SIGNAL ∊{x+⍳end[⍵]-x←pos[⍵]}¨⍸km⍀msk
}⍬
@

<<Convert [[M]] nodes to [[F0]] nodes>>=
t←F@{t=M}t
@

\section{Runtime Primitives}
\label{sec:primitives}

\subsection{Addition/Identity}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'+' ⋄ nams,←⊂'add'
@

\subsection{And (Logical)}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'∧' ⋄ nams,←⊂'and'
@

\subsection{Bracket}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'[' ⋄ nams,←⊂'brk'
@

\subsection{Catenate (First/Last Axis)}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,',' ⋄ nams,←⊂'cat'
syms,←⊂,'⍪' ⋄ nams,←⊂'ctf'
@

\subsection{Circle/Trigonometrics}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'○' ⋄ nams,←⊂'cir'
@

\subsection{Commute}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'⍨' ⋄ nams,←⊂'com'
@

\subsection{Compose}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'∘' ⋄ nams,←⊂'jot'
@

\subsection{Convolve}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'⎕CONV' ⋄ nams,←⊂'conv'
@

\subsection{Decode}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'⊥' ⋄ nams,←⊂'dec'
@

\subsection{Disclose}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'⊃' ⋄ nams,←⊂'dis'
@

\subsection{Division/Reciprocal}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'÷' ⋄ nams,←⊂'div'
@

\subsection{Drop}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'↓' ⋄ nams,←⊂'drp'
@

\subsection{Each}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'¨' ⋄ nams,←⊂'map'
@

\subsection{Enclose}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'⊂' ⋄ nams,←⊂'par'
@

\subsection{Encode}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'⊤' ⋄ nams,←⊂'enc'
@

\subsection{Equal}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'=' ⋄ nams,←⊂'eql'
@

\subsection{Exponent}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'*' ⋄ nams,←⊂'exp'
@

\subsection{Factorial/Binomial}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'!' ⋄ nams,←⊂'fac'
@

\subsection{Fast Fourier Transforms}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'⎕FFT' ⋄ nams,←⊂'fft'
@

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'⎕IFFT' ⋄ nams,←⊂'ift'
@

\subsection{Find}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'⍷' ⋄ nams,←⊂'fnd'
@

\subsection{Grade Down}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'⍒' ⋄ nams,←⊂'gdd'
@

\subsection{Grade Up}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'⍋' ⋄ nams,←⊂'gdu'
@

\subsection{Greater Than}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'>' ⋄ nams,←⊂'gth'
@

\subsection{Greater Than or Equal}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'≥' ⋄ nams,←⊂'gte'
@

\subsection{Index}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'⌷' ⋄ nams,←⊂'sqd'
@

\subsection{Index Generator}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'⍳' ⋄ nams,←⊂'iot'
@

\subsection{Inner Product}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'.' ⋄ nams,←⊂'dot'
@

\subsection{Intersection}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'∩' ⋄ nams,←⊂'int'
@

\subsection{Left}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'⊣' ⋄ nams,←⊂'lft'
@

\subsection{Less Than}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'<' ⋄ nams,←⊂'lth'
@

\subsection{Less Than or Equal}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'≤' ⋄ nams,←⊂'lte'
@

\subsection{Logarithm}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'⍟' ⋄ nams,←⊂'log'
@

\subsection{Match}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'≡' ⋄ nams,←⊂'eqv'
@

\subsection{Matrix Division}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'⌹' ⋄ nams,←⊂'mdv'
@

\subsection{Maximum/Ceiling}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'⌈' ⋄ nams,←⊂'max'
@

\subsection{Membership}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'∊' ⋄ nams,←⊂'mem'
@

\subsection{Minimum/Floor}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'⌊' ⋄ nams,←⊂'min'
@

\subsection{Multiplication}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'×' ⋄ nams,←⊂'mul'
@

\subsection{Nest/Partition}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'⊆' ⋄ nams,←⊂'nst'
@

\subsection{Not}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'~' ⋄ nams,←⊂'not'
@

\subsection{Not And (Logical)}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'⍲' ⋄ nams,←⊂'nan'
@

\subsection{Not Equal}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'≠' ⋄ nams,←⊂'neq'
@

\subsection{Not Match}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'≢' ⋄ nams,←⊂'nqv'
@

\subsection{Not Or (Logical)}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'⍱' ⋄ nams,←⊂'nor'
@

\subsection{Or (Logical)}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'∨' ⋄ nams,←⊂'lor'
@

\subsection{Outer Product}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'∘.' ⋄ nams,←⊂'oup'
@

\subsection{Power}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'⍣' ⋄ nams,←⊂'pow'
@

\subsection{Rank}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'⍤' ⋄ nams,←⊂'rnk'
@

\subsection{Reduce}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'/' ⋄ nams,←⊂'red'
@

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'⌿' ⋄ nams,←⊂'rdf'
@

\subsection{Roll}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'?' ⋄ nams,←⊂'rol'
@

\subsection{Rotate (First/Last Axis)}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'⌽' ⋄ nams,←⊂'rot'
syms,←⊂,'⊖' ⋄ nams,←⊂'rtf'
@

\subsection{Residue}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'|' ⋄ nams,←⊂'res'
@

\subsection{Right}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'⊢' ⋄ nams,←⊂'rgt'
@

<<APL Primitives>>=
rgt←{⍵}
@ %def rgt

\subsection{Scalar Each}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'%s' ⋄ nams,←⊂'scl'
@

\subsection{Scan}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'\' ⋄ nams,←⊂'scn'
@

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'⍀' ⋄ nams,←⊂'scf'
@

\subsection{Shape}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'⍴' ⋄ nams,←⊂'rho'
@

\subsection{Subtraction}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'-' ⋄ nams,←⊂'sub'
@

\subsection{Take}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'↑' ⋄ nams,←⊂'tke'
@

\subsection{Transpose}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'⍉' ⋄ nams,←⊂'trn'
@

\subsection{Union}

<<Symbol $\longleftrightarrow$ Name mapping>>=
syms,←⊂,'∪' ⋄ nams,←⊂'unq'
@

\section{Utilities}

\subsection{Must haves}

There are some APL functions that are so critical as to be worthy 
of primitive status.

\begin{itemize}
	\item Indexing
	\item Under
	\item Assert
\end{itemize}

<<Must Have APL Utilities>>=
I←{(⊂⍵)⌷⍺}
U←{⍺←⊢ ⋄ ⍵⍵⍣¯1⊢⍺ ⍺⍺⍥⍵⍵ ⍵}
assert←{
	⍺←'assertion failure'
	0∊⍵:⍎'⍺ ⎕SIGNAL 8'
	1:shy←0
}
@ %def assert

\subsection{AST Pretty-printing}

<<Pretty-printing AST trees>>=
dct←{⍺[(2×2≠/n,0)+(1↑⍨≢m)+m+n←⌽∨\⌽m←' '≠⍺⍺ ⍵]⍵⍵ ⍵}
dlk←{((x⌷⍴⍵)↑[x←2|1+⍵⍵]⍺),[⍵⍵]⍺⍺@(⊂0 0)⍣('┌'=⊃⍵)⊢⍵}

dwh←{
	z←⊃⍪/((≢¨⍺),¨⊂⌈/≢∘⍉¨⍺)↑¨⍺
	⍵('┬'dlk 1)' │├┌└─'(0⌷⍉)dct,z
}
dwv←{
	z←⊃{⍺,' ',⍵}/(1+⌈/≢¨⍺){⍺↑⍵⍪⍨'│'↑⍨≢⍉⍵}¨⍺
	⍵('├'dlk 0)' ─┬┌┐│'(0⌷⊢)dct(⊣⍪1↓⊢)z
}

lb3←{
	⍺←⍳≢⊃⍵
	z←(N∆{⍺[⍵]}@2⊢(2⊃⍵){⍺[|⍵]}@{0>⍵}@4↑⊃⍵)[⍺;]
	'(',¨')',¨⍨{⍺,';',⍵}⌿⍕¨z
}

pp3←{
	⍺←'○' ⋄ lbl←⍺⍴⍨≢⍵
	d←(⍳≢⍵)≠⍵ ⋄ _←{z⊣d+←⍵≠z←⍺[⍵]}⍣≡⍨⍵
	lyr←{
		i←⍸⍺=d
		k v←↓⍉⍵⍵[i],∘⊂⌸i
		(⍵∘{⍺[⍵]}¨v)⍺⍺¨@k⊢⍵
	}⍵
	(⍵=⍳≢⍵)⌿⊃⍺⍺ lyr⌿(1+⍳⌈/d),⊂⍉∘⍪∘⍕¨lbl
}
@ %def dct dlk dwh dwv pp3 lb3

\subsection{Debugging utilities}

The following utilities help to improve quality of life when working
with the Co-dfns source code.

The [[DISPLAY]] function is taken from \url{https://dfns.dyalog.com}
and helps to make debugging easier by allowing us to thread
[[DISPLAY]] calls into expressions. I prefer to do something like
this:

\begin{verbatim}
... {⍵⊣⎕←#.DISPLAY ⍵} ...
\end{verbatim}

\noindent
The function itself returns the character rendering of the code,
so the above little expression is one that I use to insert and do
debugging within an expression.

<<[[DISPLAY]] Utility>>=
DISPLAY←{
	⎕IO ⎕ML←0
	⍺←1 ⋄ chars←⍺⊃'..''''|-' '┌┐└┘│─'
	tl tr bl br vt hz←chars
	box←{
		vrt hrz←(¯1+⍴⍵)⍴¨vt hz
		top←(hz,'⊖→')[¯1↑⍺],hrz
		bot←(⊃⍺),hrz
		rgt←tr,vt,vrt,br
		lax←(vt,'⌽↓')[¯1↓1↓⍺],¨⊂vrt
		lft←⍉tl,(↑lax),bl
		lft,(top⍪⍵⍪bot),rgt
	}
	deco←{⍺←type open ⍵ ⋄ ⍺,axes ⍵}
	axes←{(-2⌈⍴⍴⍵)↑1+×⍴⍵}
	open←{(1⌈⍴⍵)⍴⍵}
	trim←{(~1 1⍷∧⌿⍵=' ')/⍵}
	type←{{(1=⍴⍵)⊃'+'⍵}∪,char¨⍵}
	char←{⍬≡⍴⍵:hz ⋄ (⊃⍵∊'¯',⎕D)⊃'#~'}∘⍕
	line←{(6≠10|⎕DR' '⍵)⊃' -'}
	{
		0=≡⍵:' '⍪(open ⎕FMT ⍵)⍪line ⍵
		1 ⍬≡(≡⍵)(⍴⍵):'∇' 0 0 box ⎕FMT ⍵
		1=≡⍵:(deco ⍵)box open ⎕FMT open ⍵
		('∊'deco ⍵)box trim ⎕FMT ∇¨open ⍵
	}⍵
}
@ %def DISPLAY

I also define a function [[PP]] that encapsulates the above usage
pattern that I like to use, making the whole thing less verbose and
a little more convenient.

<<[[PP]] Utility>>=
PP←{⍵⊣⎕←#.DISPLAY ⍵}
@ %def PP

Both of these function exist outside of the [[codfns]] namespace 
and so they get their own files inside of the [[src\]] directory.

<<Tangle Commands>>=
echo "Tangling src/DISPLAY.aplf..."
notangle -R'[[DISPLAY]] Utility' codfns.nw > src/DISPLAY.aplf

echo "Tangling src/PP.aplf..."
notangle -R'[[PP]] Utility' codfns.nw > src/PP.aplf
@ %def DISPLAY.aplf PP.aplf

\subsection{Reading and Writing Files}

It is helpful to be able to easily write files to disk, and the
following [[put]] and [[tie]] utilities help us to do so when we
want to.
These are pretty standard, but they could maybe be replaced by
[[⎕NPUT]] or something like that.

<<Basic [[tie]] and [[put]] utilities>>=
tie←{
	0::⎕SIGNAL ⎕EN
	22::⍵ ⎕NCREATE 0
	0 ⎕NRESIZE ⍵ ⎕NTIE 0
}

put←{
	s←(¯128+256|128+'UTF-8'⎕UCS ⍵)⎕NAPPEND(t←tie ⍺)83
	1:r←s⊣⎕NUNTIE t
}
@ %def tie put

\subsection{XML Rendering}

<<XML Rendering>>=
Xml←{⍺←0
	ast←⍺{d i←P2D⊃⍵ ⋄ i∘{⍵[⍺]}¨(⊂d),1↓⍺↓⍵}⍣(0≠⍺)⊢⍵
	d t k n←4↑ast
	cls←N∆[t],¨('-..'[1+×k]),¨⍕¨|k
	fld←{((≢⍵)↑3↓f∆),⍪⍵}¨↓⍉↑3↓ast
	⎕XML⍉↑d cls(⊂'')fld
}
@ %def Xml

\subsection{Detecting the Operating System}

It is quite helpful to be able to easily detect the operating system
that we are on.
This turns out to be helpful in more areas than just the compiler.

<<The [[opsys]] utility>>=
opsys←{⍵⊃⍨'Win' 'Lin' 'Mac'⍳⊂3↑⊃'.'⎕WG'APLVersion'}
@ %def opsys

\section{Developer Infrastructure}

\subsection{Building the Compiler}

The Co-dfns compiler is written, developed, and distributed as a
literate program.
For more information about literate programming,
see the resources available at \url{http://literateprogramming.com/}.
We use \href{https://www.cs.tufts.edu/~nr/noweb/}{noweb} as our
preferred literate programming tool because it is eminently simple,
while still handling the majority of our needs and producing high
quality output in \LaTeX\ format with all the important elements of
literate programming, including live hyperlinking and cross-references.

\subsubsection{Tangling the Source}

The process of tangling produces the executable source code 
for the compiler.
Importantly, the tangled output is \emph{not} meant to be used 
as the primary means of reading or debugging the source.
Instead, it is meant primarily as the machine readable version
of the code only.

With noweb, we need to invoke [[notangle]] once for each of the 
chunks that we wish to use to produce an output file.
To make this easy, we build up a script to do this work for us.

For Linux and Mac, the following bash script creates these files. 
We use a separate chunk that we build up incrementally 
throughout the rest of this document as a record of all the chunks
that we should create.
Notice that we explicitly tangle the [[TANGLE.sh]] file as the last
thing that we do;
this helps to ensure that we are reliably executing the rest of the 
script before changing the contents of the file,
as some systems will be affected and change execution behavior 
in strange ways if we change the [[TANGLE.sh]] file early on in the 
execution of the file.

<<[[TANGLE.sh]]>>=
#!/bin/bash

<<Tangle Commands>>

echo "Tangling TANGLE.sh..."
notangle -R'[[TANGLE.sh]]' codfns.nw > TANGLE.sh
@ %def TANGLE.sh

On Windows, the best way that we have found to do this is
by installing noweb using the
\href{https://www.cygwin.com/}{Cygwin project}
and then calling [[TANGLE.sh]] from a local [[TANGLE.bat]] file.
This document assumes that you have already successfully built and
installed via Cygwin a working Icon-driven noweb installation.

Users who prefer to work in a UNIX fashion via Cygwin or some other
subsystem on Windows can follow the build scripts directly.
For developers who prefer to work in a primarily Windows environment,
the following [[TANGLE.bat]] build script assists 
in handling the calls into Cygwin
so that you do not need to have a Cygwin terminal open all the time.

<<[[TANGLE.bat]]>>=
set SH=C:\cygwin64\bin\bash.exe -l -c
%SH% "cd $OLDPWD && ./TANGLE.sh"
@ %def TANGLE.bat

<<Tangle Commands>>=
echo "Tangling TANGLE.bat..."
notangle -R'[[TANGLE.bat]]' codfns.nw > TANGLE.bat
@

When tangled to the [[TANGLE.aplf]] file,
the following script enables the
user to simply type [[TANGLE]] within a Dyalog APL session
to update the code tree from within Dyalog itself.
This is much more convenient than keeping a Cygwin Terminal
session open along with a Dyalog APL session while programming.

\emph{Note: this command expects to be run from within the root of
the repository, not from, say, within the [[testing]] directory.}

<<[[TANGLE]]>>=
TANGLE;opsys
<<The [[opsys]] utility>>
⎕CMD opsys '.\TANGLE.bat' './TANGLE.sh' './TANGLE.sh'
@ %def TANGLE

<<Tangle Commands>>=
echo "Tangling TANGLE.aplf..."
notangle -R'[[TANGLE]]' codfns.nw > src/TANGLE.aplf
@ %def TANGLE.aplf

\subsubsection{Weaving the Source}

Weaving is the process by which we produce the final printed output
of this document,
intended for reading and general human consumption.
We rely on the \LaTeX\ typesetting system to do this.
Moreover, because we make heavy use of UTF-8 and prefer to have our
own fonts installed and used,
it is necessary to use the [[xelatex]] system instead of the typical
\LaTeX\ engine.
In order to get the indexing right, we must run the engine twice.
The first run will update the indexing files that will be picked
up on the second run and incorporated into the final document.
Note, we have tried to use the [[lualatex]] engine, which in theory
should work just as well as the [[xelatex]] engine, but we get a
strange error relating to noweb's style file, so we stick with
[[xelatex]] for now.

Running this script also depends on having the appropriate fonts
installed.
In this case, please ensure that the following fonts are installed
in your Windows font system so that they can be picked up by the \TeX\
engine.

\begin{itemize}
	\item Libre Baskerville (Regular, Italic, Bold)
	\item APL385 Unicode
	\item Lucida Sans Unicode
	\item Cambria Math
\end{itemize}

\noindent
If you do not wish to use these fonts, 
edit the font specifications at the top of [[codfns.nw]]
to the fonts that you do wish to use.

Note the use of [[-delay -index]] for options. We want to generate
indexing, but we also need to make sure that we can use some of our
own packages in the system,

\emph{Note: this command expects to be run from within the root of
the repository, not from, say, within the [[testing]] directory.}

<<[[WEAVE.sh]]>>=
#!/bin/bash
mkdir -p woven
noweave -delay -index codfns.nw > woven/codfns.tex
cd woven
xelatex --shell-escape codfns
xelatex --shell-escape codfns
@ %def WEAVE.sh

<<Tangle Commands>>=
echo "Tangling WEAVE.sh..."
notangle -R'[[WEAVE.sh]]' codfns.nw > WEAVE.sh
@

\noindent
And just like the tangling code, we want to define a [[TANGLE.bat]]
batch file to call the Cygwin environment from Windows.

<<[[WEAVE.bat]]>>=
set SH=C:\cygwin64\bin\bash.exe -l -c
%SH% "cd $OLDPWD && ./WEAVE.sh"
@ %def WEAVE.bat

<<Tangle Commands>>=
echo "Tangling WEAVE.bat..."
notangle -R'[[WEAVE.bat]]' codfns.nw > WEAVE.bat
@

Like the [[<<[[TANGLE]] Command>>]], the following command,
when tangled to the [[WEAVE.aplf]]
file enables weaving in a the Dyalog APL session 
by executing the [[WEAVE]] command.

<<[[WEAVE]]>>=
WEAVE;opsys
<<The [[opsys]] utility>>
⎕CMD opsys '.\WEAVE.bat' './WEAVE.sh' './WEAVE.sh'
@ %def WEAVE

<<Tangle Commands>>=
echo "Tangling src/WEAVE.aplf..."
notangle -R'[[WEAVE]]' codfns.nw > src/WEAVE.aplf
@ %def WEAVE.aplf

\subsection{Building the Runtime}

One of our goals with the Co-dfns runtime is to write as much of it
as possible in APL.
This means that we want to have at minimum a very small kernel that
has been written in C,
while most of the rest of the code is implemented in some APL files.
This leads to a three part breakdown of the process to
build the runtime.

<<Build the runtime>>=
<<Compile the primitives in [[prim.apln]]>>
<<Build [[codfns.dll]] DLL>>
<<Copy the runtime files into [[tests\]]>>
@

We define the command [[MK∆RTM]] to build the runtime.
This command takes a path to the root directory of the Co-dfns
repository; this is to allow us to rebuild the runtime from anywhere
in the system if we so choose.

<<[[MK∆RTM]]>>=
MK∆RTM path;put;tie;src;vsbat;vsc;wsd

<<Basic [[tie]] and [[put]] utilities>>
<<Build the runtime>>
@ %def MK∆RTM

This file is another of our external utilities that exists outside 
of the [[codfns]] namespace, so it gets its own file in [[src\]].

<<Tangle Commands>>=
echo "Tangling src/MK∆RTM.aplf..."
notangle -R'[[MK∆RTM]]' codfns.nw > src/MK∆RTM.aplf
@ %def MK∆RTM.aplf

The first step we must take is producing an appropriate C file that
contains the primitives that we have defined in [[prim.apln]].
This means that we want to only compile the code in [[prim.apln]]
as far as producing the C code.
Since we do not have a full blown runtime yet,
we will be compiling the [[prim.c]] file along with the rest of the
runtime code,
instead of the normal build process,
which assumes that we already have a working runtime.
This means that we only invoke the [[GC TT PS]] passes of the
compiler pipeline, while avoiding the [[CC]] pass.
We use the SALT system to load the source from [[prim.apln]] and then
run the compiler passes that we want before storing the resulting
code in the [[rtm\prim.c]] file.

<<Compile the primitives in [[prim.apln]]>>=
src←⎕SRC ⎕SE.SALT.Load path,'\rtm\prim.apln'
(path,'\rtm\prim.c')put codfns.{GC TT PS ⍵}src
@ %def src

Once we have the [[rtm\prim.c]] file written appropriately,
we can run the main compiler process.
For simplicity, we just compile all of the [[.c]] files that
are found in the [[rtm\]] subdirectory.
We must ensure that we are appropriatelly invoking our ArrayFire
dependencies as well as producing the appropriate debugging symbols
most of the time.

<<Build [[codfns.dll]] DLL>>=
vsbat←#.codfns.VS∆PATH
vsbat,'\VC\Auxiliary\Build\vcvarsall.bat'
wsd←path,'\'

vsc←'%comspec% /C ""',vsbat,'" amd64'
vsc,←'  && cd "',wsd,'\rtm"'
vsc,←'  && cl /MP /W3 /wd4102 /wd4275'
vsc,←'    /Od /Zc:inline /Zi /FS'
vsc,←'    /Fo".\\" /Fd"codfns.pdb"'
vsc,←'    /WX /MD /EHsc /nologo'
vsc,←'    /I"%AF_PATH%\include"'
vsc,←'    /D"NOMINMAX" /D"AF_DEBUG" /D"EXPORTING"'
vsc,←'    "*.c" /link /DLL /OPT:REF'
vsc,←'    /INCREMENTAL:NO /SUBSYSTEM:WINDOWS'
vsc,←'    /LIBPATH:"%AF_PATH%\lib"'
vsc,←'    /DYNAMICBASE "af',codfns.AF∆LIB,'.lib"'
vsc,←'    /OPT:ICF /ERRORREPORT:PROMPT'
vsc,←'    /TLBID:1 /OUT:"codfns.dll""'
@ %def vsbat wsd vsc

Finally, in order to write up the test harness to work right,
we must copy the appropriate runtime files into the [[tests\]]
directory so that we can find them when we finally start running
our code there.

<<Copy the runtime files into [[tests\]]>>=
⎕CMD ⎕←vsc
⎕CMD ⎕←'copy "',wsd,'rtm\codfns.h" "',wsd,'tests\"'
⎕CMD ⎕←'copy "',wsd,'rtm\codfns.exp" "',wsd,'tests\"'
⎕CMD ⎕←'copy "',wsd,'rtm\codfns.lib" "',wsd,'tests\"'
⎕CMD ⎕←'copy "',wsd,'rtm\codfns.pdb" "',wsd,'tests\"'
⎕CMD ⎕←'copy "',wsd,'rtm\codfns.dll" "',wsd,'tests\"'
@

\subsection{Loading the Compiler}

In order to load the compiler into an APL session as well as all the
development utilities,
we assume that you have first managed to either load up a session
with a bootstrapped version of the [[TANGLE]] command or that you
already have a tangled [[src\]] directory.
If the [[src\]] directory has not yet been created by running the
[[TANGLE]] command,
then this must be done before loading the compiler system.
After tangling,
the compiler can be loaded using the provided [[LOAD]] shortcut.
This shortcut is meant to use the
\href{https://github.com/Dyalog/link}{Dyalog Link}
system for hot-loading the files in [[src\]] into the root namespace.
We do so through the following link command:

\begin{verbatim}
Link.Create # src -source=dir -watch=dir
\end{verbatim}

\noindent
This means that we want to link the [[src\]] directory into the [[#]]
namespace,
but we also want to make sure that we only pull changes that come
from the filesystem.
This is because we are editing the code via the WEB document,
and we do not want to risk having some intermediate representation
that isn't accurate and that doesn't flow the right way;
we want all appropriate changes to begin in the WEB document
and then, and only then, flow into the session.
This also allows us to make some modifications to the code for testing
and experimentation inside of the session without consideration
for the code outside of the session,
and such changes will be removed or forgotten on the next [[TANGLE]]
command.

To set this up, we also ensure that we begin our work within the
root Co-dfns repository directory, as this is where we expect to run
the [[TANGLE]] and [[WEAVE]] commands.

There is unfortunately only a limited range of possibilities for
linking in a new directory as we wish to do.
The method we choose to use is launching a fresh Dyalog APL session
and then using an [[LX]] expression from the command line
to do the actual linking using the [[⎕SE.UCMD]] functionality.
I personally find this to be rather hackish, and I hope that an
alternative approach to doing this will show up in the near future.
Nonetheless, the arguments that we pass to [[dyalog.exe]]
look something like this:

\begin{verbatim}
LX="⎕SE.UCMD'Link.Create # src -source=dir -watch=dir'"
\end{verbatim}

If you do not use the [[LOAD]] shortcut, you can use the above
command to do the linking manually.

\section{Chunks}

\nowebchunks

\section{Index}

\nowebindex

\clearpage
\section{GNU AFFERO GPL}

\begin{center}
{\parindent 0in

Version 3, 19 November 2007

Copyright \copyright\  2007 Free Software Foundation, Inc. \texttt{https://fsf.org/}

\bigskip
Everyone is permitted to copy and distribute verbatim copies of this
license document, but changing it is not allowed.}

\end{center}

\begin{center}
{\Large Preamble}
\end{center}

\begin{flushleft}
The GNU Affero General Public License is a free, copyleft license
for software and other kinds of works, specifically designed to ensure
cooperation with the community in the case of network server software.

The licenses for most software and other practical works are
designed to take away your freedom to share and change the works.  By
contrast, our General Public Licenses are intended to guarantee your
freedom to share and change all versions of a program--to make sure it
remains free software for all its users.

When we speak of free software, we are referring to freedom, not
price. Our General Public Licenses are designed to make sure that you
have the freedom to distribute copies of free software (and charge for
them if you wish), that you receive source code or can get it if you
want it, that you can change the software or use pieces of it in new
free programs, and that you know you can do these things.

Developers that use our General Public Licenses protect your rights
with two steps: (1) assert copyright on the software, and (2) offer
you this License which gives you legal permission to copy, distribute
and/or modify the software.

A secondary benefit of defending all users' freedom is that
improvements made in alternate versions of the program, if they
receive widespread use, become available for other developers to
incorporate. Many developers of free software are heartened and
encouraged by the resulting cooperation. However, in the case of
software used on network servers, this result may fail to come about.
The GNU General Public License permits making a modified version and
letting the public access it on a server without ever releasing its
source code to the public.

The GNU Affero General Public License is designed specifically to
ensure that, in such cases, the modified source code becomes available
to the community.  It requires the operator of a network server to
provide the source code of the modified version running there to the
users of that server.  Therefore, public use of a modified version, on
a publicly accessible server, gives the public access to the source
code of the modified version.

An older license, called the Affero General Public License and
published by Affero, was designed to accomplish similar goals. This is
a different license, not a version of the Affero GPL, but Affero has
released a new version of the Affero GPL which permits relicensing under
this license.

The precise terms and conditions for copying, distribution and
modification follow.

\begin{center}
{\Large Terms and Conditions}
\end{center}

\begin{enumerate}

\addtocounter{enumi}{-1}

\item Definitions.

``This License'' refers to version 3 of the GNU Affero General Public License.

``Copyright'' also means copyright-like laws that apply to other kinds of
works, such as semiconductor masks.

``The Program'' refers to any copyrightable work licensed under this
License. Each licensee is addressed as ``you''. ``Licensees'' and
``recipients'' may be individuals or organizations.

To ``modify'' a work means to copy from or adapt all or part of the work
in a fashion requiring copyright permission, other than the making of an
exact copy.  The resulting work is called a ``modified version'' of the
earlier work or a work ``based on'' the earlier work.

A ``covered work'' means either the unmodified Program or a work based
on the Program.

To ``propagate'' a work means to do anything with it that, without
permission, would make you directly or secondarily liable for
infringement under applicable copyright law, except executing it on a
computer or modifying a private copy.  Propagation includes copying,
distribution (with or without modification), making available to the
public, and in some countries other activities as well.

To ``convey'' a work means any kind of propagation that enables other
parties to make or receive copies. Mere interaction with a user through
a computer network, with no transfer of a copy, is not conveying.

An interactive user interface displays ``Appropriate Legal Notices''
to the extent that it includes a convenient and prominently visible
feature that (1) displays an appropriate copyright notice, and (2)
tells the user that there is no warranty for the work (except to the
extent that warranties are provided), that licensees may convey the
work under this License, and how to view a copy of this License. If
the interface presents a list of user commands or options, such as a
menu, a prominent item in the list meets this criterion.

\item Source Code.

The ``source code'' for a work means the preferred form of the work
for making modifications to it.  ``Object code'' means any non-source
form of a work.

A ``Standard Interface'' means an interface that either is an official
standard defined by a recognized standards body, or, in the case of
interfaces specified for a particular programming language, one that
is widely used among developers working in that language.

The ``System Libraries'' of an executable work include anything, other
than the work as a whole, that (a) is included in the normal form of
packaging a Major Component, but which is not part of that Major
Component, and (b) serves only to enable use of the work with that
Major Component, or to implement a Standard Interface for which an
implementation is available to the public in source code form. A
``Major Component'', in this context, means a major essential component
(kernel, window system, and so on) of the specific operating system
(if any) on which the executable work runs, or a compiler used to
produce the work, or an object code interpreter used to run it.

The ``Corresponding Source'' for a work in object code form means all
the source code needed to generate, install, and (for an executable
work) run the object code and to modify the work, including scripts to
control those activities.  However, it does not include the work's
System Libraries, or general-purpose tools or generally available free
programs which are used unmodified in performing those activities but
which are not part of the work.  For example, Corresponding Source
includes interface definition files associated with source files for
the work, and the source code for shared libraries and dynamically
linked subprograms that the work is specifically designed to require,
such as by intimate data communication or control flow between those
subprograms and other parts of the work.

The Corresponding Source need not include anything that users
can regenerate automatically from other parts of the Corresponding
Source.

The Corresponding Source for a work in source code form is that
same work.

\item Basic Permissions.

All rights granted under this License are granted for the term of
copyright on the Program, and are irrevocable provided the stated
conditions are met.  This License explicitly affirms your unlimited
permission to run the unmodified Program.  The output from running a
covered work is covered by this License only if the output, given its
content, constitutes a covered work. This License acknowledges your
rights of fair use or other equivalent, as provided by copyright law.

You may make, run and propagate covered works that you do not
convey, without conditions so long as your license otherwise remains
in force.  You may convey covered works to others for the sole purpose
of having them make modifications exclusively for you, or provide you
with facilities for running those works, provided that you comply with
the terms of this License in conveying all material for which you do
not control copyright. Those thus making or running the covered works
for you must do so exclusively on your behalf, under your direction
and control, on terms that prohibit them from making any copies of
your copyrighted material outside their relationship with you.

Conveying under any other circumstances is permitted solely under
the conditions stated below. Sublicensing is not allowed; section 10
makes it unnecessary.

\item Protecting Users' Legal Rights From Anti-Circumvention Law.

No covered work shall be deemed part of an effective technological
measure under any applicable law fulfilling obligations under article
11 of the WIPO copyright treaty adopted on 20 December 1996, or
similar laws prohibiting or restricting circumvention of such
measures.

When you convey a covered work, you waive any legal power to forbid
circumvention of technological measures to the extent such circumvention
is effected by exercising rights under this License with respect to
the covered work, and you disclaim any intention to limit operation or
modification of the work as a means of enforcing, against the work's
users, your or third parties' legal rights to forbid circumvention of
technological measures.

\item Conveying Verbatim Copies.

You may convey verbatim copies of the Program's source code as you
receive it, in any medium, provided that you conspicuously and
appropriately publish on each copy an appropriate copyright notice;
keep intact all notices stating that this License and any
non-permissive terms added in accord with section 7 apply to the code;
keep intact all notices of the absence of any warranty; and give all
recipients a copy of this License along with the Program.

You may charge any price or no price for each copy that you convey,
and you may offer support or warranty protection for a fee.

\item Conveying Modified Source Versions.

You may convey a work based on the Program, or the modifications to
produce it from the Program, in the form of source code under the
terms of section 4, provided that you also meet all of these conditions:
 \begin{enumerate}
 \item The work must carry prominent notices stating that you modified
 it, and giving a relevant date.

 \item The work must carry prominent notices stating that it is
 released under this License and any conditions added under section
 7. This requirement modifies the requirement in section 4 to
 ``keep intact all notices''.

 \item You must license the entire work, as a whole, under this
 License to anyone who comes into possession of a copy. This
 License will therefore apply, along with any applicable section 7
 additional terms, to the whole of the work, and all its parts,
 regardless of how they are packaged. This License gives no
 permission to license the work in any other way, but it does not
 invalidate such permission if you have separately received it.

 \item If the work has interactive user interfaces, each must display
 Appropriate Legal Notices; however, if the Program has interactive
 interfaces that do not display Appropriate Legal Notices, your
 work need not make them do so.
\end{enumerate}
A compilation of a covered work with other separate and independent
works, which are not by their nature extensions of the covered work,
and which are not combined with it such as to form a larger program,
in or on a volume of a storage or distribution medium, is called an
``aggregate'' if the compilation and its resulting copyright are not
used to limit the access or legal rights of the compilation's users
beyond what the individual works permit. Inclusion of a covered work
in an aggregate does not cause this License to apply to the other
parts of the aggregate.

\item Conveying Non-Source Forms.

You may convey a covered work in object code form under the terms
of sections 4 and 5, provided that you also convey the
machine-readable Corresponding Source under the terms of this License,
in one of these ways:
 \begin{enumerate}
 \item Convey the object code in, or embodied in, a physical product
 (including a physical distribution medium), accompanied by the
 Corresponding Source fixed on a durable physical medium
 customarily used for software interchange.

 \item Convey the object code in, or embodied in, a physical product
 (including a physical distribution medium), accompanied by a
 written offer, valid for at least three years and valid for as
 long as you offer spare parts or customer support for that product
 model, to give anyone who possesses the object code either (1) a
 copy of the Corresponding Source for all the software in the
 product that is covered by this License, on a durable physical
 medium customarily used for software interchange, for a price no
 more than your reasonable cost of physically performing this
 conveying of source, or (2) access to copy the
 Corresponding Source from a network server at no charge.

 \item Convey individual copies of the object code with a copy of the
 written offer to provide the Corresponding Source. This
 alternative is allowed only occasionally and noncommercially, and
 only if you received the object code with such an offer, in accord
 with subsection 6b.

 \item Convey the object code by offering access from a designated
 place (gratis or for a charge), and offer equivalent access to the
 Corresponding Source in the same way through the same place at no
 further charge.  You need not require recipients to copy the
 Corresponding Source along with the object code. If the place to
 copy the object code is a network server, the Corresponding Source
 may be on a different server (operated by you or a third party)
 that supports equivalent copying facilities, provided you maintain
 clear directions next to the object code saying where to find the
 Corresponding Source.  Regardless of what server hosts the
 Corresponding Source, you remain obligated to ensure that it is
 available for as long as needed to satisfy these requirements.

 \item Convey the object code using peer-to-peer transmission, provided
 you inform other peers where the object code and Corresponding
 Source of the work are being offered to the general public at no
 charge under subsection 6d.
 \end{enumerate}

A separable portion of the object code, whose source code is excluded
from the Corresponding Source as a System Library, need not be
included in conveying the object code work.

A ``User Product'' is either (1) a ``consumer product'', which means any
tangible personal property which is normally used for personal, family,
or household purposes, or (2) anything designed or sold for incorporation
into a dwelling. In determining whether a product is a consumer product,
doubtful cases shall be resolved in favor of coverage. For a particular
product received by a particular user, ``normally used'' refers to a
typical or common use of that class of product, regardless of the status
of the particular user or of the way in which the particular user
actually uses, or expects or is expected to use, the product.  A product
is a consumer product regardless of whether the product has substantial
commercial, industrial or non-consumer uses, unless such uses represent
the only significant mode of use of the product.

``Installation Information'' for a User Product means any methods,
procedures, authorization keys, or other information required to install
and execute modified versions of a covered work in that User Product from
a modified version of its Corresponding Source.  The information must
suffice to ensure that the continued functioning of the modified object
code is in no case prevented or interfered with solely because
modification has been made.

If you convey an object code work under this section in, or with, or
specifically for use in, a User Product, and the conveying occurs as
part of a transaction in which the right of possession and use of the
User Product is transferred to the recipient in perpetuity or for a
fixed term (regardless of how the transaction is characterized), the
Corresponding Source conveyed under this section must be accompanied
by the Installation Information. But this requirement does not apply
if neither you nor any third party retains the ability to install
modified object code on the User Product (for example, the work has
been installed in ROM).

The requirement to provide Installation Information does not include a
requirement to continue to provide support service, warranty, or updates
for a work that has been modified or installed by the recipient, or for
the User Product in which it has been modified or installed. Access to a
network may be denied when the modification itself materially and
adversely affects the operation of the network or violates the rules and
protocols for communication across the network.

Corresponding Source conveyed, and Installation Information provided,
in accord with this section must be in a format that is publicly
documented (and with an implementation available to the public in
source code form), and must require no special password or key for
unpacking, reading or copying.

\item Additional Terms.

``Additional permissions'' are terms that supplement the terms of this
License by making exceptions from one or more of its conditions.
Additional permissions that are applicable to the entire Program shall
be treated as though they were included in this License, to the extent
that they are valid under applicable law.  If additional permissions
apply only to part of the Program, that part may be used separately
under those permissions, but the entire Program remains governed by
this License without regard to the additional permissions.

When you convey a copy of a covered work, you may at your option
remove any additional permissions from that copy, or from any part of
it.  (Additional permissions may be written to require their own
removal in certain cases when you modify the work.)  You may place
additional permissions on material, added by you to a covered work,
for which you have or can give appropriate copyright permission.

Notwithstanding any other provision of this License, for material you
add to a covered work, you may (if authorized by the copyright holders of
that material) supplement the terms of this License with terms:
 \begin{enumerate}
 \item Disclaiming warranty or limiting liability differently from the
 terms of sections 15 and 16 of this License; or

 \item Requiring preservation of specified reasonable legal notices or
 author attributions in that material or in the Appropriate Legal
 Notices displayed by works containing it; or

 \item Prohibiting misrepresentation of the origin of that material, or
 requiring that modified versions of such material be marked in
 reasonable ways as different from the original version; or

 \item Limiting the use for publicity purposes of names of licensors or
 authors of the material; or

 \item Declining to grant rights under trademark law for use of some
 trade names, trademarks, or service marks; or

 \item Requiring indemnification of licensors and authors of that
 material by anyone who conveys the material (or modified versions of
 it) with contractual assumptions of liability to the recipient, for
 any liability that these contractual assumptions directly impose on
 those licensors and authors.
 \end{enumerate}

All other non-permissive additional terms are considered ``further
restrictions'' within the meaning of section 10. If the Program as you
received it, or any part of it, contains a notice stating that it is
governed by this License along with a term that is a further
restriction, you may remove that term. If a license document contains
a further restriction but permits relicensing or conveying under this
License, you may add to a covered work material governed by the terms
of that license document, provided that the further restriction does
not survive such relicensing or conveying.

If you add terms to a covered work in accord with this section, you
must place, in the relevant source files, a statement of the
additional terms that apply to those files, or a notice indicating
where to find the applicable terms.

Additional terms, permissive or non-permissive, may be stated in the
form of a separately written license, or stated as exceptions;
the above requirements apply either way.

\item Termination.

You may not propagate or modify a covered work except as expressly
provided under this License. Any attempt otherwise to propagate or
modify it is void, and will automatically terminate your rights under
this License (including any patent licenses granted under the third
paragraph of section 11).

However, if you cease all violation of this License, then your
license from a particular copyright holder is reinstated (a)
provisionally, unless and until the copyright holder explicitly and
finally terminates your license, and (b) permanently, if the copyright
holder fails to notify you of the violation by some reasonable means
prior to 60 days after the cessation.

Moreover, your license from a particular copyright holder is
reinstated permanently if the copyright holder notifies you of the
violation by some reasonable means, this is the first time you have
received notice of violation of this License (for any work) from that
copyright holder, and you cure the violation prior to 30 days after
your receipt of the notice.

Termination of your rights under this section does not terminate the
licenses of parties who have received copies or rights from you under
this License.  If your rights have been terminated and not permanently
reinstated, you do not qualify to receive new licenses for the same
material under section 10.

\item Acceptance Not Required for Having Copies.

You are not required to accept this License in order to receive or
run a copy of the Program. Ancillary propagation of a covered work
occurring solely as a consequence of using peer-to-peer transmission
to receive a copy likewise does not require acceptance.  However,
nothing other than this License grants you permission to propagate or
modify any covered work. These actions infringe copyright if you do
not accept this License. Therefore, by modifying or propagating a
covered work, you indicate your acceptance of this License to do so.

\item Automatic Licensing of Downstream Recipients.

Each time you convey a covered work, the recipient automatically
receives a license from the original licensors, to run, modify and
propagate that work, subject to this License.  You are not responsible
for enforcing compliance by third parties with this License.

An ``entity transaction'' is a transaction transferring control of an
organization, or substantially all assets of one, or subdividing an
organization, or merging organizations.  If propagation of a covered
work results from an entity transaction, each party to that
transaction who receives a copy of the work also receives whatever
licenses to the work the party's predecessor in interest had or could
give under the previous paragraph, plus a right to possession of the
Corresponding Source of the work from the predecessor in interest, if
the predecessor has it or can get it with reasonable efforts.

You may not impose any further restrictions on the exercise of the
rights granted or affirmed under this License. For example, you may
not impose a license fee, royalty, or other charge for exercise of
rights granted under this License, and you may not initiate litigation
(including a cross-claim or counterclaim in a lawsuit) alleging that
any patent claim is infringed by making, using, selling, offering for
sale, or importing the Program or any portion of it.

\item Patents.

A ``contributor'' is a copyright holder who authorizes use under this
License of the Program or a work on which the Program is based.  The
work thus licensed is called the contributor's ``contributor version''.

A contributor's ``essential patent claims'' are all patent claims
owned or controlled by the contributor, whether already acquired or
hereafter acquired, that would be infringed by some manner, permitted
by this License, of making, using, or selling its contributor version,
but do not include claims that would be infringed only as a
consequence of further modification of the contributor version.  For
purposes of this definition, ``control'' includes the right to grant
patent sublicenses in a manner consistent with the requirements of
this License.

Each contributor grants you a non-exclusive, worldwide, royalty-free
patent license under the contributor's essential patent claims, to
make, use, sell, offer for sale, import and otherwise run, modify and
propagate the contents of its contributor version.

In the following three paragraphs, a ``patent license'' is any express
agreement or commitment, however denominated, not to enforce a patent
(such as an express permission to practice a patent or covenant not to
sue for patent infringement).  To ``grant'' such a patent license to a
party means to make such an agreement or commitment not to enforce a
patent against the party.

If you convey a covered work, knowingly relying on a patent license,
and the Corresponding Source of the work is not available for anyone
to copy, free of charge and under the terms of this License, through a
publicly available network server or other readily accessible means,
then you must either (1) cause the Corresponding Source to be so
available, or (2) arrange to deprive yourself of the benefit of the
patent license for this particular work, or (3) arrange, in a manner
consistent with the requirements of this License, to extend the patent
license to downstream recipients.  ``Knowingly relying'' means you have
actual knowledge that, but for the patent license, your conveying the
covered work in a country, or your recipient's use of the covered work
in a country, would infringe one or more identifiable patents in that
country that you have reason to believe are valid.

If, pursuant to or in connection with a single transaction or
arrangement, you convey, or propagate by procuring conveyance of, a
covered work, and grant a patent license to some of the parties
receiving the covered work authorizing them to use, propagate, modify
or convey a specific copy of the covered work, then the patent license
you grant is automatically extended to all recipients of the covered
work and works based on it.

A patent license is ``discriminatory'' if it does not include within
the scope of its coverage, prohibits the exercise of, or is
conditioned on the non-exercise of one or more of the rights that are
specifically granted under this License. You may not convey a covered
work if you are a party to an arrangement with a third party that is
in the business of distributing software, under which you make payment
to the third party based on the extent of your activity of conveying
the work, and under which the third party grants, to any of the
parties who would receive the covered work from you, a discriminatory
patent license (a) in connection with copies of the covered work
conveyed by you (or copies made from those copies), or (b) primarily
for and in connection with specific products or compilations that
contain the covered work, unless you entered into that arrangement,
or that patent license was granted, prior to 28 March 2007.

Nothing in this License shall be construed as excluding or limiting
any implied license or other defenses to infringement that may
otherwise be available to you under applicable patent law.

\item No Surrender of Others' Freedom.

If conditions are imposed on you (whether by court order, agreement or
otherwise) that contradict the conditions of this License, they do not
excuse you from the conditions of this License.  If you cannot convey a
covered work so as to satisfy simultaneously your obligations under this
License and any other pertinent obligations, then as a consequence you may
not convey it at all.  For example, if you agree to terms that obligate you
to collect a royalty for further conveying from those to whom you convey
the Program, the only way you could satisfy both those terms and this
License would be to refrain entirely from conveying the Program.

\item Remote Network Interaction; Use with the GNU General Public License.

Notwithstanding any other provision of this License, if you modify the
Program, your modified version must prominently offer all users interacting
with it remotely through a computer network (if your version supports such
interaction) an opportunity to receive the Corresponding Source of your
version by providing access to the Corresponding Source from a network
server at no charge, through some standard or customary means of
facilitating copying of software.  This Corresponding Source shall include
the Corresponding Source for any work covered by version 3 of the GNU
General Public License that is incorporated pursuant to the following
paragraph.

Notwithstanding any other provision of this License, you have permission to
link or combine any covered work with a work licensed under version 3 of
the GNU General Public License into a single combined work, and to convey
the resulting work.  The terms of this License will continue to apply to
the part which is the covered work, but the work with which it is combined
will remain governed by version 3 of the GNU General Public License.

\item Revised Versions of this License.

The Free Software Foundation may publish revised and/or new versions of
the GNU Affero General Public License from time to time. Such new versions will
be similar in spirit to the present version, but may differ in detail to
address new problems or concerns.

Each version is given a distinguishing version number. If the
Program specifies that a certain numbered version of the GNU Affero General
Public License ``or any later version'' applies to it, you have the
option of following the terms and conditions either of that numbered
version or of any later version published by the Free Software
Foundation.  If the Program does not specify a version number of the
GNU Affero General Public License, you may choose any version ever published
by the Free Software Foundation.

If the Program specifies that a proxy can decide which future
versions of the GNU Affero General Public License can be used, that proxy's
public statement of acceptance of a version permanently authorizes you
to choose that version for the Program.

Later license versions may give you additional or different
permissions. However, no additional obligations are imposed on any
author or copyright holder as a result of your choosing to follow a
later version.

\item Disclaimer of Warranty.

\begin{sloppypar}
 THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
 APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE
 COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM ``AS IS''
 WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED,
 INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
 MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE
 RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU.
 SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL
 NECESSARY SERVICING, REPAIR OR CORRECTION.
\end{sloppypar}

\item Limitation of Liability.

 IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN
 WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES
 AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR
 DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL
 DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM
 (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED
 INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE
 OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH
 HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH
 DAMAGES.

\item Interpretation of Sections 15 and 16.

If the disclaimer of warranty and limitation of liability provided
above cannot be given local legal effect according to their terms,
reviewing courts shall apply local law that most closely approximates
an absolute waiver of all civil liability in connection with the
Program, unless a warranty or assumption of liability accompanies a
copy of the Program in return for a fee.

\begin{center}
{\Large End of Terms and Conditions}

\bigskip
How to Apply These Terms to Your New Programs
\end{center}

If you develop a new program, and you want it to be of the greatest
possible use to the public, the best way to achieve this is to make it
free software which everyone can redistribute and change under these terms.

To do so, attach the following notices to the program. It is safest
to attach them to the start of each source file to most effectively
state the exclusion of warranty; and each file should have at least
the ``copyright'' line and a pointer to where the full notice is found.

{\footnotesize
\begin{verbatim}
<one line to give the program's name and a brief idea of what it does.>

Copyright (C) <textyear> <name of author>

This program is free software: you can redistribute it and/or 
modify it under the terms of the GNU Affero General Public 
License as published by the Free Software Foundation, either 
version 3 of the License, or (at your option) any later version. 

This program is distributed in the hope that it will be useful, 
but WITHOUT ANY WARRANTY; without even the implied warranty of 
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU 
Affero General Public License for more details.

You should have received a copy of the GNU Affero General Public 
License along with this program. If not, see 
<https://www.gnu.org/licenses/>.
\end{verbatim}
}

Also add information on how to contact you by electronic and paper mail.

If your software can interact with users remotely through a computer
network, you should also make sure that it provides a way for users to
get its source.  For example, if your program is a web application, its
interface could display a ``Source'' link that leads users to an archive
of the code. There are many ways you could offer source, and different
solutions will be better for different programs; see section 13 for the
specific requirements.

You should also get your employer (if you work as a programmer) or
school, if any, to sign a ``copyright disclaimer'' for the program, if
necessary. For more information on this, and how to apply and follow
the GNU AGPL, see \\
\texttt{https://www.gnu.org/licenses/}.

\end{enumerate}

\end{flushleft}

\end{document}
