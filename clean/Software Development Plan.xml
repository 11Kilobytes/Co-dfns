<?xml version="1.0" encoding="utf-8" ?>

<article xmlns="http://docbook.org/ns/docbook" version="5.0">
  <info>
    <author>Aaron W. Hsu</author>
    <date>Tuesday, July 4th, 2013</date>
  </info>
  <section>
    <title>Co-Dfns Software Development Plan</title>
    <section>
      <title>Mission</title>
      <simpara>
        This project has two main goals. The first is to produce a workable 
        compiler that can be demonstrated to Dyalog customers later this year. 
        The second is to prepare a framework for future compiler development and 
        research.        
      </simpara>
    </section>
    <section>
      <title>Work Products</title>
      <simpara>
        The following work products will be created during the course of this 
        project. Some of them will be useful to the end-user, and others are 
        only of interest for continued maintenance and development of the 
        system.
      </simpara>
      <glosslist>
        <title>End-user Consumable Work Products</title>
        <glossentry>
          <glossterm>Executable System</glossterm>
          <glossdef>
            <simpara>
              Each increment is turned into an executable form during 
              certification and testing. The final increment forms the 
              completed software system in executable form for the end-user. 
            </simpara>
          </glossdef>
        </glossentry>
        <glossentry>
          <glossterm>Reference Manual</glossterm>
          <glossdef>
            <simpara>
              Not part of the development cycle, at the completion of the 
              final certification phase, the development work products are 
              accumulated into an useful reference manual for the user of 
              the <citetitle>executable system</citetitle>. This may be 
              maintained as a part of the 
              <citetitle>Executable System</citetitle>.
            </simpara>
          </glossdef>
        </glossentry>
      </glosslist>
      <glosslist>
        <title>Development and Technical Work Products</title>
        <glossentry>
          <glossterm>Cleanroom Engineering Guide</glossterm>
          <glossdef>
            <simpara>
              Defines the adaptation and refinement of the Cleanroom processes 
              to meet project specific requirements. It represents the 
              normative operating procedure for development.
            </simpara>
          </glossdef>
        </glossentry>
        <glossentry>
          <glossterm>Engineering Change Log</glossterm>
          <glossdef>
            <simpara>
              Record of all engineering changes, evaluations, impacts, and 
              statuses. This records the changes initiated in one process that 
              begins another process to address the intended change.
            </simpara>
          </glossdef>
        </glossentry>
        <glossentry>
          <glossterm>Function Specification</glossterm>
          <glossdef>
            <simpara>
              Documents software boundaries and interfaces, together with 
              the external view if a system. It is a precise statement of the 
              <citetitle>Software Requirements</citetitle> and is the 
              definitive statement on the black box behavior of the system.
            </simpara>
          </glossdef>
        </glossentry>
        <glossentry>
          <glossterm>Increment Certification Report</glossterm>
          <glossdef>
            <simpara>
              Contains the values for measures of the statistical goals and 
              measures of process control. It is the means by which development 
              either progresses or enters into engineering change, depending 
              on the quality results reported by this report.
            </simpara>
          </glossdef>
        </glossentry>
        <glossentry>
          <glossterm>Increment Construction Plan</glossterm>
          <glossdef>
            <simpara>
              Specifies the number of increments into which the project will 
              be divided, the functions that will be implemented in each 
              increment, and the schedule for each increment. 
            </simpara>
          </glossdef>
        </glossentry>
        <glossentry>
          <glossterm>Increment Design</glossterm>
          <glossdef>
            <simpara>
              The box structure implementation of a set of functions named in 
              the <citetitle>Increment Construction Plan</citetitle> and 
              defined in the <citetitle>Function Specification</citetitle>. 
              Increment designs are cumulative from one increment to the other. 
            </simpara>
          </glossdef>
        </glossentry>
        <glossentry>
          <glossterm>Increment Test Plan</glossterm>
          <glossdef>
            <simpara>
              Contains all the information needed for statistical certification 
              and testing.               
            </simpara>
          </glossdef>
        </glossentry>
        <glossentry>
          <glossterm>Increment Verification Report</glossterm>
          <glossdef>
            <simpara>
              The record of experience during correctness verification. 
            </simpara>
          </glossdef>
        </glossentry>
        <glossentry>
          <glossterm>Project Record</glossterm>
          <glossdef>
            <simpara>
              Contains a journal and record of all proceedings throughout the 
              project together with any additional documents that need to be 
              kept around.
            </simpara>
          </glossdef>
        </glossentry>
        <glossentry>
          <glossterm>Reengineering Plan</glossterm>
          <glossdef>
            <simpara>
              Contains the information and plans necessary to prepare existing 
              software for reuse in this project.
            </simpara>
          </glossdef>
        </glossentry>
        <glossentry>
          <glossterm>Reengineered Software</glossterm>
          <glossdef>
            <simpara>
              Consists of the specifications, designs, code, usage models, 
              and/or testing artifacts produced during the reengineering of 
              reused components.
            </simpara>
          </glossdef>
        </glossentry>
        <glossentry>
          <glossterm>Software Architecture</glossterm>
          <glossdef>
            <simpara>
              Identifies the conceptual architecture, expressed in terms of 
              principal software components and their relationships; the 
              module architecture, expressed in terms of layers of functional 
              decomposition; and the execution architecture, expressed in 
              terms of dynamic software operation. 
            </simpara>
          </glossdef>
        </glossentry>
        <glossentry>
          <glossterm>Software Development Plan</glossterm>
          <glossdef>
            <simpara>
              A composite plan for project management. Consists of the 
              following sub-plans:
            </simpara>
            <simpara>
              The <citetitle>Project Goals</citetitle>, which establish the 
              intended results of the project.
            </simpara>
            <simpara>
              The <citetitle>Work Product Plan</citetitle> defines the 
              Cleanroom work products to be produced by the project. 
            </simpara>
            <simpara>
              The <citetitle>Schedule Plan</citetitle> defines estimates for 
              overall tasks, schedules, and milestones. 
            </simpara>
            <simpara>
              The <citetitle>Measurement Plan</citetitle> defines product and 
              process measurements, standards, and goals for managing the 
              project, including those for Cleanroom software certification 
              and statistical process control.
            </simpara>
            <simpara>
              The <citetitle>Reuse Plan</citetitle> identifies sources of 
              reusable assets, and asset acquisition and evaluation tasks. 
              It also identifies oppurtunities to reuse domain models, 
              reference architectures, software specifications, designs, 
              code, and usage models.
            </simpara>
            <simpara>
              The <citetitle>Configuration Management Plan</citetitle> 
              defines requirements for change control of designated work 
              products. 
            </simpara>
          </glossdef>
        </glossentry>
        <glossentry>
          <glossterm>Software Requirements</glossterm>
          <glossdef>
            <simpara>
              Defines the functional, usage, performance, and environmental 
              requirements for the system. Included among these requirements 
              are operational constraints such as dependencies on other 
              systems, capacity requirements, and reliability requirements. 
            </simpara>
          </glossdef>
        </glossentry>
        <glossentry>
          <glossterm>Statement of Work</glossterm>
          <glossdef>
            <simpara>
              Initial seed for all other work to begin.
            </simpara>
          </glossdef>
        </glossentry>
        <glossentry>
          <glossterm>Statistical Test Cases</glossterm>
          <glossdef>
            <simpara>
              Generated randomly from a usage model for use in statistical 
              testing of an increment. Once generated, test cases may undergo 
              postprocessing to add information for human testers or for an 
              automated test tool. Each test case is a complete usage scenario 
              given as a sequence of user inputs. 
            </simpara>
          </glossdef>
        </glossentry>
        <glossentry>
          <glossterm>Statistical Testing Report</glossterm>
          <glossdef>
            <simpara>
              The record of experience in testing, and includes number of 
              compilation sessions, faults found duing compilation, 
              number of testing sessions, number of test cases executed 
              during each session, failures observed during test case 
              executions, faults found during investigation of failures, time 
              required to correct each fault, and any other information 
              relevant to assessment of the correctness of the executing 
              software. 
            </simpara>
          </glossdef>
        </glossentry>
        <glossentry>
          <glossterm>Usage Models</glossterm>
          <glossdef>
            <simpara>
              A formal representation of software use. Defines usage states 
              and the probabilities of transitions betreen usage states. 
            </simpara>
          </glossdef>
        </glossentry>
        <glossentry>
          <glossterm>Usage Specification</glossterm>
          <glossdef>
            <simpara>
              Description of the expected users, usage scenarios, and usage 
              environments of the software. It contains definitions of 
              high-level usage models that record this information, as well 
              as the results of model analysis.
            </simpara>
          </glossdef>
        </glossentry>
      </glosslist>
    </section>
    <section>
      <title>Schedule</title>
      <section>
        <title>Overview</title>
        <simpara>
          The main deadline in this project is the October conference. To meet 
          this deadline, the final product needs to be completed by the 1st of 
          the same month. A good deal of existing code and research is 
          already in place, but these will be reused as part of the 
          reengineering effort. The key component is the architecture 
          specification, which will need more time than normal, and more 
          rigorous specification as prototypes indicate that the main details 
          of program behavior are contained in the pass decomposition. As the 
          external behavior is less complicated, more resources must be 
          allocated to a clear architecture to decompose the problem further. 
        </simpara>
        <simpara>
          There is a planned productivity hit during the weeks of July 15th 
          to the 26th, to account for an educational program. This is the 
          time for allocating the least intensive pieces, where increment 
          planning, and other high-level specifications can happen, since 
          some of that burden is off-loaded onto the Architecture 
          Specification. 
        </simpara>
      </section>
      <section>
        <title>Milestones</title>
        <simpara>
          The following milestones represent the high-level organizational 
          milestones. Milestone 4 will be refined into Increments with their 
          own due dates during Increment Planning.
        </simpara>
        <informaltable frame="void" rules="rows">
          <thead>
            <tr>
              <th>Number</th>
              <th colspan="5">Description</th>
              <th colspan="2">Due Date</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>1</td>
              <td colspan="5">Planning and Specification</td>
              <td colspan="2">July 10th, 2013</td>
            </tr>
            <tr>
              <td>2</td>
              <td colspan="5">Architecture Specification</td>
              <td colspan="2">July 17th, 2013</td>
            </tr>
            <tr>
              <td>3</td>
              <td colspan="5">Increment, Usage Model, and Test Planning</td>
              <td colspan="2">July 24th, 2013</td>
            </tr>
            <tr>
              <td>4</td>
              <td colspan="5">Increment Design</td>
              <td colspan="2">October 1st, 2013</td>
            </tr>
          </tbody>
        </informaltable>
      </section>
    </section>
    <section>
      <title>Measurement Plan</title>
      <simpara>
        The following sections define a plan for statistical quality control 
        and measures of product readiness. They include the goals for 
        software certification together with the defined metrics for measuring 
        product and process quality. They define the process requirements to 
        assure that the certification process accurately certifies the quality 
        of the software and that the process can be evaluated for quality 
        assessment.
      </simpara>
      <section>
        <title>Certification Goals</title>
        <simpara>
          The following goals drive the certification effort and therefore 
          encapsulate the core of the measurement requirements for the 
          project. 
        </simpara>
        <simpara>
          The project itself is considered a demonstration vehicle and initial 
          product to move forward. As such, the software is not considered 
          mission critical or sensitive to failures. It should, however, be 
          adequately reliable for use in small scale industrial applications, 
          meaning that failures, if they occur at all, should be immediate and 
          obvious. 
        </simpara>
        <orderedlist>
          <listitem>
            <simpara>
              The system should be semantically complete, and testing of all 
              primitives and language features should succeed to a reasonable 
              metric. No feature should be unimplemented to at least a basic 
              degree.
            </simpara>
          </listitem>
          <listitem>
            <simpara>
              Testing need not cover performance metrics, though testing and 
              verification of optimizations should be conducted.
            </simpara>
          </listitem>
          <listitem>
            <simpara>
              System should test every requirement.
            </simpara>
          </listitem>
          <listitem>
            <simpara>
              Testing should cover both compilation and execution of compiled 
              code, and should cover both error conditions in compilation as 
              well as error conditions in the executed code. 
            </simpara>
          </listitem>
          <listitem>
            <simpara>
              Certify the correctness and reliability of the compiled code to 
              99% reliability at 95% confidence. 
            </simpara>
          </listitem>
          <listitem>
            <simpara>
              Certify the reliability of executing the compiler to 90% at 95% 
              confidence.
            </simpara>
          </listitem>
          <listitem>
            <simpara>
              There should be no known faults in the generated code.
            </simpara>
          </listitem>
        </orderedlist>
      </section>
      <section>
        <title>Statistical Measures of Software Quality</title>
        <simpara>
          Two primary metrics will be used to certify software quality, the 
          Mean Time To Failure and Reliability. The MTTF and Reliability will 
          be calculated per increment on the basis of whole system sampling 
          using the sampling model for statistical benchmarks. Both will be 
          expressed in terms of individual test case executions, as opposed to 
          some metric of time. 
        </simpara>
        <simpara>
          The MTTF and Reliability are connected to one another, with the 
          MTTF expressed in terms of reliability by the following expression:
        </simpara>
        <programlisting>÷1-Reliability</programlisting>
        <simpara>
          We will use a zero-failures metric for certification. This means 
          that an increment will be certified only after it has been 
          shown to have zero failures in a statistically representative 
          testing session. The method for calculating the statistical 
          significance is the sampling method, which has the benefits of being 
          a conservatice measure that is simple to calculate. Given a desired 
          reliability and confidence, we can determine the number of test 
          cases that must run without failure using the following calculation:
        </simpara>
        <programlisting>⌈(10⍟1-C)÷10⍟R</programlisting>
        <simpara>
          Here, <varname>C</varname> is the desired confidence and 
          <varname>R</varname> is the desired reliability. 
        </simpara>
        <simpara>
          In our requirements, we have two different reliability metrics, 
          since we separate compiler reliability into the reliability of the 
          generated code and the reliability of the compiler itself. The 
          following table gives the number of test cases with zero failures 
          that must run in order to certify a version of the compiler 
          component at the desired reliability and confidence, as determined 
          by the certification goals listed above.
        </simpara>
        <informaltable frame="void" rules="rows">
          <thead>
            <tr>
              <th>Component</th>
              <th>Reliability</th>
              <th>Confidence</th>
              <th>Test Cases</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Compiler</td>
              <td>0.90</td>
              <td>0.95</td>
              <td>29</td>
            </tr>
            <tr>
              <td>Generated Code</td>
              <td>0.99</td>
              <td>0.95</td>
              <td>299</td>
            </tr>
          </tbody>
        </informaltable>
      </section>
      <section>
        <title>Testing Process Measures and Control</title>
        <simpara>
          In addition to the certification record, the testing process must 
          record the iterations between testing, the changes in system 
          reliability and MTTF, and the relative improvement of each version 
          of the software both from increment to increment as well as between 
          engineering changes within the increment. The number of faults 
          discovered during the process should be tracked and monitered for 
          signs that the development process is not producing software of 
          the appropriate quality.
        </simpara>
        <simpara>
          Development should not progress to the next increment until the 
          current increment under testing is certified to appropriate levels. 
          Moreover, of testing encounters a failure it should immediately 
          initiate an engineering change and the fault should be addressed. 
          Once the fault has been confirmed to be removed, the entire 
          expirement must be run from the beginning on the new version of 
          the software. 
        </simpara>
      </section>
      <section>
        <title>Usage Model Evolution</title>
        <simpara>
          Current statistical data on real input distributions is 
          non-existent, so estimated models will have to be used for the 
          majority of the project. However, the final increment, after 
          certification on the existing estimated model, will contain the 
          necessary code for calculating the actual input distributions based 
          on the real-life, public code libraries that exist. After the 
          standard certification is complete, a usage model derived from 
          the code libraries available publically and uniquely for this 
          project should be created and used to calculate a final 
          certification test. This final certification should serve as the 
          public quality assurance. 
        </simpara>
      </section>
    </section>
  </section>
</article>